## Lecture 01: Introduction

Concurrency: might not be running at the same instant

Parallelism: multiple tasks running at the exact same time

##### Critical Section Requirements

1. Mutex
2. Progress
3. No starvation
4. Performance

##### Mechanisms

1. Locks
2. Semaphores
3. Monitors
4. Messages

##### Condition for Deadlock

1. Mutex
2. Hold and wait
3. No preemption
4. Circular wait

## Lecture 02: Tasks, Threads & Synchronisation

##### Program Execution

* Decomposition of computations
* Assignment of tasks
  * Task parallelism
  * Data parallelism
* Orchestration
* (Mapping)

#### § Ownership & Lifetime in C++

##### Ownership

Every object on the free store must have exactly one owner

Owner is an object containing pointer to an object allocated by `new` which a `delete` is required

* Scoped objects: destruction is implicit at scope exit

* Free store objects: `new` & `delete` are required

##### Lifetime

Begins when:

* Storage with proper alignment and size is obtained
* Initialisation is complete

Ends when:

* Non-class type: destroyed
* Class type: destructor call starts
* Storage which the object occupies is released/reused

RAII: resource acquisition is initialisation

#### § Threads in C++

Every program has at least one thread

Instances of `std::thread` are movable and not copyable

##### Ways to Start Threads

```c++
// 1. function
void do_work();
std::thread my_thread(do_work);

// 2. callable function object, function is copied
class background_task {
  public:
  	void operator()() const {
      do_work();
    }
}
background_task f;
std::thread my_thread(f);

// 3. function object
std::thread my_thread{background_task()};
/* same but not recommended */
std::thread my_thread((background_task()));
/* declare function that returns a thread */
/* does not launch a new thread */
std::thread my_thread(background_task());

// 4. lambda expression
std::thread my_thread([]{
  do_work();
});
```

##### Join & Detach

* `join()`

Wait for a thread to finish, use `joinable()` to check

Make sure to join even when there is an exception

```c++
void f() {
	int local_state = 0;
  func my_func(local_state);
  std::thread t(my_func);
  try {
    do_work_in_current_thread();
  } catch (...) {
    t.join();
    throw;
  }
  t.join();
}
```

* `detach()`

Local variables passed as parameters might end their lifetime before thread ends, extra care needed

##### Passing Arguments to Thread Function

```c++
// 1. pass by value
void f(int i, std::string const& s);
std::thread t(f, 7, "hi");

// 2. explicit cast + pass by value
// cannot pass `buffer`, func might exit
void f(int i, std::string const& s);
void bar(int param) {
  char buffer[1024];
  sprintf(buffer, "%i", param);
  std::thread t(f, 7, std::string(buffer));
  t.deatch();
}

// 3. pass by reference
void update(id my_id, data& my_data);
void bar(id my_id) {
  data my_data;
  std::thread t(update, my_id, std::ref(my_data));
}
```

##### Transferring Ownership of Thread

```c++
// transfer out of function
std::thread f() { // without param
  void some_function();
  return std::thread(some_function);
}

std::thread g() { //with param
  void some_other_function(int);
	std::thread t(some_other_function, 7);
  return t;
}

// transfer into function
void f(std::thread t);
void g() {
  void some_function();
  f(std::thread(some_function));
  std::thread t(some_function);
  f(std::move(t));
}

```

##### Synchronising Threads

Race condition: outcome depends on relative order of execution, flaw that affects program's correctness

Data race: two non-read non-synchronised memory accesses target the same location, causes undefined behaviour 

To avoid race condition: wrap data structure with protection mechanism

In C++:

1. Lock

   `std::mutex` with `lock()` and `unlock()` is not recommended

   `std::unique_lock`: can be unlocked by other API call

   `std::scoped_lock`: can lock multiple mutex, only be destroyed when it goes out of scope

   `std::lock_guard`: can only lock one mutex

   Do NOT pass references to protected data outside the scope of lock (any code that has access to the references can now modify protected data without locking)

   ```c++
   std::list<int> lst;
   std::mutex mtx;
   void add_to_list(int value) {
     std::lock_guard<std::mutex> guard(mtx);
     lst.push_back(value);
   }
   bool list_contains(int value) {
     std::lock_guard<std::mutex> guard(mtx);
     return std::find(lst.begin(), lst.end(), value) != lst.end();
   }
   ```

2. Condition variable

   Associated with an event or other condition

   One or more threads can wait for that condition

   1. `std::condition_variable`

      Works with `std::mutex`, lightweight, less overhead

   2. `std::condition_variable_any`

      Works with anything mutex-like, additional costs in size & performance

   ```c++
   std::mutex mtx;
   std::condition_variable cond;
   std::queue<data> queue;
   
   // notify
   std::lock_guard<std::mutex> lck(mtx);
   queue.push(data);
   cond.notify_one();
   
   // lambda expression to prevent spurious wake
   std::unique_lock<std::mutex> lck(mtx);
   cond.wait(lck, []{return !queue.empty()});
   ```

## Lecture 03: Atomics & Memory Model in C++

Correctly synchronised program should behave as if memory operations are executed in order that appears equivalent to some sequentially consistent interleaved execution

Modern C++ guarantee sequential consistency for data-race-free programs (SC for DRF)

#### § Memory Model

##### Structure

* Every variable is an object, including those that are members of other objects
* Every object occupies at least one memory location
* Variable of fundamental types occupy exactly one memory location

##### Concurrency

Two thread access the same memory location

* Read-only doesn't need protection or synchronisation
* Either thread is modifying the data: potential for race condition

Data race: two access to a single memory location from separate threads, causes undefined behaviour

* No enforced ordering between access
* One or both accesses are not atomic
* One or both accesses are a write

#### § Modification Order (MO)

Composed of all writes to an object from all threads in the program, MO varies between runs

If object is not atomic: programmer is responsible for synchronisation

If object is atomic: compiler is responsible for syncrhonisation (does not mean race-free)

##### Requirements

1. Once a thread has seen a particular entry in the MO
   - Subsequent reads from that thread must return later values
   - Subsequent writes from that thread must occur later in the MO
2. A read of object that follows a write in the same thread must either
   - Return value written
   - Return another value that occurs later in the MO
3. All threads must agree on MO of each individual object (but do not have to agree on relative order on separate objects)

##### Atomic Types

Used to enforce MO, indivisible atomic operation

Might emulate atomicity using internal mutex, performance gains might not materialise

`is_lock_free()` returns true if operation on a given type are done directly with atomic instructions

##### Memory Order in C++

Each atomic operation has optional memory-ordering argument

1. Sequentially consistent ordering

   `std::memory_order_seq_cst`

   All thread must see the same order of operations

   Operations can't be re-ordered

   Noticeable performance penalty on a weakly-ordered machine

2. Non-sequentially consistent ordering

   Relaxed: `std::memory_order_relaxed`

   Acquire-release: `std::memory_order_acquire` / `std::memory_order_release` / `std::memory_order_acq_rel`

   No single global order of events

   Only requirement is all threads agree on MO of each individual variable

Building blocks of operation ordering:

* Sequenced-before (within same thread)
* Synchronises-with (between atomic load and store)
* Happens-before (same thread or different thread)
* Visible side-effect

Inter-thread synchronisation boils down to preventing data races and defining which side effects become visible under what conditions (by establishing happens-before)

SC for DRF is guaranteed only if every atomic uses `std::memory_order_seq_cst`

##### Fences

A line in the code that certain operations can't cross

`std::atomic_thread_fence` with `std::memory_order_release` prevents all preceding reads and writes from moving past all subsequent stores

## Lecture 04: Testing & Debugging Multithreaded Programs

Guidelines:

* Run smallest amount of code that could potentially demonstrate a problem
* Eliminate concurrency from the test to verify
* Run on a multicore vs single core

Debugging tools:

* Valgrind
* Sanitisers

## Lecture 05: Concurrent Data Structure

#### § Design

##### Goal

- Multiple threads can access the data structure concurrently

- Each thread sees self-consistent view

##### Criteria

- No thread sees invariant broken by actions of another thread during update
- Uphold invariants during exceptions
- Lock at appropriate granularity & level of protection, the smaller the protected regions, the greater the potential for concurrency
- Avoid nested locks to minimise opportunities for deadlock
- Constructors and destructors require exclusive access

##### Alternatives

* Multiple threads perform one type of operation concurrently, another operation requires exclusive access by a single thread
* Multiple threads access data structure concurrently if they are performing different actions

#### § Thread-Safe Stack

```cpp
template<typename T>
class thread_safe_stack {
  private:
  	std::stack<T> data;
    mutable std::mutex m;
  
  public:
    void push(T value) {
      std::lock_guard<std::mutex> lock(m);
      data.push(std::move(value));
    }  
    std::shared_ptr<T> pop() {
			std::lock_guard<std::mutex> lock(m);
      if (data.empty()) { throw empty_stack(); }
      std::shared_ptr<T> res(
        std::make_shared<T>(std::move(data.top()))
      );
      data.top();
      return res;
    }
		void pop(T& value) {
      std::lock_guard<std::mutex> lock(m);
      if (data.empty()) { throw empty_stack(); }
      value = std::move(data.top());
      data.pop();
    }
    bool empty() const {
      std::lock_guard<std::mutex> lock(m);
      return data.empty();
    }
}
```

* Global mutex, lock-based, work is serialised

* No means of waiting for an item to be added, a thread must periodically call `empty()` or `pop()` and catch `empty_stack` exceptions

#### § Thread-Safe Queue

##### Q1. One mutex, notification

```cpp
template<typename T>
class thread_safe_queue {
  private:
  	std::queue<T> data;
	  std::condition_variable cond;
    mutable std::mutex m;
  
  public:
    void push(T value) {
			std::lock_guard<std::mutex> lock(m);
      data.push(std::move(value));
      cond.notify_one();
    }  
    std::shared_ptr<T> wait_and_pop() {
			std::lock_guard<std::mutex> lock(m);
			cond.wait(lock, [this]{ return !data.empty(); });
      std::shared_ptr<T> res(
        std::make_shared<T>(std::move(data.front()))
      );
      data.pop();
      return res;
    }
		void wait_and_pop(T& value) {
      std::lock_guard<std::mutex> lock(m);
      cond.wait(lock, [this]{ return !data.empty(); });
      value = std::move(data.front());
      data.pop();
    }
	  std::shared_ptr<T> try_pop() {
      std::lock_guard<std::mutex> lock(m);
      if (data.empty()) { return std::shared_ptr<T>(); }
      std::shared_ptr<T> res(
        std::make_shared<T>(std::move(data.front()))
      );
      data.pop();
      return res;
    }
  	bool try_pop(T& value) {
      std::lock_guard<std::mutex> lock(m);
      if (data.empty()) { return false; }
      value = std::move(data.front());
      data.pop();
      return true;
    }
    bool empty() const {
      std::lock_guard<std::mutex> lock(m);
      return data.empty();
    }
}
```

If a thread throws exception in `wait_and_pop()`, none of other threads will be woken

Solution:

1. Replace with `cond.notify_all()`, wake up all threads but most will go back to sleep
2. Move `std::shared_ptr<>` initialisation to `push()`, see Q2

##### Q2. Shared pointers

```cpp
template<typename T>
class thread_safe_queue {
  private:
  	std::queue<std::shared_ptr<T>> data;
	  ...
  
  public:
    void push(T value) {
      std::shared_ptr<T> item(
        std::make_shared<T>(std::move(value))
      );
			std::lock_guard<std::mutex> lock(m);
      data.push(item);
      cond.notify_one();
    }  
    std::shared_ptr<T> wait_and_pop() {
			std::lock_guard<std::mutex> lock(m);
			cond.wait(lock, [this]{ return !data.empty(); });
      std::shared_ptr<T> res = data.front();
      data.pop();
      return res;
    }
		void wait_and_pop(T& value) {
      std::lock_guard<std::mutex> lock(m);
      cond.wait(lock, [this]{ return !data.empty(); });
      value = std::move(*data.front());
      data.pop();
    }
	  std::shared_ptr<T> try_pop() {
      std::lock_guard<std::mutex> lock(m);
      if (data.empty()) { return std::shared_ptr<T>(); }
      std::shared_ptr<T> res = data.front();
      data.pop();
      return res;
    }
  	bool try_pop(T& value) {
      std::lock_guard<std::mutex> lock(m);
      if (data.empty()) { return false; }
      value = std::move(*data.front());
      data.pop();
      return true;
    }
		...
}
```

Allocation can be done outside the lock, beneficial for performance

But using one mutex limits concurrency, fine-grained locking needs us to write our own data structure, see Q3

##### Q3. Single-threaded with new structure

```cpp
template<typename T>
class thread_safe_queue {
  private:
  	struct node {
      T data;
      std::unique_ptr<node> next;
      node(T ori_data): data(std::move(ori_data)) {}
    };
    std::unique_ptr<node> front;
    node* back;
    ...
  
  public:
  	thread_safe_queue(): back(nullptr) {}
    void push(T value) {
      std::unique_ptr<node> p(new node(std::move(value)));
      node* const new_back = p.get();
      if (back) {
        back->next = std::move(p);
      } else {
        front = std::move(p);
      }
      back = new_back;
    }  
	  std::shared_ptr<T> try_pop() {
      if (!front) { return std::shared_ptr<T>(); }
      std::shared_ptr<T> res(
        std:make_shared<T>(std::move(front->data));
      );
      std::unique_ptr<node> const old_front = std::move(front);
      if (!front) { back = nullptr; }
      return res;
    }
}
```

Implementing our own `push()` and `try_pop()`

Problem when adding mutex to `front` and `back`:

* `push()` modifies both `front` and `back`
* Troublesome when there is a single item in the queue

##### Q4. Q3 with separated data

```cpp
template<typename T>
class thread_safe_queue {
  private:
    ...
  
  public:
    void push(T value) {
      std::shared_ptr<T> new_data(
        std::make_shared<T>(std::move(value));
      );
			// create dummy node
      std::unique_ptr<node> p(new node);
      back->data = new_data;
      node* const new_back = p.get();
      back->next = std::move(p);
      back = new_back;
    }  
	  std::shared_ptr<T> try_pop() {
      if (front.get() == back) {
        return std::shared_ptr<T>();
      }
      std::shared_ptr<T> const res(front->data);
      std::unique_ptr<node> old_front = std::move(front);
      front = std::move(old_front->next);
      return res;
    }
}
```

Pre-allocate dummy node with no data, ensure there is at least one node in the queue

`push()`: replace data in dummy node

`pop()`: get `old_front`'s `next` node

##### Q5. Q4 with concurrency

```cpp
template<typename T>
class thread_safe_queue {
  private:
    std::mutex front_mutex;
	  std::mutex back_mutex;
  	node* get_back() {
      std::lock_guard<std::mutex> lock(back_mutex);
      return back;
    }
	  std::unique_ptr<node> pop_front() {
      std::lock_guard<std::mutex> lock(front_mutex);
      if (front.get() === get_back()) { return nullptr; }
      std::unique_ptr<node> old_front = std::move(front);
      front = std::move(old_front->next);
      return old_front;
    }
    ...
  
  public:
    void push(T value) {
			std::shared_ptr<T> new_data(
        std::make_shared<T>(std::move(value));
      );
      std::unique_ptr<node> p(new node);
      node* const new_back = p.get();
      std::lock_guard<std::mutex> lock(back_mutex);
      back->data = new_data;
      back->next = std::move(p);
      back = new_back;
    }  
	  std::shared_ptr<T> try_pop() {
      std::unique_ptr<node> old_front = pop_front();
      return old_front
        ? old_front->data
        : std::shared_ptr<T>();
    }
}
```

##### Q6: Q5 with fine-grained lock

```cpp
struct Node {
  std::mutex mut;
  Node *next = nullptr;
  T data{};
};

void push(T value) {
  Node *new_node = new Node{};
  std::scoped_lock lock{mut_back};
  std::scoped_lock node_lock{back->mut};
  back->data = data;
  back->next = new_node;
  back = new_node;
}  

std::optional<T> try_pop() {
  Node *old_node;
  {
    std::scoped_lock lock{mut_front};
    old_node = front;
    std::scoped_lock node_lock{old_node->mut};
    if (old_node->next == nullptr) {
      return std::nullopt;
    }
    front = front->next;
  }
  T data = old_node->data;
  delete old_node;
  return data;
}
```

Each node has its own mutex

#### § Lock-Free Data Structures

##### Blocking/Nonblocking

1. Blocking data structures

   Uses mutexes, condition variables, futures to synchronise

   Blocking library calls will suspend the execution of a thread until another thread performs an action and unblocks

2. Nonblocking data structures

   Do not use blocking library functions

   * Obstruction-free

     If all other threads are paused, then any given thread will complete its operation in bounded steps

   * Lock-free

     One of the thread will complete its operation in bounded steps

   * Wait-free

     Every thread will complete its operation in bounded steps

##### Lock-Free/Wait-Free

1. Lock-free data structures

   More than on thread must be able to access data structure concurrently

   If one thread is suspended, the other threads must still be able to complete their operations without waiting for suspended thread

2. Wait-free data structures

   Must be lock-free and avoids starvation from loops

   Algorithms that involve unbounded number of retries are not wait-free

##### Pros of Lock-Free

1. Enable maximum concurrency

   Some thread make progress with every step

2. Robustness

   If a thread dies halfway through an operation, other threads can proceed normally

##### Cons of Lock-Free

1. Possible livelocks

2. Decrease overall performance

   Atomic operations can be much slower, hardware must synchronise atomic variables

## Lecture 06: Concurrency in Go

Abstractions in GO: coroutines and channels

#### § Concurrency: Goroutines

A function running independently in the same address space as other go routines

Follows fork-join module, cheaper than threads

Go runtime multiplexes goroutines onto OS threads

No garbage collection for goroutines, need to prevent leaks

```go
// print might not happen
// main goroutine might finish before sayHello
func main() { go sayHello() }
func sayHello() { fmt.Println("hello") }
```

```go
func main() {
  var wg sync.WaitGroup
  for _, word := range []string{"a", "b", "c"} {
    wg.Add(1)
    // closure, might print all c
    go func() {
      defer wg.Done(); // called when exiting scope
      fmt.Println(word)
    }()
    // pass copy to the closure
    go func(word string) {
      defer wg.Done(); // called when exiting scope
      fmt.Println(word)
    }(word)
  }
  wg.Wait() // join
}
```

#### § Communication: Channels

Reference to a place in memory where the channel resides

```go
// bidirectional
sendRecvChan := make(chan interface{})
// unidirectional
sendChan := make(chan<- interface{})
recvChan := make(<-chan interface{})
```

```go
// blocking
stringChan := make(chan string)
go func() {
  stringChan <- "hello" 
}()
// if ok: value generated by write
// if not ok: default value from closed channel
word, ok := <-stringChan
```

```go
// buffered
chan := make(chan rune, 4)
```

| Operation | Channel state      | Result                                                       |
| --------- | ------------------ | ------------------------------------------------------------ |
| Read      | `nil`              | Block                                                        |
|           | Open and not empty | Read value                                                   |
|           | Open and empty     | Block                                                        |
|           | Closed             | default value, `false`                                       |
|           | Write only         | Compilation error                                            |
| Write     | `nil`              | Block                                                        |
|           | Open and full      | Block                                                        |
|           | Open and not full  | Write value                                                  |
|           | Closed             | `panic`                                                      |
|           | Receive only       | Compilation error                                            |
| Close     | `nil`              | `panic`                                                      |
|           | Open and not empty | Closes, reads succeed until channel is drained, then default value |
|           | Open and empty     | Closes, produces default value                               |
|           | Closed             | `panic`                                                      |
|           | Receive only       | Compilation error                                            |

##### Ownership

Owner is the grouting that instantiates, writes and closes a channel

Owner should encapsulate and expose via a reader channel

Reader channel should know when a channel is closed

Ownership increases safety

##### `select` Statement

Compose channels together

All channels in `select` are considered simultaneously, blocks if none of the channels are ready

Use `default` to do work while waiting

```go
c1 := make(chan interface{}); close(c1)
c2 := make(chan interface{}); close(c2)
var c1Count, c2Count int
// pseudorandom uniform selection
for i := 1000; i>= 0; i-- {
  select {
	  case <-c1: c1Count++
    case <-c2: c2Count++
  }
}
```

#### § Go Memory Model

Within a single goroutine, reads and writes must behave in program order

Execution order observed by one goroutine may differ from the order perceived by another

Exit of a goroutine is not guaranteed to be synchronised before any event in the program

Send on a channel is synchronised before the completion of corresponding receive from that channel

```go
var c = make(chan int, 10); var a string
func f() { a = "hello world"; c <- 0 }
func main() {
  go f(); <-c; print(a) // always print hello world
}
```

Receive from an unbuffered channel is synchronised before the send on that channel completes

```go
var c = make(chan int); var a string // unbuffered
func f() { a = "hello world"; <-c }
func main() {
  go f(); c <- 0; print(a) // always print hello world
}
```

## Lecture 07: Concurrency Patterns in Go

##### • Confinement

Ad-hoc confinement: data modified from only one goroutine

Lexical confinement: restrict access to shared locations

```go
// only expose reading handle of channel
func chanOwner() <-chan int {
  results := make(chan int, 5)
  go func() {
    defer close(results)
    for i := 0; i <= 5; i++ { results <- i }
  }()
  return results;
}
```

```go
// only expose slice of array
func printData(wg *sync.WaitGroup, data []byte) {
  ...
}
var wg sync.WaitGroup
wg.Add(2)
data := []byte("golang")
go printData(&wg, data[:3])
go printData(&wg, data[3:])
wg.Wait()
```

##### • Preventing Leakage

Goroutines cost resources, need to ensure termination

If a goroutine is responsible for creating a goroutine, it is also responsible in stopping the goroutine

```go
func doWork(strings <-chan string) <-chan interface{} {
  completed := make(chan interface{})
  go func() {
    defer close(completed)
    for s := range strings { // forever blocks due to nil
      fmt.Println(s)
    }
  }()
  return completed
}

doWork(nil)
fmt.Println("done")
```

Remedy: `done` channel

```go
func doWork(
  done <-chan interface{},
  strings <-chan string,
) <-chan interface{} {
  completed := make(chan interface{})
  go func() {
    defer close(completed)
    for {
      select {
      case s := <-strings:
        fmt.Println(s)
      case <-done:
        return
      }
    }
  }()
  return completed
}

done := make(chan, interface{})
completed := doWork(done, nil)
// close done when necessary
```

##### • Error Handling

Stop execution after too many errors

```go
type Result struct {
  Error error
  Response *http.Response
}

done := make(chan interface{})
defer close(done)
errCount := 0
urls := []string{...}
for result := range checkStatus(done, urls...) {
  if result.Error != nil {
    errCount++
    if errCount >= 3 { break }
    continue
  }
}
```

##### • Pipeline

A series of staged connected by channels, separation of concerns

Each stage, goroutines receive values from upstream, perform some function, send values downstream

Each stage has any number of inbound and outbound channels except the first and last stage

Efficient, but not obviously faster than task pool

```go
func generator(
  done <-chan interface{},
  integers ...int,
) <-chan int {
  intStream := make(chan int)
  go func() {
    defer close(intStream)
    for _, i := range integers {
      select {
      case <-done: return
      case intStream <- i:
      }
    }
  }()
  return intStream
}

func multiply||add(
  done <-chan interface{},
  intStream <-chan int,
  multiplier||additive int,
) <-chan int {
  multStream||addStream := make(chan int)
  go func() {
    defer close(multStream||addStream)
    for i := range integers {
      select {
      case <-done: return
      case multStream||addStream
        <- i * multiplier || i + additive:
      }
    }
  }()
  return multStream||addStream
}

done := make(chan interface{})
defer close(done)
intStream := generator(done, 1, 2, 3, 4)
pipeline := multiply(done, add(done, intStream, 1), 2)
```

##### • Fan-In-Fan-Out

Some stages in pipeline might be slower, but does not rely on values that the stage had calculated before

Fan out to start multiple goroutines to handle input (preferable `runtime.NumCPU()` goroutines)

Fan in to combine multiple results into one channel

```go
func fanIn(
  done <-chan interface{},
  channels ...<-chan interface{},
) <-chan interface{} {
  var wg sync.WaitGroup
  multiplexedStream := make(chan interface{})
  multiplex := func(c <- chan interface{}) {
    defer wg.Done()
    for i := range c {
      select {
      case <- done: return
      case multiplexedStream <- i;
      }
    }
  }
  // select from all channels
  wg.Add(len(channels))
  for _, c := range channels {
    go multiplex(c)
  }
  go func() {
    wg.Wait()
    close(multiplexedStream)
  }()
  return multiplexedStream
}
```

```go
done := make(chan interface{})
defer close(done)
// fan out
numFinders := runtime.NumCPU()
finders := make([]<-chan interface{}, numFinders)
for i := 0; i < numFinders; i++ {
  finders[i] = primeFinder(done, randIntStream)
}
for prime := range fanIn(done, finders...) {
  fmt.Println(prime)
}
```

## Lecture 08: Classical Synchronisation Problems

#### § Barrier

##### C++

`std::latch`: single use barrier

`std::barrier`: reusable barrier

```cpp
struct Barrier {
  std::ptrdiff_t expected;
  std::ptrdiff_t count;
  std::mutex mut;
  std::counting_semaphore<> turnstile1;
  std::counting_semaphore<> turnstile2;
  
  Barrier(std::ptrdiff_t expected):
  	expected{expected}, count{0}, mut{},
  	turnstile1{0}, turnstile2{1} {}
  
  void arrive_and_wait() {
    {
      std:scoped_lock lock{mut};
      count++;
      if (count == expected) {
        turnstile2.acquire();
        turnstile1.release(expected);
      }
    }
    turnstile1.acquire();
    {
      std:scoped_lock lock{mut};
      count--;
      if (count == 0) {
        turnstile1.acquire();
        turnstile2.release(expected);
      }
    }
    turnstile2.acquire();
  }
}
```

##### Go

```go
type Barrier struct {
  wg1 sync.WaitGroup
  wg2 sync.WaitGroup
}

func (b *Barrier) Init(expected int) {
  b.wg1.Add(expected)
  b.wg2.Add(expected)
}

func (b *Barrier) Wait() {
  b.wg1.Done()
  b.wg1.Wait()
  b.wg1.Add(1)
  
  b.wg2.Done()
  b.wg2.Wait()
  b.wg2.Add(1)
}
```

#### § Dining Philosophers

##### C++

Limited eater

Possible starvation due to C++'s unfair muteness and semaphores

```cpp
template <size_t numP>
struct DiningTable {
  using CStick = std::mutex;
  CStick cSticks[numP];
  std::counting_semaphore<> footman_sem{numP - 1};
  
  CStick& get_left_cstick(size_t pid) {
    return cSticks[pid];
  }
  CStick& get_right_cstick(size_t pid) {
    return cSticks[(pid + 1) % numP];
  }
  
  void eat(size_t pid, void (*callback)(size_t pid)) {
    footman_sem.acquire();
    std::scoped_lock left_lock{get_left_cstick(pid)};
    std::scoped_lock right_lock{get_right_cstick(pid)};
    callback(pid);
    footman_sem.release();
  }
}
```

##### Go

Odd-even ring communication

```go
type CStick struct {}
type DiningTable struct {
  numPhilosophers int
  cStickChs 			[]chan CStick
}

func (t *DiningTable) Init(numPhilosophers int) {
  t.numPhilosophers = numPhilosophers
  t.cStickChs = make([]chan CStick, 0, numPhilosophers)
  for i := 0; i < numPhilosophers; i++ {
    cStickCh = make(chan CStick, 1)
    cStickCh <- CStick{}
    t.cStickChs = append(t.cStickChs, cStickCh)
  }
}

func (t *DiningTable) leftCStickCh(pid int) chan CStick {
  return t.cStickChs[pid]
}
func (t *DiningTable) rightCStickCh(pid int) chan CStick {
  return t.cStickChs[(pid + 1) % t.numPhilosophers]
}
func (t *DiningTable) oddCStickCh(pid int) chan CStick {
  if pid % 2 == 0 { return t.leftCStickCh(pid) }
  else { return t.rightCStickCh(pid) }
}
func (t *DiningTable) evenCStickCh(pid int) chan CStick {
  if pid % 2 == 1 { return t.leftCStickCh(pid) }
  else { return t.rightCStickCh(pid) }
}

func (t *DiningTable) Eat(pid int, callback func(pid int)) {
  oddCStickCh = t.oddCStickCh(pid)
  evenCStickCh = t.evenCStickCh(pid)
 
  breakLock: // swap order to avoid deadlock
  for {
	  <-oddCStickCh
    select {
    case <-evenCStickCh:
      break breakLock
    default:
    }
    oddCStickCh <- CStick{}
    
	  <-evenCStickCh
    select {
    case <-oddCStickCh:
      break breakLock
    default:
    }
    evenCStickCh <- CStick{}
  }
  
  callback(pid)
  
  oddCStickCh <- CStick{}
	evenCStickCh <- CStick{}
}
```

Limited eater

```go
type DiningTable struct {
  numPhilosophers int
  cStickChs 		  []chan CStick
  footman 				chan struct{}
}

func (t *DiningTable) Init(numPhilosophers int) {
  ... // same as odd-even ring
  t.footman = make(chan struct{}, numPhilosophers - 1)
  for i := 0; i < numPhilosophers - 1; i++ {
    t.footman <- struct{}{}
  }
}

func (t *DiningTable) Eat(pid int, callback func(pid int)) {
  leftCStickCh = t.leftCStickCh(pid)
  rightCStickCh = t.rightCStickCh(pid)
 
  <-t.footman
  
  select {
  case <-leftCStickCh:
    <-rightCStickCh
  case <-rightCStickCh:
    <-leftCStickCh
  }
  
  callback(pid)
  
  oddCStickCh <- CStick{}
	evenCStickCh <- CStick{}
  t.footman <- struct{}{}
}
```

#### § Barber Shop

In the implementations below, customers might not get served in FIFO order

##### C++

```cpp
template <size_t MaxCustomers>
struct BarberShop {
  size_t customers;
  std::mutex mut;
  std::counting_semaphore<> customer_sem;
  std::counting_semaphore<> barber_sem;
  std::counting_semaphore<> done_customer_sem;
  std::counting_semaphore<> done_barber_sem;
  
  void barber(void (*cutHair)()) {
    while (true) {
      customer_sem.acquire();
      barber_sem.release();
      cutHair();
      done_customer_sem.acquire();
      done_barber_sem.release();
    }
  }
  
  void customer(void (*leave)(), void (*getHairCut)()) {
    {
      std::scoped_lock lock{mut};
      if (customers == MaxCustomers + 1) {
        leave();
        return;
      }
      customers++;
    }
    
    customer_sem.release();
    barber_sem.acquire();
    getHairCut();
    done_customer_sem.release();
    done_barber_sem.acquire();
    
    {
      std::scoped_lock lock{mut};
      customers--;
    }
  }
}
```

##### Go

```go
type BarberShop struct {
  chairs					chan int
  barberAvailable chan struct{}
}

func (bs *BarberShop) Init(numChairs int) {
  bs.chairs = make(chan int, numChairs)
  bs.barberAvailable = make(chan struct{})
}

func (bs *BarberShop) Barber(cutHair func()) {
  for {
    bs.barberAvailable <- struct{}{}
    <-bs.chairs
    cutHair()
  }
}

func (bs *BarberShop) Customer() {
  select {
  case bs.chairs <- 1:
    <-bs.barberAvailable
    getHairCut()
  default:
    leave()
  }
}
```

## Lecture 09: Safety in Concurrent Programming with Rust

#### § Ownership

Safety issues in C++:

* Dangling pointers
* Aliased pointers: point to the same chunk of memory
* Mutation: changing a pointer

Solution: ownership & borrowing

Ownership prevents double-free, borrowing prevents use-after-free

There is **<u>NO</u>** data race in Rust

##### Giving Ownership

Default in Rust, deep copy of data is explicit using `clone()`

```rust
fn publish(book: Vec<String>) {
  ... // runs destructor for book
}

fn main() {
  let mut book = Vec::new(); // owner: main function
  book.push(...);
  publish(book); // ownership given to publish
  publish(book); // compilation error
}
```

##### Shared Borrow

Allows aliasing, but no mutation

"Don't break your friend's toys"

```rust
fn publish(book: &Vec<String>) {
  ...
}

fn main() {
  let mut book = Vec::new();
  book.push(...);
  publish(&book);
  publish(&book); // shared reference, successful
}

fn example() {
  let mut vector = Vec::new();
  ...
  let elem = &vector[0];
	// mutation not allowed while shared borrow exists
  vector.push(item);
}
```

##### Mutable Borrow

"It's my turn now"

```rust
fn publish(book: &mut Vec<String>) {
  book.push(...);
}

fn main() {
  let mut book = Vec::new();
  book.push(...);
  {
    let r = &mut book;
		// cannot access while mutable borrow exists
    book.len();
  }
  publish(&mut book); // okay
}
```

#### § Concurrency

Not built into the language, library-based

```rust
// example is a JoinHandle, will be detached if dropped
let example = thread::spawn(|| { "world" });
println!("hello, {}", example.join().unwrap());
```

`move`: closure will take ownership of the values it uses from the environment

```rust
let mut vector = Vec::new();
thread::spawn(move || { vector.push(item); })
vector.push(item); // error
```

Without `move`: possible use-after-free, value does not live long enough

```rust
let mut vector = Vec::new();
thread::spawn(|| {
  vector.push(item); // error
})
vector.push(item);
```

Scoped threads:

```rust
fn main() {
  let v = vec![1, 2, 3];
  std::thread::scope(|scope| {
    scope.spawn(|inner_scope| {
      println!("{:?}", v);
    });
  }).unwrap();
  println!("vector v is back: {:?}", v);
}
```

#### § Traits

Interface to implement for a given type, might have methods

Marker traits:

* `Send`: transferred across thread boundaries

* `Sync`: safe to share references between threads

  Type `T` is `Sync` $\Leftrightarrow$ `&T` is `Send`

* `Copy`: safe to `memcpy`

#### § Reference Counting

`Rc<T>`: no `Send` trait, cannot be sent across threads

`Arc<T>`: allows shared references but no mutation, gets destructed when reference count goes to 0

```rust
let v1 = Arc::new(vec![1, 2]);
let v2 = v1.clone();
thread::spawn(move || {
  // will throw error if v1 is defined with Rc
  println!("{}", v1.len());
});

```

#### § Synchronisation

##### Mutex

Based on ownership, `lock` returns a guard

```rust
fn increment(counter: &Mutex<i32>) {
  let mut data Guard<i32> = counter.lock();
  *data += 1;
}
```

##### Atomics

Ordering of memory operations: `SeqCst`

```rust
let number = AtomicUsize::new(10);
let prev = number.fetch_add(1, SeqCst);
assert_eq!(number.load(SeqCst), 2);
```

##### Channel

```rust
let (tx1, rx) = mpsc::channel();
let tx2 = tx.clone();
thread::spawn(move || tx1.send(5));
thread::spawn(move || tx2.send(4));
```

## Lecture 10: Asynchronous Programming in Rust

##### Closures

Can be called like a function

Can be passed as parameters and returned to and from functions

##### Non-Blocking I/O

Enables concurrency with one thread

`epoll`: kernel-provided mechanism that notifies what file descriptors are ready for I/O

##### Futures

Allow us to keep track of in-progress operations with associated state in one package

Event loop is the runtime for futures, keeps polling until it is ready

When `poll()` is called, `Context` structure passed in, which includes a `wake()` function

After `wake()` is called, executor use `Context` to see which `Future` can be polled to make new progress

Futures should **<u>NOT</u>** block

```rust
trait Future {
  type Output;
  fn poll(
    &mut self, cx: &mut Context
  )	-> Poll<Self::Output>;
}

enum Poll<T> {
  Ready(T),
  Pending,
}
```

##### Async Await

```rust
fn addToInbox(
  email_id: u64, recipient_id: u64
) -> impl Future<Output=Result<(), Error>> {
  loadMessage(email_id)
  	.and_then(|message| get_recipient(message, recipient_id))
  	// should not need message here, but need to pass down
  	.map(|(message, recipient| recipient.verifyHasSpace(&message))
  	.and_them(|(message, recipient)| recipient.addToInbox(message))
}
```

Code above works, better than manually dealing with callbacks and state machines, but

1. Clunky syntax
2. Code becomes messier as complexity increases
3. Sharing mutable data is troublesome

Async function returns a `Future`

`.await` waits for a `Future` and gets its value, can be only called in an async function

Async code makes sense when:

1. You need high degree of concurrency
2. Work is primarily I/O bound

Async functions:

* Have no stack
* Have no recursion
* Are nearly optimal in Rust in terms of memory usage and allocations

```rust
async fn addToInbox(
  email_id: u64, recipient_id: u64
) -> Result<(), Error> {
  let message = loadMessage(email_id).await?;
  let recipient = get_recipient(recipient_id).await?;
  recipient.verifyHasSpace(&message)?;
  recipient.addToInbox(message).await
}
```

`main()` now returns a `Future`, need an executor to execute it

```rust
#[tokio::main]
async fn main() {
  ...
}
```
