## Lecture 01: Asymptotic Analysis

$\Theta(g(n)) = O(g(n)) \cap\Omega(g(n))$

$c, n_0, n$ are all positive

##### $O$-notation

$\exists c, n_0$ such that $\forall n \geq n_0, \, 0 \leq f(n) \leq cg(n)$

##### $\Omega$-notation

$\exists c, n_0$ such that $\forall n \geq n_0, \, 0 \leq cg(n) \leq f(n)$

##### $\Theta$-notation

$\exists c_1, c_2, n_0$ such that $\forall n \geq n_0, \, 0 \leq c_1g(n) \leq f(n) \leq c_2g(n)$

##### $o$-notation

$\exists n_0$ such that $\forall c, \, \forall n \geq n_0, \, 0 \leq f(n) \leq cg(n)$

##### $\omega$-notation

$\exists n_0$ such that $\forall c, \, \forall n \geq n_0, \, 0 \leq cg(n) \leq f(n)$

## Lecture 02: Recurrences

$\displaystyle T(n) = aT\left(\frac{n}{b}\right) + f(n)$

##### Telescoping Method

For any sequence $a_0, a_1, \dots, a_n$, $\displaystyle \sum_{k=0}^{n-1}(a_k - a_{k + 1}) = a_0 - a_n$

Express as $\displaystyle \frac{T(n)}{g(n)} = \frac{T\left(\frac{n}{b}\right)}{g\left(\frac{n}{b}\right)} + h(n)$

##### Recursion Tree

Draw tree, deduce depth and time taken for each level

##### Master Method

Condition: $a \geq 1$, $b > 1$, $f$ is asymptotically positive

Compare $f(n)$ with $n^{\log_ba}$

1. $f(n) = O\left(n^{\log_ba - \varepsilon}\right)$ for some $\varepsilon > 0$

   $f(n)$ grows polynomially slower than $n^{\log_ba}$ by $n^\varepsilon$ factor

   $T(n) = \Theta\left(n^{\log_ba}\right)$

   Note: if $\varepsilon = 0$, it is case 2

2. $f(n) = \Theta\left(n^{\log_ba}\log^kn\right)$ for some $k \geq 0$

   $f(n)$ and $n^{\log_ba}$ grows at similar rates

   $T(n) = \Theta\left(n^{\log_ba}\log^{k + 1}n\right)$

3. $f(n) = \Omega\left(n^{\log_ba + \varepsilon}\right)$ for some $\varepsilon > 0$

   $\displaystyle af\left(\frac{n}{b}\right) \leq cf(n)$ for some $c < 1$

   $f(n)$ grows polynomially faster than $n^{\log_ba}$ by $n^\varepsilon$ factor

   $T(n) = \Theta(f(n))$

##### Substitution Method 

1. Guess the solution
2. Verify by induction

## Lecture 03: Proof of Correctness, Divide & Conquer

#### § Correctness

##### Iterative Algorithms

To show correctness using invariant:

1. Initialisation: invariant is true before first iteration
2. Maintenance: if invariant is true before an iteration, it remains true before the next iteration
3. Termination: when algorithm terminates, invariant provides a useful property for correctness

E.g. insertion sort's invariants:

1. $\texttt{A[1..i] = A'[1..i]}$
2. $\texttt{A[i+2..j] = A'[i+1..j-1]}$
3. All elements in $\texttt{A[i+2..j] > key}$

##### Recursive Algorithms

To show correctness using mathematical induction:

* Use strong induction
* Prove base cases
* Show algorithm works correctly assuming it works correctly for smaller cases

E.g. binary search:

$P(n)$: binary search $\texttt{(A,a,b,x)}$ returns correct answer when $\texttt{b-a+1=n}$

1. Base case: $\texttt{n=0}$, empty array, correctly returns false

2. Inductive step: assume binary search $\texttt{(A,a',b',x)}$ returns correct answer for all $\texttt{0<=j<=n-1, j= b'-a'+1}$

   1. If $\texttt{x==A[mid]}$, clearly it returns correct answer

   2. If $\texttt{x<A[mid]}$, $\texttt{x} \in \texttt{A[a..b]} \Leftrightarrow \texttt{x} \in \texttt{A[a..mid-1]}$ since array is sorted

      By inductive hypothesis, $\texttt{(A,a,mid-1,x)}$ will return correct answer

   3. If $\texttt{x>A[mid]}$, argument is similar to (2)

#### § Divide & Conquer

$\displaystyle T(n) = aT\left(\frac{n}{b}\right) + f(n)$

$a$: number of sub problems

$b$: sub problem size

$f(n)$: time to divide and combine

##### 1. Powering a Number

Compute $f(n, m) = a^n \mod m$

$f(x+y, m) = f(x, m) \times f(y, m) \mod m$

1. Recursively compute $f(n - 1, m)$ and $f(1, m)$

   $T(n) = T(n - 1) + T(1) + \Theta(1) = \Theta(n)$

2. Recursively compute $\displaystyle f\left(\left\lfloor \frac{n}{2}\right\rfloor, m\right)$

   If $n$ is even: $\displaystyle f(n, m) = \left(f\left(\left\lfloor \frac{n}{2}\right\rfloor, m\right)\right)^2 \mod m$

   If $n$ is odd: $\displaystyle f(n, m) = f(1, m) \times \left(f\left(\left\lfloor \frac{n}{2}\right\rfloor, m\right)\right)^2 \mod m$

   $\displaystyle T(n) = T\left(\frac{n}{2}\right) + \Theta(1) = \Theta(\log n)$

##### 2. Matrix Multiplication

$\displaystyle A = \left[a_{ij} \right], \, B = \left[b_{ij} \right], \, C = \left[c_{ij} \right]$

1. Standard algorithm

   $\displaystyle c_{ij} = \sum^n_{k = 1} a_{ik} \cdot b_{kj}, \, \Theta\left(n^3\right)$

2. Divide & conquer algorithm

   For each matrix, divide into 4 sub-matrices of size $n/2$

   8 multiplications, 4 additions

   $\displaystyle T(n) = 8T\left(\frac{n}{2}\right) + \Theta\left(n^2\right)$

   $n^{\log_28} = n^3$, case 1, $\Theta\left(n^3\right)$, no improvement

3. Strassen's algorithm

   7 multiplication, 18 additions

   $\displaystyle T(n) = 7T\left(\frac{n}{2}\right) + \Theta\left(n^2\right)$

   $n^{\log_27} \approx n^{2.81}$, case 1, $\Theta\left(n^{2.81}\right)$

## Lecture 04: Sorting

##### Classification of Sorting Algorithms

1. In-place: uses very little additional memory ($O(1)$ or $O(\lg n)$)
2. Stable: original order of equal elements is preserved
3. Comparison / Non-comparison

#### § Comparison Sort

##### Decision Tree

* Node: comparison

* Branch: outcome of the comparison

* Leaf: class label

E.g. sort $\langle a_1, a_2, \dots, a_n \rangle$:

Each internal node is labelled $i:j$ for $i, j \in \{1, 2, \dots, n\}$

Left subtree: $a_i < a_j$	Right subtree: $a_i \geq a_j$

Leaf: $\langle \pi(1), \pi(2), \dots, \pi(n) \rangle$ to indicate $a_{\pi(1)} \leq a_{\pi(2)} \leq \dots \leq a_{\pi(n)}$ has been established

Tree will contain all possible instructions

Worst-case running time = height of tree

##### Proof for Comparison Sort Lower Bound

Decision tree must contain $\geq n!$ leaves since there are $n!$ permutations

Height-$h$ binary tree has $\leq 2^h$ leaves

Hence, $n! \leq 2^h$
$$
\begin{aligned}
h &\geq \lg(n!) &\text{(lg is monotonically increasing)} \\
&\geq \lg\left(\left(\frac{n}{e}\right)^n\right) &\text{(Stirling's Formula)} \\ 
&= n \lg n - n \lg e \\
&= \Omega(n \lg n)
\end{aligned}
$$

#### § Linear-Time Sort

##### Counting Sort

Input: $A[1..n], \, A[j] \in \{1, 2, \dots, k\}$

Output: sorted $B[i..n]$

Auxiliary storage: $C[1..k]$
$$
\begin{aligned}
&\text{for } i \leftarrow 1 \text{ to } k \\
&\quad\text{do } C[i] \leftarrow 0 \\
&\text{for } j \leftarrow 1 \text{ to } n \\
&\quad\text{do } C[A[j]] \leftarrow C[A[j]] &\text{(count elements)}\\
&\text{for } i \leftarrow 2 \text{ to } k \\
&\quad\text{do } C[i] \leftarrow C[i] + C[i - 1] &\text{(prefix sum)}\\
&\text{for } j \leftarrow n \text{ to } 1 \\
&\quad\text{do } B[C[A[j]]] \leftarrow A[j] &\text{(move back)}\\
&\quad\text{do }C[A[j]] \leftarrow C[A[j]] - 1 &\text{(make it stable)}
\end{aligned}
$$
$\texttt{for}$ loops: $\Theta(k)$, $\Theta(k)$, $\Theta(k)$, $\Theta(n)$, total time complexity $\Theta(n + k)$

if $k = O(n) \to \Theta(n)$

##### Radix Sort

$b$-bit words, $n$ elements

for every $r$-bit pieces, sort the bits using counting sort

Each pass of counting sort takes $\Theta\left(n + 2^r\right)$, total $b/r$ passes

$\displaystyle T(n, b) = \Theta\left(\frac{b}{r}\left(n+2^r\right)\right)$

Optimal: $r = \lg n$, $\displaystyle T(n, b) = \Theta\left(\frac{bn}{\lg n}\right)$

Proof of correctness by induction:

1. Let $P(t)$ be proposition that numbers are sorted by their low-order $t$ digits
2. Base: $P(1)$ is true since sorting is stable
3. Inductive: given $P(t-1)$, after sorting on digit $t$, two numbers equal in digit $t$ are put in same order as input due to stability

Downside: radix sort displays little locality of reference

## Lecture 05: Randomised Algorithms

Algorithms: output and running time are function of the input

Randomised algorithms: output and running time are functions of the input and random bits chosen

##### Worst Case for Quicksort

Input sorted, partition around minimum element

$T(n) = T(0) + T(n - 1) + \Theta(n) = \Theta\left(n^2\right)$

##### Average Case for Quicksort

Assume all elements are distinct, let $e_i$ be the $i^{\text{th}}$ smallest elements of $A$

$\displaystyle A(n)= \frac{1}{n!}\sum_\pi Q(\pi)$

Input is chosen uniformly at random from all $n!$ permutations

$Q(\pi)$: the time complexity and number of comparisons when input is permutation $\pi$

Let $P(i)$ be the set of all permutations that start with $e_i$, $\displaystyle \frac{1}{n}$ permutations constitutes $P(i)$

Let $G(n, i)$ be the average running time over $P(i)$, $\displaystyle A(n) = \frac{1}{n}\sum_{i = 1}^n G(n, i)$

There are exactly $\displaystyle \pmatrix{n - 1 \\ i - 1}$ permutations from $P(i)$ that get mapped to one permutation $S(i)$

$\displaystyle G(n, i) = A(i - 1) + A(n - i) + (n - 1)$

$\displaystyle A(n) = \frac{1}{n}\sum_{i = 1}^n(A(i - 1) + A(n - i) + n - 1)= \frac{2}{n}\sum_{i=1}^n A(i-1) + n - 1$

$\displaystyle \frac{A(n)}{n + 1} - \frac{A(n - 1)}{n} = \frac{4}{n + 1} - \frac{2}{n}$

By telescoping, $A(n) = 2(n + 1) H(n) - 4n$, where $H(n)$ is the harmonic series

$A(n) \approx 1.39 \, n\log_2n$

Pros of quick sort:

1. Less overhead as compared to merge sort
2. More cache hits
3. Chances of deviation from average case decreases

Cons of quick sort:

1. Distribution sensitive

#### § Types of Randomised Algorithms

##### Las Vegas Algorithms

Output is always correct, running time is random

##### Monte Carlo Algorithms

Output may be incorrect, running time is deterministic

1. Matrix multiplication

   Deterministic: multiply and check, $O(n^{2.37})$

   Randomised: Frievalds' Algorithm

   Pick a vector $x \in \{0, 1\}^n$ uniformly and random and check $A \cdot (B \cdot x) = C \cdot x$

   Repeat $k$ times, $O(kn^2)$, error probability $2^{-k}$

2. Minimum cut

   Deterministic: Stoer and Wagner, $O(mn)$

   Randomised: Karger

   $O(m \log n)$, error probability $n^{-c}$

#### § Balls into Bins

$m$ balls, $n$ bins, each ball selects its bin independently and uniformly at random

Size of sample space: $S = n^m$, for each $w \in S$, $\displaystyle P(w) = \frac{1}{n^m}$

Define $E_j^i$ as $j^{\text{th}}$ ball falls into $i^{\text{th}}$ bin

$E_j^i$ and $E_j^k$ disjoint, $E_j^i$ and $E_\ell^k$ independent, $E_j^i$ and $E_\ell^i$ independent

$\displaystyle P(i^{\text{th}}\text{ bin empty}) = \left(1 - \frac{1}{n}\right)^m$, $\displaystyle P(\text{specific set of } \ell \text{bins empty}) = \left(1 - \frac{\ell}{n}\right)^m$

Number of empty bins:

1. Useless

   $\begin{aligned} \displaystyle E(X) &= \sum_i i \cdot P(X = i) \\ &= \sum_i i \pmatrix{n \\ i}P(i \text{ bins are empty and rest are nonempty}) \\ &= \sum_i i \pmatrix{n \\ i}\left(1 - \frac{i}{n}\right)^m(1 - p(n - i, m)) \end{aligned}$

2. Linearity of expectations of indicator random variables

   $\displaystyle \begin{aligned} E(X) &= E(X_1 + X_2 + \dots + X_n) \\ &= E(X_1) + E(X_2) + \dots + E(X_n) \\ &= n\left(1 - \frac 1 n \right)^m \end{aligned}$

#### § Randomised Quick Sort

Let $e_i$ be the $i^{\text{th}}$ smallest element of array $A$

$e_i$ and $e_j$ are compared if and only if the first pivot from $S_{ij}$ is either $e_i$ or $e_j$

Let $B_i$ mean that first pivot element selected during randomised quick sort is $e_i$

$\displaystyle P(e_i \text{ compare } e_j) = P(B_i \cup B_j) = \frac{1}{j - i + 1} + \frac{1}{j - i + 1} = \frac{2}{j - i + 1}$

Number of comparisons:

Let $Y_{ij}$ be the indicator random variable where $e_i$ is compared with $e_j$

$\displaystyle E(Y_{ij}) = \frac{2}{j - i + 1}$

$\displaystyle \begin{aligned} E(Y) &= \sum_{i < j} E(Y_{ij}) =\sum_{i = 1}^n \sum_{j = i + 1}^n \frac{2}{j - i + 1} \\ &= 2 \sum_{i = 1}^n \left(\frac 1 2 + \frac 1 3 + \dots + \frac 1 {n - i + 1} \right) + 2n - 2n \\ &<2 \sum_{i = 1}^n \left(1 + \frac 1 2 + \frac 1 3 + \dots + \frac 1 n \right) - 2n \\ &= 2n \ln n - \Omega(n) \end{aligned}$

##### Conclusion

Randomised quick sort runs in $\Theta(n \lg n)$

## Lecture 06: Order Statistics

To find the $i^{\text{th}}$ smallest elements

1. Sort, report $i^{\text{th}}$ smallest element, $\Theta(n \lg n)$
2. Randomised divide-and-conquer, worst case $\Theta(n)$

Proof that selecting  $i^{\text{th}}$ smallest elements requires $\Omega(n)$:

* Suppose we can find within $n$ steps, we can only see at most $n - 1$ elements
* The last unseen element might be the smallest element

##### Worst-Case Linear-Time Order Statistics

$\text{select}(i, n)$

1. Divide $n$ into groups of five, find median of each group
2. Recursively $\text{select}$ median $x$ of $\displaystyle \left\lfloor \frac{n}{5} \right\rfloor$ medians to be pivot
3. Partition around $x$, let $k = \text{rank}(x)$
   1. If $i = k$, return $x$
   2. If $i < k$, recursively $\text{select}$ $i^{\text{th}}$ smallest element in lower part
   3. If $i < k$, recursively $\text{select}$ $(i - k)^\text{th}$ smallest element in upper part

After each iteration $\displaystyle 3 \left\lfloor \frac{n}{10} \right\rfloor$ elements $\leq x$, $\displaystyle 3 \left\lfloor \frac{n}{10} \right\rfloor$ elements $\geq x$, 

For $n \geq 60$, $\displaystyle 3 \left\lfloor \frac{n}{10} \right\rfloor \geq \frac{n}{4}$

$\displaystyle T(n) = T\left(\frac{1}{5} n \right) + T\left(\frac{3}{4} n \right) + \Theta(n)$

## Lecture 07: Amortised Analysis

Amortised analysis guarantees average performance of each operation in the worst case

No probability is involved (different form average-case analysis)

#### § Types of Amortised Analysis

Using binary counter as an example

##### Aggregate Method

Simple, but lacks precision

Original:

$\displaystyle T(n) = \sum_{i=1}^n t(i)$, worst case $t(i) = k \Rightarrow T(n) = O(nk)$

Amortised:

Let $f(i)$ be number of times $i^{\text{th}}$ bit flips

$\displaystyle T(n) = \sum_{i = 0}^{k - 1}f(i), \,f(i) = \frac{n}{2^i} \Rightarrow T(n) < 2n = O(n)$

##### Accounting Method

Charge $i^{\text{th}}$ operation a fictitious amortised cost $c(i)$

Any amount not immediately consumed is stored in the bank

Bank balance must not go negative, and $\displaystyle \forall n, \sum_{i = 1}^n t(i) \leq \sum_{i = 1}^n c(i)$

Charge $2$ for each $0 \to 1$, $1$ used for actual bit setting, $1$ used for resetting $1 \to 0$

Amortised cost for each increment: $2 = O(1)$

Actual cost for $n$ increments: $\leq 2n = O(n)$

##### Potential Method

$\phi(i)$: potential at the end of $i^{\text{th}}$ operation

Conditions to be fulfilled by $\phi$:

1. $\phi(0) = 0$
2. $ \forall i, \, \phi(i) \geq 0$

Amortised cost of $i^\text{th}$ operation: actual cost of $i^\text{th}$ operation $+ \Delta \phi_i$

​	$\Delta \phi_i = \phi(i) - \phi(i - 1)$

Amortised cost of $n$ operations: actual cost of $n$ operations $+ (\phi(n) - \phi(0))$

​	$=$ actual cost of $n$ operations + $\phi(n) \geq$ actual cost of $n$ operations

Select a suitable $\phi$, so that for the costly operation, $\Delta \phi_i$ is negative to an extent that it nullifies the actual cost

For binary counter, let $\phi(i)$ be number of $1$s in the counter, let $l$ be the length of longest suffix with all $1$s

* Actual cost of $i^\text{th}$ increment: $l_i + 1$
* $\Delta \phi_i$: $-l_i + 1$
* Amortised cost of $i^\text{th}$ increment: $2$

#### § Example: Dynamic Table for Only Insertion

Proposed way: when table is full, create table of size $2n$, copy over content

Original analysis: worst-case for one insertion is $O(n)$, worst-case for $n$ insertions is $O(n^2)$

##### Aggregate Method

Let $t(i)$ be the cost of $i^{\text{th}}$ insertion

$t(i) = i$ if $i - 1 = 2^m, \, m \in \Z$	$t(i) = 1$ otherwise

Cost of $n$ insertions = $\displaystyle \sum_{i = 1}^n t(i) \leq n + \sum_{j = 0}^{\log(n - 1)}  \leq 3n = O(n)$

##### Potential Method

Observe that table is at least half full

Let $\phi(i) = 2i - \text{size}(T)$

* When table is not full:

  Actual cost: $1$

  $\Delta \phi_i = \phi(i) - \phi(i - 1) =(2i - l) - (2(i - 1) - l) = 2$

  Amortised cost: $1 + 2 = 3$

* When table is already full:

  Actual cost: $i$

  $\Delta \phi_i = \phi(i) - \phi(i - 1) = 2 - (i - 1) = 3 - i$

  Amortised cost: $i + (3 - i) = 3$

Amortised cost of $n$ insertions: $3n = O(n)$

## Lecture 08: Dynamic Programming

Express the solution recursively

Small number of subproblems but huge overlap

#### § Longest Common Subsequence (LCS)

Subsequence: obtain by removing zero or more elements from the original sequence

Input: $A: a_1, a_2, \dots, a_n$, $B: b_1, b_2, \dots, b_m$

Output: <u>**a**</u> longest sequence $C$ that is both subsequence of $A$ and $B$

##### Brute Force

Check all possible subsequences of $A$ and verify if it is subsequence of $B$

$2^n$ subsequences in $A$, verification takes $O(m)$

Overall time complexity: $O(m 2^n)$

##### Recursive

Let $\text{LCS}(i, j)$ be the LCS of $A[1..i]$ and $B[1..j]$, let $L(i, j)$ be the length of LCS

Base case: $\forall i, j, \, \text{LCS}(i, 0) = \varnothing, \, \text{LCS}(0, j) = \varnothing$

* If $a_i = b_j = \delta$:

  $\text{LCS}(i, j)$ should terminate with $\delta$

  $\text{LCS}(i, j) = \text{LCS}(i - 1, j - 1) \Vert \delta$

  Proof by contradiction, we can create a longer subsequence by simply appending $\delta$ if it is not the LCS

  $L(i, j) = L(i - 1, m - 1) + 1$

* If $a_i \neq b_j$:

  Either $a_i$ or $b_j$ is not the last symbol of $\text{LCS}(i, j)$

  $\text{LCS}(i, j) = \text{LCS}(i - 1, j) \text{ or } \text{LCS}(i, j - 1)$

  $L(i, j) = \max(L(i - 1, j),L(i, j - 1))$

$T(n, m) = T(n - 1, m) + T(n, m - 1) = \pmatrix{n + m \\ n} > 2^n$

##### DP

We were solving same sub-problem multiple times

Only $(n + 1) \times (m + 1)$ sub-problems

Bottom up, memoise values in a table of $m \times n$

Assign row 0 and column 0 with 0

If $a_i = b_j$, $T[i,j] = T[i - 1, j - 1] + 1$

If $a_i \neq b_j$, $T[i,j] = \max(T[i - 1, j], T[i, j - 1])$

Time per table entry: $O(1)$

Total time: $O(nm)$	Total space: $O(nm)$

#### § Knapsack Problem

Input: $(w_1, v_1), (w_2, v_2), \dots, (w_n, v_n)$, $W$

Output: subset $S \subseteq \{1, 2, \dots, n\}$ that maximises $\sum_{i \in S} v_i$ such that $\sum_{i \in S}w_i \leq W$

##### Brute Force

$2^n$ subsets, $O(2^n)$

##### Recursive

Let $m(i, j)$ be max value obtainable using items $[1..i]$ with total weight $\leq j$

Base case: $\forall i, j, \, m(i, 0) = 0, \, m(0, j) = 0$

* If $n^{\text{th}}$ item is taken:

  Sub-problem: $(w_1, v_1), \dots, (w_{n - 1}, v_{n - 1}), \, W - w_n$

* If $n^{\text{th}}$ item is not taken:

  Sub-problem: $(w_1, v_1), \dots, (w_{n - 1}, v_{n - 1}), \, W$

  Proof by contradiction with cut-and-paste argument

##### DP

Bottom-up, memoise values in a table of $m \times W$

Assign row 0 and column 0 with 0

If $w_i \leq j$, $T[i, j] = \max(T[i - 1, j - w_i] + v_i, T[i - 1, j])$

If $w_i > j$, $T[i, j] = T[i - 1, j]$

Time per table entry: $O(1)$

Total time: $O(nW)$	Total space: $O(nW)$

## Lecture 09: Greedy Algorithms

1. Recast the problem so that we make a choice and are left with one sub-problem
2. Prove that there is always <u>**an**</u> optimal solution to the original problem that makes the greedy choice
3. Use optimal sub-structure to show that we can combine optimal solution to the sub-problem

#### § Fractional Knapsack

Input: $(w_1, v_1), (w_2, v_2), \dots, (w_n, v_n)$, $W$

Output: weights $x_1, \dots, x_n$ that maximises $\sum_i v_i \cdot \frac{x_i}{w_i}$, and are subject to $\sum_i x_i \leq W$ and $ \forall j \in [n], \, 0 \leq x_j \leq w_j$

##### Optimal Sub-structure

If we remove $w$ kgs of item $j$ from the optimal knapsack, the remaining load must be optimal knapsack for $W - w$ kgs that can take from $n - 1$ original items and $w_j - w$ kgs of item $j$

Proof (cut-and-paste):

1. Suppose the remaining load is not the optimal knapsack
2. This means there is a knapsack of value $\displaystyle > X - v_j \cdot \frac{w}{w_j}$ with weight $\leq W - w$
3. Combing with $w$ kgs of item $k$ gives knapsack that weighs $\leq W$ and values $> X$, contradiction

##### Greedy Choice Property

Let $j^\ast$ be the item with maximum $v_j/w_j$, there exists <u>**an**</u> optimal knapsack containing $\min(w_{j^\ast}, W)$ kgs of item $j^\ast$

Proof (cut-and-paste):

1. Suppose optimal knapsack contains $x_1 + x_2 + \dots + x_n = \min(w_{j^\ast}, W)$
2. Replace this weight by $\min(w_{j^\ast}, W)$ of item $j^\ast$
3. Total weight stays the same, total value does not decrease, knapsack stays optimal

##### Iterative Algorithm

Sort all items in reverse by $v_i/w_i$

Put $\min(w_j, W)$ in the knapsack

If not done, use optimal sub-structure to solve remaining weight $W - w_j$

Total time: $O(n \log n)$

#### § Huffman Code

Given set $A$ of $n$ alphabets and their frequencies, compute encoding $\gamma$ such that

* $\gamma$ is prefix coding i.e. $\not \exists \,  x, y \in A$  such that $\gamma(x)$ is the prefix of $\gamma(y)$
* Average bit length (ABL) of $\gamma$ is minimum

Use labelled binary tree $T$ that has $n$ leaves:

* Bijective mapping between alphabets and leaves
* Label of path from root to leaf corresponds to prefix code
* $\text{ABL}(\gamma) = \sum_{x \in A}f(x) \cdot \text{depth}(x)$

A optimal prefix coding:

* $T$ must be a full binary tree, every internal node has degree 2
* More/Less frequent alphabets are closer/farther to the root
* Lemma: there exists **<u>an</u>** optimal $\gamma$ in which $a_1$ and $a_2$ are siblings in $T$

##### Algorithm

If $\text{OPT}_\text{ABL}(A) = \text{OPT}_\text{ABL}(A') + f(a_1) + f(a_2)$:

1. If $|A| = 2$, return 1 level binary tree
2. Let $a_1, a_2$ be two alphabets with least frequencies
3. Remove both and create $a'$ where $f(a') = f(a_1) + f(a_2)$
4. Insert $a'$ into $A$, find optimal $T$
5. Replace $a'$ with 1 level binary tree

To prove $\text{OPT}_\text{ABL}(A) = \text{OPT}_\text{ABL}(A') + f(a_1) + f(a_2)$:

* Derive a prefix coding for $A$ from $\text{OPT}(A')$

  Replace $a'$ with one level binary tree of $a_1$ & $a_2$

   $\text{OPT}_\text{ABL}(A) \leq \text{OPT}_\text{ABL}(A') + f(a_1) + f(a_2)$

* Derive a prefix coding for $A'$ from $\text{OPT}(A)$

  Replace $a_1$ & $a_2$ (siblings) with $a'$

   $\text{OPT}_\text{ABL}(A') \leq \text{OPT}_\text{ABL}(A) - f(a_1) - f(a_2)$

Building heap with frequencies as key: $O(n \log n)$

Removal & insertion: $O(\log n)$

Overall: $O(n \log n)$

#### § Longest Increasing Subsequence (LIS)

##### Naive

1. Sort the numbers ($O(n \log n)$)
2. Use DP to find LCS between sorted sequence and original sequence ($O(n)$)

##### Patience Solitaire

Length of LIS $=$ minimum number of piles in a valid game

1. Place card on the leftmost allowed pile with binary search
2. If no allowed pile, create new pile

## Lecture 10: Reductions & Intractability

There is a polynomial time reduction from $A$ to $B$ ($A \leq_p B$) if:

1. $\alpha$ is a $\text{YES}$-instance for $A$ $\Leftrightarrow$ $\beta$ is a $\text{YES}$-instance for $B$ 
2. Transformation takes $p(n)$ time in the size of $\alpha$

If $B$ is easily solvable, so is $A$ | If $A$ is hard, so is $B$

Runtime must be polynomial in the length of encoding of problem instance

E.g. $\text{Knapsack}$ vs $\text{Fractional Knapsack}$

* Input: $O(n \log M + \log W)$

* $\text{Knapsack}$: $O(nW)$, not polynomial

* $\text{Fractional Knapsack}$: $O(n \log n)$, polynomial 

Decision problem: function that maps an instance to a solution set $\{\text{YES},\text{NO}\}$

Given an optimisation problem, we can convert it into decision problem

If we cannot solve decision problem quickly, we cannot solve optimisation problem quickly

##### Vertex Cover & Independent Set

Given undirected graph $G = (V, E)$

$X \subseteq V$ is vertex vertex cover if $\forall \, (u, v) \in E, \, u \in X \lor v \in X$

​	Optimisation: compute vertex cover of smallest size

​	Decision: does there exist a vertex cover of size $\leq k$?

$X \subseteq V$ is independent set if $\forall \, u, v \in X, \, (u, v) \not \in E$

​	Optimisation: compute independent set of largest size

​	Decision: does there exist independent set of size $\geq k$?

To prove $\text{VC} \leq_p \text{IS}$:

1. $X \subseteq V$ is vertex cover $\Rightarrow$ $V \backslash X$ is independent set

   Let $Y = V \backslash X$, consider any two vertices $u, v \in Y$

   $u \not \in X$, $v \not \in X$, if $(u, v) \in E$, then $X$ is not vertex cover

2. $X \subseteq V$ is independent set $\Rightarrow$ $V \backslash X$ is vertex cover

   Let $Y = V \backslash X$, consider any edge $(u, v) \in E$

   At most one of $\{u, v\}$ can be in $X$, at least one of $\{u, v\}$ must be in $Y$

   Hence, $(u, v)$ is covered by $Y$

Corresponding reduction: on input $(G, k)$, $f$ outputs $(G, n - k)$, takes polynomial time

## Lecture 11: NP-Completeness

#### § Definitions

* NP class

  Set of all decision problems with efficient certifier (polynomial time algorithm with output $\{\text{YES}, \text{NO}\}$)

* P class

  Set of all decision problems with efficient algorithm

* NP-complete

  A problem $X$ is NP-complete if $\forall A \in \text{NP}, \, A \leq_p X$

  If any NP-complete problem can be solved in polynomial time, P = NP

#### § Proof of NP Completeness

Let $X$ be a problem which we want to show NP completeness

1. Show that $X \in \text{NP}$
2. Pick problem $A$ which is already known to be NP-complete
3. Show $A \leq_p X$

##### Example: $\textbf{3-SAT} \leq_p \textbf{IS}$

Given an instance $\Phi$ of $\text{3-SAT}$, we construct an instance $(G, k)$ of $\text{IS}$ so that $\Phi$ is satisfiable $\Leftrightarrow$ $G$ has an IS of size $k$

Reduction (clearly polynomial):

1. $G$ contains 3 vertices for each clause, one for each literal
2. Connect 3 literals in a traingle
3. Connect literal to each of its negations
4. Set $k$ as number of clauses

$\Rightarrow$: Suppose $\Phi$ is a $\text{YES}$-instance, take satisfying assignment and select a true literal from each clause, corresponding $k$ vertices form an independent set

$\Leftarrow$: Suppose $(G, k)$ is a $\text{YES}$-instance, let $S$ be the independent set, each of $k$ triangles must contain exactly one vertex, set these literals to true and all clauses are satisfied
