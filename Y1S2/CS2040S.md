## Lecture 02: Java

#### OOP Paradigm:

Abstraction: separate interface, hide implementation

Encapsulation: bundle variables/functions as a class

Inheritance: build new classes by extending existing ones

Polymorphism: same interface, different behaviour based on context

#### Java Principles:

Everything has a type

Everything is an object

Object has:

- State (data, private)
- Behaviour (method, public)

#### Access Control:

| Keyword        | Access                            |
| -------------- | --------------------------------- |
| none specified | within same package               |
| `public`       | everywhere                        |
| `private`      | only same class                   |
| `protected`    | within same package by subclasses |

Always specify the access

Constructor / Interface / Interface methods: almost always be public

#### Interface:

Only method names

`class Object implements Interface`

Implement all functionality

#### Static:

Regular variables/functions: per object

Static variables/functions: per class

#### Initialisation:

Constructor: same name as class

Multiple constructors with different signatures

Instance variable array can set to null first, use constructor to initialise

```java
class Object implements Interface {
  // instance variables
  Type variable = value;
  // constructor
  Object(Type arg) {...}
  // methods
  void do() {...}
}
```

## Lecture 03: Searching

If $T(n) = O(f(n))$, $S(n) = O(g(n))$:

$T(n) = O(f(n)) \leftarrow \forall n > n_0, \, T(n) \leq cf(n)$ 

$T(n) = \Omega(f(n)) \leftarrow \forall n > n_0, \, T(n) \geq cf(n)$

$T(n) = \Theta(f(n)) \leftrightarrow T(n) = O(f(n)) = \Omega(f(n))$

$T(n) + S(n) = O(f(n) + g(n))$

$T(n) * S(n) = O(f(n) * g(n))$

#### Cost Rules:

(Nested) Loops: (#iterations) * (max cost of one iteration)

Sequential: (cost of first) + (cost of second)

If/else: max(cost of first, cost of second)

#### ⦿ Binary Search:

```java
int BinarySearch(A, key, n) {
	begin = 0;
  end = n - 1;
  while (begin < end) {
    mid = begin + (end-begin)/2;
    if (key <= A[mid]) {
      end = mid;
    } else {
      begin = mid + 1;
    }
  }
  return A[begin] == key
    ? begin
    : -1;
}
```

**Precondition:** array is sorted

**Postcondition:** `A[begin] = key` if element in array

**Invariant:**

- `A[begin] <= key <= A[end]`
- `(end - begin) <= n / 2^k` in iteration `k`

**Performance:** $O(\log n)$

#### ⦿ Quick Select:

```java
// paranoid quickselect
void QuickSelect(A, n, k) {
  if (n == 1) { return A[1]; }
  else {
    while (pivot not good enough) {
      pIdx = random(1, n);
      p = partition(A[1:n], n, pIdx);
    }
    if (k == p) {
      return A[p];
    } else if (k < p) {
      return QuickSelect(A[1:p-1], p, k);
    } else {	// k > p
      return QuickSelect(A[p+1:n], n-p k-p);
    }
  }
}
```

**Performance:** $O(n)$

## Lecture 04: Peak Finding

#### ⦿ Peak Finding:

```java
int FindPeak(A, n) {
	if (A[n/2+1] > A[n/2]) {
		findPeak(A[n/2+1:n], n/2);
	} else if (A[n/2-1] > A[n/2]) {
		findPeak(A[1:n/2-1], n/2);
	} else {
		return n/2;
	}
}
```

**Invariant:**

- $\exists$ peak on the recursed half (proof by contradiction)
- $\exists$ peak in `[begin, end]`
- Peak in `[begin, end]` $\to$ peak in `[1, n]`

**Performance:** $T(n) = T(n/2) + \Theta(1) = O(\log n)$

#### ⦿ Steep Peak Finding:

Might recurse on both sides

**Performance:** $T(n) = 2T(n/2) + \Theta(1) = O(n)$

## Lecture 05-07: Sorting

#### Properties:

In-place: only $O(1)$ extra space needed

Stability: preserves order of equal elements

#### ⦿ Bubble Sort:

```java
void BubbleSort(A, n) { // stable
	do {
		for (int j = 0; j < n-1; j++) {
			if (A[j] > A[j+1]) {
				swap(A[j], A[j+1]);
			}
		}
	} while (no swaps);
}
```

**Invariant:** biggest `j` items sorted in final `j` positions

**Performance:**

- Best: $\Omega(n)$ (already sorted)
- Average: $O(n^2)$
- Worst: $O(n^2)$ (`n` iterations)

#### ⦿ Selection Sort:

```java
void SelectionSort(A, n) { // unstable
  for (int j = 0; j < n-1; j++) {
    k = max_pos(A[j:n]);
    swap(A[j], A[k])
  }
}
```

**Invariant:** smallest `j` items sorted in first `j` positions

**Performance:** $\Omega(n^2)$, $O(n^2)$

#### ⦿ Insertion Sort:

```java
void InsertionSort(A, n) { // stable
  for (int j = 1; j < n; j++) {
    key = A[j];
    i = j-1;
    while (i >= 0 && A[i] > key) {
      A[i+1] = A[i];
      i--;
    }
    A[i+1] = key;
  }
}
```

**Invariant:** first `j` items are sorted

**Performance:**

- Best: $\Omega(n)$ (already sorted)
- Average: $O(n^2)$ (`j` needs to move `j/2` slots backward)
- Worst: $O(n^2)$ (inverse sorted)

#### ⦿ Merge Sort:

```java
void MergeSort(A, n) { // stable
  if (n == 1) { return; }
  else {
    X = MergeSort(A[1:n/2], n/2);
    Y = MergeSort(A[n/2+1:n], n/2);
    return Merge(X, Y, n/2);
  }
}
```

**Performance:**

- `Merge(X, Y, n/2)`: $O(n)$
- $n = 1 \to T(n) = \Theta(1)$
- $n > 1 \to T(n) = 2T(n/2) + cn = O(n\log n)$
- $n = 1 \to S(n) = \Theta(1)$
- $n > 1 \to S(n) = 2S(n/2) + n = O(n\log n)$

```java
// better space usage
void MergeSort(A, B, begin, end) {
  if (begin == end) { return; }
  else {
    mid = begin + (end-begin)/2;
    MergeSort(B, A, begin, mid);
    MergeSort(B, A, mid+1, end);
    Merge(A, B, begin, mid, end);
  }
}
```

#### ⦿ Quick Sort:

```java
// paranoid quicksort
void QuickSort(A, n) { // unstable
  if (n == 1) { return; }
  else {
    while (pivot not good enough) {
      pIdx = random(1, n);
	    p = partition(A[1:n], n, pIdx);      
    }
    x = QuickSort(A[1:p-1], p-1);
    y = QuickSort(A[p+1:n], n-p);
  }
}

void partition(A, n, pIdx) {
  pivot = A[pIdx];
  swap(A[1], A[pIdx]);
  low = 2;	high = n + 1;
  while (low < high) {
    while (A[low] < pivot) { low++; }
    while (A[high] > pivot) { high--; }
    if (low < high) { swap(A[low], A[high]); }
  }
  swap(A[1], A[low - 1]);
}
```

```java
// 3-way partitioning
// option 1: two pass, pack duplicates
// option 2: one pass, four regions
void 3wayPartition(A, n, pIdx) {
  pivot = A[pIdx];
  low = 1;	curr = 1;	high = n;
  while (curr < high) {
    if (A[curr] < pivot) {
      low++;
      swap(A[curr], A[low]);
      curr++;
    } else if (A[curr] == pivot) {
      curr++;
    } else {	// A[curr] > pivot
      swap(A[curr], A[high]);
      high--;
    }
  }
}
```

**Invariant:**

- $\forall \texttt{i} \geq \texttt{high}, \, \texttt{A[i]} > \texttt{pivot}$
- $\forall \, 1 < \texttt{j} \leq \texttt{low}, \, \texttt{A[j]} < \texttt{pivot}$

**Performance:**

- `partition`: $O(n)$

- Best: $\Omega(n \log n)$ (choose median element)
- Worst expected for random: $O(n \log n)$
- Worst: $O(n^2)$ (partition sorts one element)

To improve:

- Recurse into smaller half first

- Halt recursion early, insertion sort on small arrays

## Lecture 08-10: Trees

### ⦿ Binary Search Trees:

`left` $<$ `key` $<$ `right`

Shapes of binary tree: $C_n$ (Catalan)

#### Modifying Operations:

**Insert:** $O(h)$

```java
void insert(int value) {
  if (value < key) {
    if (leftTree != null) { leftTree.insert(value); }
    else { leftTree = new TreeNode(value); }
  } else if (value > key) {
    if (rightTree != null) { rightTree.insert(value); }
    else { rightTree = new TreeNode(value); }
  } else { return; }	// key already in the tree
}
```

**Delete:** $O(h)$

Case 1: no children

- Remove `v`

Case 2: 1 child

- Remove `v`
- Connect `v.child` to `v.parent`

Case 3: 2 children

- `x = successor(v)` (`x` has at most 1 child)
- `delete(x) `(recursive)
- Substitute `v` with `x`

#### Query Operations

**Height:**

```java
h(null) = -1;		h(leaf) = 0;
h(v) = max(h(v.left), h(v.right)) + 1;
```

**Search:** $O(h)$ (omitted)

**SearchMin / SearchMax:** $O(h)$ (omitted)

**Successor:** $O(h)$

```java
TreeNode successor() {
  // case 1: has right child, search min in right
  if (rightTree != null) {
    return rightTree.searchMin();
  }
  // case 2: no right child, go up to the first right
  TreeNode parent = parentTree;
  TreeNode child = this;
  while (parent != null && child == parent.rightTree) {
    child = parent;
    parent = child.parentTree;
  }
  return parent;
}
```

**Traversal:** $O(n)$

In: left, self, right		Pre: self, left, right

Post: left, right, self	Level: bottom up

### ⦿ AVL Trees:

#### 1. Augment:

In every node, store `v.height = h(v)`

(alternatively, store height difference)

Update `height` on insert & delete

#### 2. Define Balance:

$\mid \texttt{v.left.height} - \texttt{v.right.height} \mid \leq 1$

BST is balanced if every node balanced

$\texttt{h} \cong \phi \log(n), \, n > 2^{h/2}$

#### 3. Maintain Balance:

**Insert:** ($O(1)$, two rotations)

`v` left heavy, and `v.left` is:

- balanced: `right-rotate(v)`
- left-heavy: `right-rotate(v)`
- right heavy: `left-rotate(v.left)`, `right-rotate(v)`

`v` right heavy, and `v.right` is:

- balanced: `left-rotate(v)`
- right-heavy: `left-rotate(v)`
- left-heavy: `right-rotate(v.right)`, `left-rotate(v)`

**Delete:** ($O(\log n)$ rotations)

1. if `v` has two children, swap with successor
2. `delete(v)`
3. for every ancestor, check balance and rotate if needed

### ⦿  $(a, b)$-Trees:

#### Rule 1: $(a, b)$-child Policy

Root: $[2, b]$ children, $[1, b-1]$ keys

Internal: $[a, b]$ children, $[a-1, b-1]$ keys

Leaf: $[a-1, b-1]$ keys

#### Rule 2: Key Ordering

Sorted keys $v_1, v_2, \dots, v_k$, subtrees $t_1, t_2, \dots, t_{k+1}$

$t_1$: $(-\infty, v_1]$		$t_i, \, i \in[2, k]$: $(v_{i-1}, v_i]$		$t_{k+1}$: $(v_k, \infty)$

#### Rule 3: Leaf Depth

All leaf nodes have same depth, grow root upwards

Max height: $O(\log_an) + 1$

Min height: $O(\log_bn)$

### ⦿ Tries:

**Search:** $O(L)$

**Insert:** $O(L * \text{overhead})$

Faster, but use more space

### ⦿ Order Statistics:

#### 1. Augment:

In every node, store `v.weight = w(v)`

`w(leaf) = 1; w(v) = w(v.left) + w(v.right) + 1;`

Update `weight` on insert & delete

#### 2. Maintain:

$O(1)$ to update `weight` during rotation

#### 3. New Operations:

```java
TreeNode select(k) {
  int rank = left.weight = 1;
  if (k == rank) { return v; }
  else if (k < rank) { return left.select(k); }
  else if (k > rank) { return right.select(k - rank); }
}
```

```java
int order(TreeNode) {
  int rank = node.left.weight + 1;
  while (node != null) {
    if (node.isRightChild()) {
      rank += node.parent.left.weight + 1;
    }
    node = node.parent;
  }
  return rank;
}
```

### ⦿ Interval Trees:

In every node, store `v.max = max(node.right, left.max, right.max)`

**Interval search:** $O(\log n)$

```java
Pair intervalSearch(x) {
  TreeNode c = root;
  while (c != null && !c.contains(x)) {
    if (c.left == null) { c = c.right; }
    else if (x > c.left.max) { c = c.right; }
    else { c = c.left; }
  }
  return c.interval
}
```

- `c = c.right`: no overlap in left subtree
- `c = c.left`: if no overlap in left subtree, no overlap in right subtree either
- Conclusion: search finds overlapping interval if it exists

**All-overlaps search:** $O(k \log n)$

Repeat until no more intervals:

- Search for interval
- Add to list
- Delete interval

Repeat for all intervals in list:

- Add back to tree

### ⦿ Orthogonal Range Searching:

Store all points in the leaves, internal nodes store `left.max`

**Query:** $O(k + \log n)$

`v = findSplit(low, high);` $O(\log n)$

```java
TreeNode findSplit(low, high) {
  TreeNode v = root;
  while (true) {
    if (high <= v. key) { v = v.left; }
    else if (low > v.key) { v = v.right; }
    else { break; }
  }
  return v;
}
```

`leftTraversal(v.left, low, high);`

```java
void leftTraversal(v, low, high) {
  if (low <= v.key) {
    allTraversal(v.right);
    leftTraversal(v.left, low, high);
  } else {
    leftTraversal(v.right, low, high);
  }
}
```

`rightTraversal(v.right, low, high);`

```java
void rightTraversal(v, low, high) {
  if (v.key <= high) {
    allTraversal(v.left);
    rightTraversal(v.right, low, high);
  } else {
    rightTraversal(v.left, low, high);
  }
}
```

## Lecture 11-14: Hashing

$h: U \to \{1\dots m\}$, collision if $h(k_1) = h(k_2)$

$k$: item to map		$i$: number of collisions

Properties of good hash function:

1. $h(k, i)$ enumerates all buckets, permutation of $\{1 \dots m\}$
2. uniform hashing, equally likely to be mapped to every permutation

#### ⦿ Chaining:

$n$ items, $m$ buckets, $\text{load } \alpha = n/m$ 

Insertion: $O(1 + \text{cost}(h)) = O(1)$

Worst case search: $O(n + \text{cost}(h)) = O(n)$

Expected search: $O(\alpha + cost(h)) = O(1)$

Space: $O(m + n)$

#### ⦿ Open Addressing:

##### 1. Linear Probing:

​	$h(k, i) = h(k, 1) + i \text{ mod } m$

##### 2. Double Hashing:

​	$h(k, i) = f(k) + i \cdot g(k) \text{ mod } m$

​	$g(k)$ co-prime to $m \to h(k, i)$ hits all buckets

Insertion / Search: $\displaystyle \leq \frac{1}{1 - \alpha}$

Problems:

- Sensitive to choice of $h$: clustering
- Sensitive to $\alpha$

Remedy:

- Grow and shrink table
- Choose new $m$ and $h$, compute and copy
- Cost: $O(m_1 + m_2 + n)$ (scan, initialise, compute and copy)

Attempts:

1. Increment by 1: $O(n + (n + 1) + n) = O(n)$, $O(n^2)$ overall
2. Double: $O(n + 2n + n) = O(n)$, $O(1)$ for most insertion
3. Square: $O(n + n^2 + n) = O(n^2)$, $O(1)$ for most insertion, insufficient space usage

Solution:

$(n == m) \to m = 2m \, \mid (n < m/4) \to m = m / 2$ 

Amortised cost $T(n)$: $\forall k \in \Z$, cost of $k$ ops $< kT(n)$

With SUHA, insertion amortised $O(1)$, search expected $O(1)$

#### ⦿ Fingerprint:

Store vector of `0/1` bits, does not store key in table

No false -ve, have false +ve

$\displaystyle P(\text{no collision}) = \left( 1 - \frac{1}{m} \right) ^n \approx \left( \frac{1}{e} \right)^{n/m}$

$\displaystyle P(\text{false +ve}) =1 -  \left( \frac{1}{e} \right)^{n/m}$

#### ⦿ Bloom Filter:

Use $k$ hash functions for fingerprint

No false -ve, have false +ve but requires $k$ collsions

$\displaystyle P(\text{collision at one spot})  \approx 1 - \left( \frac{1}{e} \right)^{kn/m}$

$\displaystyle P(\text{false +ve}) = \left(1 - e^{-kn/m}\right)^k$

## Lecture 15-18: Graphs

#### ⦿ Representations:

If graph is dense, use matrix, else use list

##### Adjacency List:

Nodes stored in array, edges stored in linked list

Fast in neighbour query between 2 nodes

Space: $O(V + E)$

##### Adjacency Matrix:

$A[v][w] = 1 \leftrightarrow (v, w) \in E$

Fast in enumerating neighbours and producing any neighbour

Space: $O\left(V^2\right)$

### ⦿ BFS (queue):

Time: $O(V + E)$ (A list)

Each vertex added to `nextFrontier` once

Each edge enumerated once

Parent pointers store shortest path

```java
void BFS(Node[] nodeList, int startNode) {
  boolean[] visited = new boolean[nodeList.length];
  Arrays.fill(visited, false);
  
  int[] parent = new int[nodeList.length];
  Arrays.fill(parent, -1);
  
  Collection<Integer> frontier = new Collection<>();
  frontier.add(startNode);
  
  while(!frontier.isEmpty()) {
    Collection<Integer> nextFrontier = new Collection<>();
    for (Integer v : frontier) {
      for (Integer w : nodeList[v].nbrList) {
        if (visited[w]) { continue; }
        visited[w] = true;
        parent[w] = v;
        nextFrontier.add(w);
      }
    }
    frontier = nextFrontier
  }
}
```

### ⦿ DFS (stack):

Time: $O(V + E)$ (A list), $O\left(V^2\right)$ (A matrix)

Each vertex called by `DFS_visit` once

Each edge enumerated once ($O(E)$ for list, $O(V)$ for matrix)

```java
void DFS(Node[] nodeList) {
  boolean[] visited = new boolean[nodeList.length];
  Arrays.fill(visited, false);
  
  for (start = i; start < nodeList.length; start++) {
    if (visited[start]) { continue; }
    visited[start] = true;
    DFS_visit(nodeList, visited, start);
  }
}

void DFS_visit(Node[] nodeList, boolean[] visited, int startNode) {
  for (Integer v : nodeList[startNode].nbrList) {
    if (visited[v]) { continue; }
    visited[v] = true;
    DFS_visit(nodeList, visited, v);
  }
}
```

### ⦿ Bellman-Ford:

Time: $O(EV)$

BFS finds min hops but not min distance for weight graph

Terminate early if entire sequence of $|E|$ relax has no effect

After $k$ iteration, $k$-hop estimates are correct

Run $|V| + 1$ times to detect negative weight cycle

One pass sufficient for topo sorted DAG

```java
void BellmanFord() {
  for (i = 0; i < V.length; i++) {
    for (Edge e : graph) {
      relax(e);
    }
  }
}

void relax(int u, int v) {
  if (dist[v] > dist[u] + weight(u, v)) {
    dist[v] = dist[u] + weight(u, v);
  }
}
```

### ⦿ Dijkstra's Algorithm (priority queue):

Time: $O((V + E) \log V) = O(E \log V)$ (AVL PQ ops $O(\log V)$)

Each vertex added to PQ once (`insert` / `deleteMin`)

Each edge relaxed once (`relax` / `decreaseKey`)

Does not work with negative edges

```java
void Dijkstra() {
  Graph G;
  IPriorityQueue pq = new PriQueue();
  double[] distTo;
  
  void searchPath(int start) {
    pq.insert(start, 0.0);
    distTo = new double[G.size()];
    Arrays.fill(distTo, INFTY);
    distTo[start] = 0;
    while(!pq.isEmpty()) {
      int w = pq.deleteMin();
      for (Edge e : G[w].nbrList) {
        relax(e)
      }
    }
  }
	
  void relax(int u, int v, int weight) {
    if (distTo[v] > distTo[u] + w) {
      distTo[v] = distTo[u] + weight;
      parent[v] = u;
      if (pq.contains(v)) {
        pq.decreaseKey(v, distTo[v]);        
      } else {
        pq.insert(v, distTo[v]);
      }
    }
  }
}
```

### ⦿ Topological Sort:

Non-unique

#### ⦿ Post-Order DFS:

Time: $O(V+E)$

```java
void post_order_DFS(Node[] nodeList, boolean[] visited, int startNode) {
  for (Integer v : nodeList[startNode].nbrList) {
    if (visited[v]) { continue; }
    DFS_visit(nodeList, visited, v);
    schedule.prepend(v);
  }
}
```

#### ⦿ Kahn's Algorithm:

Time: $O(E \log V)$

```pseudocode
Repeat:
	S = all nodes that have no incoming edges
	add nodes in S to topo-order
	remove all edges adjacent to nodes in S
	remove nodes in S from graph
```

Nodes in priority queue, keys are incoming edges

Removal of min-degree node: $O(\log V)$

For each outgoing edge, decrease key by 1: $E \times O(\log V)$

### ⦿ DAG Paths:

Only works for acyclic

Shortest: topological sort, relax in order

Longest: negate edges, topological sort, relax in order

## Lecture 19: Heaps, Union Find

### ⦿ Heap:

#### Properties:

1. Heap ordering `priority[parent] >= priority[child]`
2. Complete binary tree

Height: $O(\log n)$

#### Operations:

##### ⦿ Insert:

```java
void insert(Priority p, Key k) {
  Node v = tree.add(p, k);
  bubbleUp(v);
}

void bubbleUp(Node v) {
  while (v != null) {
    if (priority(v) <= priority(parent(v))) {
			return;
    }
    swap(v, parent(v));
    v = parent(v);
  }
}
```

##### ⦿ increaseKey:

Update priority, bubble up

##### ⦿ decreaseKey:

Update priority, bubble down

```java
void bubbleDown(Node v) {
  while (!isLeaf(v)) {
    leftP = priority(left(v));
    rightP = priority(right(v));
    maxP = max(leftP, rightP, priority(v));
    if (leftP == max) {
      swap(v, left(v));
      v = left(v);
    } else if (rightP == max) {
      swap(v, right(v));
      v = right(v);
    } else { return; }
  }
}
```

##### ⦿ delete:

Swap with last node, remove last node, bubble down

##### ⦿ extractMax:

Delete root

#### Storing in an Array:

```java
left(x) = 2x + 1;
right(x) = 2x + 2;
parent(x) = floor((x-1)/2);
```

### ⦿ Heap Sort:

Time: $O(n \log n)$

In-place, unstable

#### 1. Unsorted $\to$ Heap

```java
// O(n log n) heapify, A[0..i] is a heap
for (int i = 0; i < n; i++) {
  int value = A[i];
  A[i] = EMPTY;
  heapInsert(value, A, 0, i);
}
```

Base: each leaf is a heap

Recursion: left and right are heaps

```java
// O(n) heapify
// base: each leaf is a heap
// recursion: left and right are heaps
for (int i = n-1; i >= 0; i++) {
  bubbleDown(i, A);
}
```

#### 2. Heap $\to$ Sorted

Time: $O(n \log n)$ ($n$ elements, $O(\log n)$ for `extractMax`)

```java
for (int i = n-1; i >= 0; i--) {
  int value = extractMax(arr);
  arr[i] = value
}
```

### ⦿ UFDS:

#### 1. Quick Find

Two objects are connected if same component ID

Find $O(1)$

```java
boolean find(int p, int q) {
  return (componentId[p] == componentId[q]);
}
```

Union $O(n)$

```java
void union(int p, int q) {
  update = componentId[q];
  for (int i = 0; i < componentId.length; i++) {
    if (componentId[i] == update) {
      componentId[i] = componentId[p];
    }
  }
}
```

#### 2. Quick Union

Two objects are connected if part of same tree

Find $O(n)$

```java
boolean find(int p, int q) {
  while (parent[p] != p) { p = parent[p]; }
  while (parent[q] != q) { q = parent[q]; }
  return p == q;
}
```

Union $O(n)$

```java
void union(int p, int q) {
  while (parent[p] != p) { p = parent[p]; }
  while (parent[q] != q) { q = parent[q]; }
  parent[p] = q;
}
```

#### 3. Weighted Union

Find $O(\log n)$, Union $O(\log n)$

```java
void union(int p, int q) {
  while (parent[p] != p) { p = parent[p]; }
  while (parent[q] != q) { q = parent[q]; }
  if (size[p] > size[q]) {
    parent[q] = p;
    size[p] = size[p] + size[q];
  } else {
    parent[p] = q;
    size[q] = size[p] + size[q];
  }
}
```

#### 4. Path Compression

Find $O(\log n)$, Union $O(\log n)$

```java
int findRoot(int p) {
  root = p;
  while (parent[root] != root) {
    root = parent[root];
  }
  while (parent[p] != p) {
    temp = parent[p];
    parent[p] = root;
    p = temp;
  }
  return root;
}
```

#### 5. Weighted Union with Path Compression

Find $\alpha(m, n)$, Union $\alpha(m, n)$

## Lecture 20-21: MST

##### Properties:

1. No cycles
2. Cut pieces of MST are still MSTs
3. Max weight edge in a cycle is **not in** MST
4. Min weight edge across a cut is **in** MST

##### Directed MST (DAG with one root) $O(E)$:

For every node except the root: add min weight incoming edge

##### Maximum Spanning Tree:

1. Negate every edge, run MST algo
2. Run Kruskal's in reverse

### ⦿ Red-Blue Generic Algorithm:

If a cycle has no red, colour the max weight edge red

If a cut has no blue, colour the min weight edge blue

Repeat until no more edges can be coloured

### ⦿ Prim's Algorithm:

Time: $O(E \log V)$

Prim: pq to order nodes by edge weight

Dijkstra: pq to order nodes by distance

```java
PriorityQueue pq = new PriorityQueue();
for (Node v : Graph.V) { pq.insert(v, INFTY); }
pq.decreaseKey(start, 0);

HashSet<Node> S = new HashSet<>();
S.add(start);

HashMap<Node, Node> parent = new HashMap<>();
parent.put(start, null);

while (!pq.isEmpty()) {
  Node v = pq.deleteMin();
  S.add(v);
  for (Node w : v.nbrList) {
    if (!S.get(w)) {
      pq.decreaseKey(w, weight(v, w));
      parent.put(w, v);
    }
  }
}
```

##### Known Weight Variant $O(V + E) = O(E)$:

With hash table

Inserting/Removing nodes: $O(V)$

decreaseKey: $O(E)$

### ⦿ Kruskal's Algorithm:

Sorting edges: $O(E \log E) = O(E \log V)$

Every edge find & union $O(\log V)$

Time: $O(E \log V)$

```java
Edge[] sortedEdges = sort(Graph.E);
ArrayList<Edge> MSTEdges = new ArrayList<>();
UnionFind uf = new UnionFind(Graph.V);

for (int i = 0; i < sortedEdges.length; i++) {
  Edge e = sortedEdges[i];
  Node v = e.start();
  Node w = e.end();
  if (!uf.find(v, w)) {
    MSTEdges.add(e);
    uf.union(v, w);
  }
}
```

##### Known Weight Variant $O(\alpha E)$:

Put edges in linked lists $O(E)$

For each edge, check whether to add, union two components $O(\alpha)$

### ⦿ Boruvka's Algorithm:

Each Boruvka step $k \to k/2 \implies O(\log V)$ steps

Time: $O((V+E) \log V) = O(E \log V)$

Initial $O(V)$ (store identifier):

- $n$ connected component, one for each node

Boruvka $O(V+E)$:

- For each connected component, add min weight outgoing edge $O(V+E)$ (BFS/DFS)
- Merge components $O(V)$ (update identifier)

### ⦿ 2-opt Steiner Tree:

1. For every pair of required nodes, calculate shortest path
2. Construct new graph on required nodes
3. Run MST on new graph
4. Map new edges back to original graph

$O$: optimal tree	$D$: DFS on $O$

$D'$: $D$ without Steiner nodes	$T$: 2-opt tree

$\text{cost}(T) \leq \text{cost}(D') \leq \text{cost}(D) = 2 * \text{cost}(O)$

## 	Lecture 22-23: Dynamic Programming

##### Properties:

- Optimal sub-structure

- Overlapping sub-problems

##### Approaches:

- Bottom-up: solve smallest problems, combine, solve root problem
- DAG + topological sort: solve in reverse order
- Top-down: start at root, recurse, solve and memoise

#### ⦿ Longest Increasing Subsequence:

Original: $O(V+E) = O(V^2)$, $V$ times, $O(n^3)$

**Sub-problem** $\texttt{S[i]}$: $\texttt{LIS(A[i..n])}$ ending at $\texttt{A[i]}$

**Base:** $\texttt{S[n-1] = 1}$

**Solve:** $\texttt{S[i] = max(S[j]) + 1}$ where $\texttt{j} \in \texttt{E}$

**Time:** $n$ sub-problems, $\texttt{S[i]}$ takes $O(i)$, $O(n^2)$

#### ⦿ Lazy Prize Collecting:

Original: $k$ copies, longest path from super source, $O(kE)$

**Sub-problem** $\texttt{P[v,k]}$: max prize starting from $\texttt{v}$ taking $\texttt{k}$ steps

**Base:** $\texttt{P[v,0] = 0}$

**Solve:** $\texttt{P[v,k] = max(P[w,k-1]} + \texttt{weight(v,w))}$ where $\texttt{w} \in \texttt{v.nbrList()}$

**Time:** $kV$ sub-problems, each takes $O(|\texttt{v.nbrList}|)$, $O(kV^2)$

Tighter bound: $k$ rows, $E$ to solve each row, (examine each edge once), $O(kE)$

#### ⦿ Vertex Cover:

**Sub-problem** $\texttt{S[v,0/1]}$: size of vertex cover in subtree rooted at $\texttt{v}$ if $\texttt{v}$ is (not) covered

**Base:** $\texttt{S[leaf,0] = 0}$, $\texttt{S[leaf,1]= 1}$

**Solve:** $\texttt{S[v,0] = } \sum \texttt{S[w,1]}$ where $\texttt{w} \in \texttt{v.nbrList()}$

$\texttt{S[v,1] = 1+} \sum \texttt{min(S[w,0], S[w,1])}$ where $\texttt{w} \in \texttt{v.nbrList()}$

**Time:** $2V$ sub-problems, each takes $O(V)$, $O(V^2)$

Tighter bound: each edge examined once, $O(V)$

#### ⦿ All Pairs Shortest Paths:

1. Dijkstra's upon query

	Preprocessing: $0$	$q$ queries: $O(q \cdot E \log V)$

2. Dijkstra's upon query, memoise

	Preprocessing: $0$	$q$ queries: $O(V \cdot E \log V)$

3. APSP

	Preprocessing: $O(V^3)$	$q$ queries: $O(q)$

Original: SSSP for every $v$, sparse $O(V^2 \log V)$, unweighted $O(V(E+V))$

DP **(Floyd-Warshall)**:

**Sub-problem** $\texttt{S[v,w,P]}$: shortest path from $\texttt{v}$ to $\texttt{w}$ that only uses intermediate node sin set $\texttt{P}$

**Base:** $\texttt{S[v,w,} \varnothing \texttt{] = weight(v,w)}$

**Solve:**

$\begin{aligned}\texttt{S[v,w,P}_\texttt{n}\texttt{] = min(} & \texttt{S[v,w,P}_\texttt{n-1}\texttt{],} \\ &\texttt{S[v,n,P}_\texttt{n-1} \texttt{] + S[n,w,P}_\texttt{n-1} \texttt{])} \end{aligned}$

**Time:** $O(V^3)$

```java
int[][] APSP(E) {
  int[][] S = new int[V.length][V.length];
  for (int v = 0; v < V.length; v++) {
    for (int w = 0; w < V.length; w++) {
      S[v][w] = E[v][w];
    }
  }
  for (int k = 0; k < V.length; k++) {
    for (int v = 0; v < V.length; v++) {
      for (int w = 0; v < V.length; w++) {
        S[v][w] = min(S[v][w], S[v][k] + S[k][w]);
      }
    }
  }
  return S;
}
```

Variant for transitive closure:

$\texttt{P[v,w,k] = P[v,w,k-1]||(P[v,k,k-1]&&P[k,w,k-1])}$

