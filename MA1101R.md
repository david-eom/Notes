## Chapter 1: Linear Systems & Gaussian Elimination

#### §1.1 Linear Systems (LS) and Their Solutions

$ax + by + cz = d$

If $a, b, c \neq 0$, slanting

$a = 0$: $\parallel$ x-plane; 	$b = 0$: $\parallel$ y-plane; 	$c = 0$: $\parallel$ z-plane

$d = 0$: passes through origin

LS has either:

- No solution **(inconsistent)**

* Exactly one solution
* Infinitely many solutions

#### §1.2 Elementary Row Operations (ERO)

1. $cR_{i}, \space c \neq 0$
2. $R_{i} \leftrightarrow R_{j}$
3. $R_{i}+cR_{j}, \space c \in \R$ (changes $R_{i}$)

Order of ERO matters

**Row equivalence**: obtained by EROs, **same set of solutions**

#### §1.3 Row-Echelon Forms (REF)

**REF properties:**

1. Zero rows at the bottom
2. Leading entry (pivot point) in a lower row further to the right (staircase)

**RREF properties:**

3. Leading entry $= 1$
4. All other entries in pivot column  $= 0$

Many REFs but one unique RREF

The stairs must have the same height

To obtain general solution from REF:

**no. of parameters = non-pivot columns** (no. of varibles - no. of equations)

#### §1.4 Gaussian Elimination (GE)

**GE (reduce augmented matrix to REF by ERO):**

1. Locate leftmost non-zero column
2. Interchange & bring non-zero column to top
3. Add multiples of top row to make leading entries below 0
4. Cover top row, repeat from step 1

**GJE (reduce REF to RREF by ERO):**

5. Multiply a constant to make leading entry 1
6. Bottom to top, add multiples to introduce 0 above leading entries

(Make first row leading entry 1 if possible)

$Ax = b$ has:

1. No solution if
  - Last column of REF is pivot column
  - $b \not \in \text{Col}(A)$ (§4.1)
2. One solution if
	- Every column except last in REF is pivot column
3. Infinitely many solutions if
	- Nnon-pivot column in REF other than last column (hidden variable)
	- No. of variables > no. of non-zero rows in REF

| REF Non-Zero Rows | Solution Parameters | Geometrical Interpretation |
| :---------------: | :-----------------: | :------------------------: |
|         3         |          0          |    intersect at a point    |
|         2         |          1          |    intersect at a line     |
|         1         |          2          |    intersect at a plane    |
|         0         |          3          |             NA             |

#### §1.5 Homogeneous Linear System

LS with all constant terms 0 **($Ax = 0$)**

$x_1 = 0, \space  x_2 = 0, \space \cdots, \space  x_n = 0$ : trivial solution

- HLS has either **only trivial solutions** or **infinitely many solutions in addition**

- HLS with more variables than equations has infinitely many solutions



## Chapter 2: Matrices

#### §2.1 Introduction to Matrices

$(a_{ij})_{m \times n}$: number on $i^{th}$ row $j^{th}$ column in a matrix of $m$ rows $n$ columns

Types of square matrices:

| Types                            | E.g.                                                         | $(i, j)$-entries                                             |
| :------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| diagonal                         | $\begin{pmatrix} x & 0 & 0 \\ 0 & y & 0 \\ 0 & 0 & z \end{pmatrix}$ | $a_{ij} = 0$ whenever $i \neq j$                             |
| scalar                           | $\begin{pmatrix} c & 0 & 0 \\ 0 & c & 0 \\ 0 & 0 & c \end{pmatrix}$ | $a_{ij} =  \begin{cases} 0, & \text{if}\ i \neq j \\ c, & \text{if}\ i = j \end{cases}$ |
| identity<br />**($I_n$)**        | $\begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}$ | $a_{ij} =  \begin{cases} 0, & \text{if}\ i \neq j \\ 1, & \text{if}\ i = j \end{cases}$ |
| zero<br />**($0_{m \times n}$)** | $\begin{pmatrix} 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0\end{pmatrix}$ | $a_{ij} = 0 \space \forall \space i, j$                      |
| upper triangular                 | $\begin{pmatrix} a & b & c \\ 0 & d & e \\ 0 & 0 & f \end{pmatrix}$ | $a_{ij} = 0 \space \forall \space i > j$                     |
| lower triangular                 | $\begin{pmatrix} a & 0 & 0 \\ b & c & 0 \\ d & e & f \end{pmatrix}$ | $a_{ij} = 0 \space \forall \space i < j$                     |
| symmetric<br />**($A = A^T$)**   | $\begin{pmatrix} x & a & b \\ a & y & c \\ b & c & z \end{pmatrix}$ | $a_{ij} = a_{ji} \space \forall \space i, j$                 |

#### §2.2 Matrix Operations

|    Types     |   Notaion    |                     $(i, j)$-entries                     |
| :----------: | :----------: | :------------------------------------------------------: |
|   equality   |   $A = B$    |       $a_{ij} = b_{ij} \space \forall \space i, j$       |
|   addition   |   $A + B$    |             $(a_{ij} + b_{ij})_{m \times n}$             |
| subtraction  |   $A - B$    |             $(a_{ij} - b_{ij})_{m \times n}$             |
|   negative   | $\text{-} A$ |             $(\text{-} a_{ij})_{m \times n}$             |
| scalar multi |     $cA$     |                 $(ca_{ij})_{m \times n}$                 |
| matrix multi | $A \times B$ |       $\displaystyle \sum^{p}_{k=1} a_{ik}b_{kj}$        |
| matrix power |    $A^n$     |                           N.A.                           |
|  transpose   |    $A^T$     | $A = (a_{ij})_{m \times n}, A^T = (a_{ji})_{n \times m}$ |

$A = (a_{ij})_{m \times p}$, $B = (b_{ij})_{p \times n}$, $A \times B$ is an $m \times n$ matrix.

**Properties:**

- **For addition and scalar multi:**
	1. Commutative: $A + B = B + A$
	2. Associative: $(A + B) + C = A + (B + C)$
	3. Additive identity: $0_{m \times n} + A = A$
	4. Additive inverse: $A + (\text{-}A) = 0_{m \times n}$
	5. Distributive: $a(A + B) = aA + aB$
	6. Scalar addition: $(a + b)A = aA + bA$
	7. $(ab)A = a(bA)$
	8. if $aA = 0_{m \times n}$, $a = 0$ or $A = 0$
- **For matrix multi:**
	1. Not commutative: $AB \neq BA$ in general, $AB =0 \nrightarrow A = 0 / B = 0$ 
	2. Associative: $A(BC) = (AB)C$
	3. Distributive: $A(B + C) = AB + AC$
	4. Scalar multi communtative: $c(AB) = (cA)B = A(cB)$
	5. Multiplicative identity: $I_n$ behaves like "1"
	6. Non-zero zero divisor: $\exists \space A \neq 0_{m \times p}, B \neq 0_{p \times n} \space s.t. AB = 0_{m \times n}$
	7. $0_{m \times n}$ behaves like "0"
- **For matrix power:**
	1. $A^rA^s = A^{r+s}, \space A^rA^{-s} = A^{r-s}$
	2. $(AB)^n \neq A^nB^n$
	3. $A^0 = I$
- **For matrix transpose:**
	1. $(A^T)^T = A$
	2. $(A + B)^T = A^T + B^T$
	3. $(cA)^T = cA^T$
	4. $(AB)^T = B^TA^T$ **(swopped)**

#### §2.3 Inverse of Square Matrices

$AA^{-1} = I, \space A^{-1}A = I \rightarrow A^{-1}$ is the inverse of $A$

Invertible (non-singular) matrix has exactly **one inverse**

*(only square matrices have inverse)*

$\begin{aligned} AB &= I \\ CAB &= CI \\ IB &= C \\ B &= C \end{aligned}$

**Check invertibility:**

- RREF = $I$, or;
- REF no zero row (if have, then singular)

Cancellation law (if $A$ invertible)

$AB_1 = AB_2 \rightarrow B_1 = B_2$

$\begin{aligned} AB_1 &= AB_2 \\ A^{-1}AB_1 &= A^{-1}AB_2 \\ IB_1 &= IB_2 \\ B_1 &= B_2 \end{aligned}$

| Matrix              | Intermediate              | Inverse                                  |
| ------------------- | ------------------------- | ---------------------------------------- |
| $aA$                | $(aA)^{-1}$               | $\displaystyle \frac{1}{a}A^{-1}$        |
| $A^T$               | $(A^T)^{-1}$              | $(A^{-1})^T$                             |
| $A^{-1}$            | $(A^{-1})^{-1}$           | $A$                                      |
| $A_1A_2 \cdots A_k$ | $(A_1A_2\cdots A_k)^{-1}$ | $(A_k)^{-1} \cdots (A_2)^{-1}(A_1)^{-1}$ |

#### §2.4 Elementary Matrices

Perform ERO $\rightarrow$ **pre-multiply** square matrix $E$

ERO has reverse $\rightarrow$ $E$ invertible

$E_k \cdots E_2E_1 A = B \rightarrow A = E_1^{-1}E_2^{-1} \cdots E_k^{-1} B$

| ERO                                                          | Elementary Matrix e.g.                                       |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| $cR_{i}$<br />$(cR_2)$                                       | $\begin{pmatrix} 1 & 0 & 0 \\ 0 & c & 0 \\ 0 & 0 & 1 \end{pmatrix}$ |
| $R_{i} \leftrightarrow R_{j}$<br />$(R_2 \leftrightarrow R_3)$ | $\begin{pmatrix} 1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0 \end{pmatrix}$ |
| $R_{i}+cR_{j}$<br />$(R_3 - cR_1)$                           | $\begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ -c & 0 & 1 \end{pmatrix}$ |

(Perform ECO $\rightarrow$ **post-multiply**)

**To find $A^{-1}$:**

1. $(A|I)$, form $n \times 2n$ augmented matrix
2. Make LHS $I$ by GJE ($1 \rightarrow 3$)
3. $(I|A^{-1})$

$\because E_k \cdots E_2E_1 A = I \rightarrow E_k \cdots E_2E_1 I = A^{-1}$

**Let $A$ be a square matrix:**

1. $A$ is invertible
2. $Ax = 0$ has only trivial solution
3. RREF of $A$ is $I$
4. $A$ is a product of $E$
5. $\det(A) \neq 0$ *(§2.4)*
6. rows / columns form basis for $\R^n$ *(§3.6)*
7. $\text{rank}(A) = n, \space \text{nullity}(A) = 0$ *(§4.3)*
8. eigenvalue of $A \neq 0$ *(§6.1)*

| Property          | ==Proof==                                                    |
| ----------------- | ------------------------------------------------------------ |
| $1 \rightarrow 2$ | $Au = 0, \space u = A^{-1} 0 = 0$                            |
| $2 \rightarrow 3$ | $(A | 0)$, every column is pivot column, by GJE, $(R | 0) = I$ |
| $3 \rightarrow 4$ | $E_k \cdots E_2E_1 A = I \rightarrow A = E_1^{-1}E_2^{-1} \cdots E_k^{-1}  I$ |
| $4 \rightarrow 1$ | $E, \space I$ are invertible matrices                        |

==To prove $AB = I \rightarrow BA = I$:==

1. Consider $Bx = 0$ with solution $u$

	$\begin{aligned} Bu &= 0 \\ A  Bu &= A  0 \\ Iu &= 0 \\ u &= 0 \end{aligned}$

2. $2 \rightarrow 1$, $B$ is invertible

3. $\dots BA = I$

#### §2.5 Determinants & Adjoints

##### Determinants

$A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}, \space \det(A) = ad - bc$

For higher-order square matrices **(cofactor expansion)**:

- Along $i^{th}$ row:

	$\det(A) = a_{i1}A_{i1} + a_{i2}A_{i2} + \cdots + a_{in}A_{in}$

- Along $j^{th}$ column:

	$\det(A) = a_{1j}A_{1j} + a_{2j}A_{2j} + \cdots + a_{nj}A_{nj}$

where:

- $A_{ij} = (-1)^{i+j}\det(M_{ij})$: $(i, j)$-cofactor
- $M_{ij}$: sub-matrix by deleting $i^{th}$ row and $j^{th}$ column

==To prove that $A$ is invertible $\leftrightarrow \det(A) \neq 0$:==

1. Let $B$ be RREF of $A$

	$\begin{aligned} E_k \cdots E_2E_1 A &= B \\ \det(E_k \cdots E_2E_1 A) &= \det(B) \\ \det(E_k) \cdots \det(E_2)\det(E_1) \cdot \det(A) &= \det(B) \end{aligned}$

2. Since $\det(E) \neq 0$:

	$A$ invertible $\rightarrow$ $B = I$ $\rightarrow$ $\det(B) = 1$ $\rightarrow$ $\det(A) \neq 0$

	$A$ not invertible $\rightarrow$ $B$ has zero row $\rightarrow$ $\det(B) = 0$ $\rightarrow$ $\det(A) = 0$

==To prove that $\det(AB) = \det(A)\det(B)$:==

1. If $A$ is singular

	$\det(A) = 0, \space \det(A)\det(B) = 0$

	$AB$ is also singular

	$\det(AB) = 0 = \det(A)\det(B)$

2. If $A$ is invertible

	$A = E_1 E_2 \cdots E_k$ (product of $E$)

	$\begin{aligned} \det(AB) &= \det(E_1 E_2 \cdots E_kB) \\ &= \det(E_1)\det(E_2) \cdots \det(E_k)\det(B) \\ &= \det(E_1 E_2 \cdots E_k)\det(B) \\ &= \det(A)\det(B) \end{aligned}$



| ERO                                                       | ECO                                                       | $\bold{det(E)}$ | $\bold{det} (*= \det(E))$   |
| --------------------------------------------------------- | --------------------------------------------------------- | --------------- | --------------------------- |
| $\displaystyle A \xrightarrow{cR_{i}} B$                  | $\displaystyle A \xrightarrow{cC_{i}} B$                  | $c$             | $\det(B) = c \cdot \det(A)$ |
| $\displaystyle A \xrightarrow{R_i \leftrightarrow R_j} B$ | $\displaystyle A \xrightarrow{C_i \leftrightarrow C_j} B$ | $-1$            | $\det(B) = -\det(A)$        |
| $\displaystyle A \xrightarrow{R_{i}+cR_{j}} B$            | $\displaystyle A \xrightarrow{C_{i}+cC_{j}} B$            | $1$             | $\det(B) = \det(A)$         |

(Can use ERO make matrix triangular to find $\det$)

==To prove that $\det(B) = \det(A)$ when $\displaystyle A \xrightarrow{R_{i}+cR_{j}} B$:==

​	$\displaystyle A \xrightarrow{R_i = R_j} A'$ (replace $i^{th}$ row by $j^{th}$ row)

​	$k(a_{j1}A_{i1} + a_{j2}A_{i2} + \cdots + a_{jn}A_{in}) = 0$ (cofactor expansion along $i^{th}$ row)
$$
\begin{aligned} A \xrightarrow{R_{i}+cR_{j}} B &= \begin{pmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ \vdots & \vdots & & \vdots \\ a_{i1} + ka_{j1} & a_{i2} + ka_{j2} & \cdots &  a_{in} + ka_{jn} \\ \vdots & \vdots & & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \end{pmatrix}  \\
 det(B) &= (a_{i1} + ka_{j1})A_{i1} + (a_{i2} + ka_{j2})A_{i2} + \cdots + (a_{in} + ka_{jn})A_{in} \\
&= (a_{i1}A_{i1} + a_{i2}A_{i2} + \cdots + a_{in}A_{in}) + k(a_{j1}A_{i1} + a_{j2}A_{i2} + \cdots + a_{jn}A_{in}) \\
&= det(A) + k(a_{j1}A_{i1} + a_{j2}A_{i2} + \cdots + a_{jn}A_{in}) \\
&= det(A) + 0
\end{aligned}
$$

**Properties:**

1. $\det(cA) = c^n \det(A) \neq c \cdot \det(A)$ (performing $cR_i$ $n$ times)
2. $\det(AB) = \det(A)\det(B)$ (multiplicative)
3. If $A$ invertible, $\displaystyle \det(A^{-1}) = \frac{1}{\det(A)}$
4. $\det(A + B) \neq \det(A) + \det(B)$
5. $\det(A) = \det(A^T)$ ==(proven by induction)==
6. If $A$ is triangular, $\det(A) = a_{11} \cdot a_{22} \cdots a_{nn}$ 
7. $\det$ for square matrices with two identical rows/columns is 0 ==(proven by induction)==



##### Adjoints

$adj(A) = (A_{ij})^T$, where $A$ is square matrix and $A_{ij}$ is $(i, j)$-cofactor

*(Only co-factor, without $a_{ij}$)*

 For **any** square matrix, $A \space adj(A) = \det(A) \cdot I$

==To prove $\displaystyle A^{-1} = \frac{1}{\det(A)}adj(A)$ for **invertible** square matrix:==
$$
A = \begin{pmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots &  & \vdots \\ a_{n1} & a_{n2} & \cdots & a_{nn}\end{pmatrix}, \space adj(A) = \begin{pmatrix} A_{11} & A_{21} & \cdots & A_{n1} \\ A_{12} & A_{22} & \cdots & A_{n2} \\ \vdots & \vdots &  & \vdots \\ A_{1n} & A_{2n} & \cdots & A_{nn}\end{pmatrix} \\
\begin{aligned}
(i, i)\text{-entry of } A\cdot adj(A) &=
a_{i1}A_{i1} + a_{i2}A_{i2} + \cdots + a_{in}A_{in}\\
\text{diagonal entries} &= \det(A)\\ 
(i, j)\text{-entry of } A\cdot adj(A) &=
a_{i1}A_{j1} + a_{i2}A_{j2} + \cdots + a_{in}A_{jn}\\
\text{non-diagonal entries} &= 0
\end{aligned}
\\
\begin{aligned}
A\cdot adj(A) &= \begin{pmatrix} \det(A) & 0 & \cdots & 0 \\ 0 & \det(A) & \cdots & 0 \\ \vdots & \vdots &  & \vdots \\ 0 & 0 & \cdots & \det(A)\end{pmatrix} = \det(A) \cdot I \\
A [\frac{1}{\det(A)}adj(A)] &= I \\
A^{-1} &= \frac{1}{\det(A)}adj(A)
\end{aligned}
$$


Cramer's Rule:

$Ax = b$, invertible $A_{n \times n}$ 

$A_i$:  obtained by replacing $i^{th}$ column of $A$ by $b$

LS has a unique solution:

$\displaystyle x = \frac{1}{\det(A)}\begin{pmatrix} \det(A_1) \\  \det(A_2) \\ \vdots \\  \det(A_n)\end{pmatrix}$

==To prove Cramer's Rule:==
$$
\begin{aligned} Ax &= b \\
x &= A^{-1}b \\
x &= \frac{1}{\det(A)}adj(A)b \\
\begin{pmatrix} x_1 \\  x_2 \\ \vdots \\  x_n \end{pmatrix} &= \frac{1}{\det(A)} \begin{pmatrix} A_{11} & A_{21} & \cdots & A_{n1} \\ A_{12} & A_{22} & \cdots & A_{n2} \\ \vdots & \vdots &  & \vdots \\ A_{1n} & A_{2n} & \cdots & A_{nn}\end{pmatrix} \begin{pmatrix} b_1 \\  b_2 \\ \vdots \\  b_n\end{pmatrix} \\
x_i &= \frac{1}{\det(A)}(b_1A_{1i} + b_2A_{2i} + \cdots + b_nA_{ni}) \\
&= \frac{\det(A_i)}{\det(A)}
\end{aligned}
$$

## Chapter 3: Vector Spaces

#### §3.1 Euclidean n-Spaces

Vector represents point or arrow

$n$-vector: $(u_1, u_2, \cdots , u_n)$, row or column vector

Set of all $n$-vectors of $\R$ is $\R^n$

Subset of $\R^n$:

- Implicit form:

	$\{ v \in \R^n \space | \space \text{conditions} \}$

- Explicit form

	$\{ \text{explicit tuple} \space | \space \text{parameters} \}$

*e.g.* line as subset of $\R^3$ (intersection of two planes)

$\{ (x, y, z) \space | \space \text{equation of line} \}$

$\{ \text{general solution} \space | \space \text{1 parameter} \}$

$\mid S \mid$: number of elements in finite set $S$

#### §3.2 Linear Combinations and Linear Spans

##### Linear combination

$c_1u_1 + c_2u_2 + \cdots + c_ku_k$, $u \in \R^N$, $c \in \R$

Vector can be expressed as LC if LS is consistent

Every vector is $\R^3$ is a LC of standard basis vectors

$e_1 = (1, 0, 0), \space e_2 = (0, 1, 0), \space e_3 = (0, 0, 1)$

##### Linear span

Set of all LC

For a finite subset of $\R^n$: $S = \{u_1, u_2, \dots, u_k\}$

Set notation: $\{c_1u_1 + c_2u_2 + \cdots + c_ku_k | c_1, c_2, \dots , c_k \in \R\}$ (can change to explicit)

LS notation: $\text{span}\{S\}$

$S \subseteq \text{span}(S) \subseteq R^n$

==To prove $0 \in \text{span}(S)$ for any $S$:==

$c_1u_1 + c_2u_2 + \cdots + c_ku_k \in \text{span}(S)$

In particular, $0u_1 + 0u_2 + \cdots + 0u_k \in \text{span}(S)$

==To prove any LC of vectors in LS is a vector in LS:==

$v_1, v_2, \dots, v_r \in \text{span}(S)$, $c \in \R$

$\rightarrow v_1, v_2, \dots, v_r$ is LC of $u_1, u_2, \dots, u_k$

$\rightarrow c_1v_1 + c_2v_2 + \cdots + c_kv_k \in \text{span}(S)$

Hence:

**To determine equality of two LS:**

$\text{span}(S_1) \subseteq \text{span}(S_2)$ and $\text{span}(S_2) \subseteq \text{span}(S_1)$

Form LS, check if consistent (RHS is LC of LHS, RHS $\subseteq$ LHS)

**Redundant vector:**

$u \in \text{span}(S) \leftrightarrow \text{span}(S) = \text{span}(S \cup u)$

$u$ is a LC of vectors in $S$



**To determine whether $\text{span} = R^n$:**

$=$ if LS with $(x, y, z)$ is consistent, vice versa

​	(RREF has no zero rows, must be $I$)

$\neq$ if no. of vectors < dimension of space



**Closure properties:**

Addition: $u, v \in \text{span}(S) \rightarrow u + v \in \text{span}(S)$

Scalar multi: $u \in \text{span}(S), c\in \R \rightarrow cu \in \text{span}(S)$

| Objects              | Span                      | Set notation                                                 |
| -------------------- | ------------------------- | ------------------------------------------------------------ |
| line thru origin     | $\text{span}\{u\}$        | $\{tu | t\in \R\}$                                           |
| line not thru origin | $x +\text{span}\{u\}$     | $\{x + tu|t \in \R\}$<br />$\{x + w|w \in \text{span}\{u\}\}$ |
| plane thru origin    | $\text{span}\{u, v\}$     | $\{tu + sv | t, s \in \R\}$                                  |
| plan not thru origin | $x + \text{span}\{u, v\}$ | $\{x + tu + sv | t, s \in \R\}$<br />$\{x + w | w \in \text{span}\{u, v\}\}$ |

#### §3.3 Subspaces

$V \subseteq \R^n$ is a subspace if

1. ==$0 \in V$==
2. ==$\forall u, v \in V, \space \forall \alpha, \beta \in \R, \space \alpha u + \beta v \in V$ (closure)==

Can be expressed in LS $V = \text{span}(S)$

$\{0\}$ is a subspace of $\R^n \because \{0\} = \text{span}\{0\}$

$\R^n$ is a subspace of $\R^n \because \R^n = \text{span}\{e_1, e_2, \dots, e_n\}$

| $\R^2$ subspaces | $\R^3$ subspaces  | LS form                                    |
| ---------------- | ----------------- | ------------------------------------------ |
| $\{0\}$          | $\{0\}$           | $\text{span}\{0\}$                         |
| line thru origin | line thru origin  | $\text{span}\{u\}$<br />non-zero           |
| $\R^2$           | plane thru origin | $\text{span}\{u, v\}$<br />non-parallel    |
|                  | $\R^3$            | $\text{span}\{u, v, w\}$<br />non-coplanar |

**Solution space:**

Solution set of homogeneous LS $Ax = 0$

can be written as LS, is a subspace of $\R^n$



**To show $S \subseteq \R^n$ is a subspace:**

- Express $S$ as LS
- Show $S$ is solution set of a homogenous LS ($0 \in S$)
- Show $S$ is a line/plane thru origin (for $\R^2$, $\R^3$)

**To show $S \subseteq \R^n$ is not a subspace:**

- Show $0 \not \in S$
- Find $u, v\in S, u + v \not \in S$
- Find $v \in S, cv \not \in S$
- Show $S$ is not a line/plane thru origin (for $\R^2$, $\R^3$)

#### §3.4 Linear Independence

$S = \{u_1, u_2, \dots, u_k\} \subseteq \R^n$ is LI if

$c_1u_1 + c_2u_2 + \cdots + c_ku_k = 0$ has only trivial solution:

**To show LI:**

- No vector is LC of other vectors (no redundant vectors)
- REF of $[u_1 \space u_2 \space \cdots \space u_k \space | \space 0]$ no non-pivot columns (only trivial solution) ($\det \neq 0$)
- One vector: non-zero
- Two vectors: not scalar multiples

**To show not LI:**

- $0 \in S$
- $k > n$

==To prove $u_1, u_2, \dots, u_k, u_{k + 1}$ are LI if $u_1, u_2, \dots, u_k$ are LI and $u_{k + 1}$ is not a LC of the vectors (by contradiction)==

Suppose $u_1, u_2, \dots, u_k, u_{k + 1}$ not LI

$c_1u_1 + c_2u_2 + \cdots + c_{k + 1}u_{k + 1} = 0$ has non-trivial solution

1. If $c_{k + 1} = 0$:

	$c_1u_1 + c_2u_2 + \cdots + c_ku_k = 0$

	Contradicts LI of $u_1, u_2, \dots, u_k$

2. If $c_{k + 1} \neq 0$:

	$c_1u_1 + c_2u_2 + \cdots + c_ku_k = -c_{k + 1}u_{k + 1}$

	Contradicts $u_{k + 1}$ not a LC

| To show LS spans $\R^n$                 | To show LI                              |
| --------------------------------------- | --------------------------------------- |
| $c_1u_1 + c_2u_2 + \cdots + c_ku_k = v$ | $c_1u_1 + c_2u_2 + \cdots + c_ku_k = 0$ |
| $v$: general vector                     | $0$: zero vector                        |
| Check LS consistent                     | Check LS trivial solution               |

#### §3.5 Bases

$S = \{u_1, u_2, \dots, u_k\} \subseteq \R^n$ is basis for $\R^n / V$ if

1. $S$ is LI
2. $\text{span}(S) = \R^n / V$

**To show whether basis for $\R^n / V$:**

- Check span

	($\R^n$: general vector, consistency)

	($V$: contain redundant vector)

- Check LI (zero vector, trivial solution)

- $\dim \space +$ either $1$ or $2$ *(§3.6)*

- Form matrix, check invertibility properties *(§3.6, only for $\R^n$)*

Smallest possible no. of vectors that span $\R^n / V$

$\R^n$ has infinitely many bases

Basis for $\R^n$: $n$ elements

Basis for $\{0\}$: $\varnothing$

**To find basis for a subspace:**

- Express explicit form in LC
- Check LI

==To prove uniqueness of expression in terms of basis:==

$v = c_1u_1 + c_2u_2 + \cdots + c_ku_k$

$v = d_1u_1 + d_2u_2 + \cdots + d_ku_k$

$(c_1 - d_1)u_1 + (c_2 - d_2)u_2 + \cdots + (c_k - d_k)u_k = 0$

Given LI, only trivial solution

$c_1 = d_1, c_2 = d_2, \dots, c_k = d_k$, expression is unique

**Coordinate vectors:**

$v = c_1u_1 + c_2u_2 + \cdots + c_ku_k$

$(v)_s = (c_1, c_2, \dots, c_k)$, relative to $S$

If $S$ is standard basis, $(v)_s = v$

**To find coordinate vectors:**

- $v \rightarrow (v)_s$: set up LS
- $(v)_s \rightarrow v$: substitution

**Coordinate vectors properties:**

1. $u = v \leftrightarrow (u)_s = (v)_s$

2. $(c_1v_1 + c_2v_2 + \cdots + c_kv_k)_s$ $=$ 
	$c_1(v_1)_s + c_2(v_2)_s + \cdots + c_k(v_k)_s$

	(coordinate vectors of LC = LC of coordinate vectors)

3. $v_1, v_2, \dots, v_r$ are (not) LI $\leftrightarrow$ $(v_1)_s, (v_2)_s, \dots, (v_r)_s$ are (not) LI

4. $\{v_1, v_2, \dots, v_r\}$ is basis for $V$ $\leftrightarrow$ $\{(v_1)_s, (v_2)_s, \dots, (v_r)_s\}$ is basis for $\R^k$ (coordinate space)

#### §3.6 Dimensions

$S = \{u_1, u_2, \dots, u_k\}$ is **==a==** basis for $V$, $\dim(V) = k$

Subset with $> k$ vectors: not LI

Subset with $< k$ vectors: cannot span $V$

$\dim$ of solution space (HLS $\rightarrow$ REF)

​	$=$ no. of non-pivot columns

​	$=$ no. of parameters in general solution

​	$=$ no. of vectors in basis

$\dim(0) = 0$, basis: $\varnothing$

No subspace of $\R^n$ has $\dim(n)$ except $\R^n$ itself

$U \subseteq V \rightarrow \dim(U) \leq \dim(V)$

$U \subsetneq V \rightarrow \dim(U) < \dim(V)$

**To show whether $S$ is basis for $V$ (3 choose 2) (alt 1):**

1. $S$ LI
2. $\text{span}(S) = V$
3. $|S| = \dim(V)$

==To prove $1 \& 3 \rightarrow $ basis (contradiction):==

- Assume $S$ not basis for $V$

	$\text{span}(S) \neq V$

	$\exists u \in V \space (u \not\in \text{span}(S))$ 

	Let $S' = S \cup \{u\}$

	$u$ not redundant, $S'$ is LI

	$k+1$ vectors, $S'$ is not LI, contradiction

==To prove $2 \& 3 \rightarrow $ basis:==

- Assume $S$ not basis for $V$

	There is a redundant vector $v$

	Let $S' = S - \{v\}$

	$v$ redundant, $\text{span}(S'') = V$

	$k-1$ vectors, $\text{span}(S'') \neq V$, contradiction



**To show whether $S$ is basis for (only) $\R^n$ (alt 2):**

- Form matrix (rows or column)
- Check properties in §2.4

Limitation:

- Must be square matrix
- Cannot check for subspaces

==To prove invertible $\rightarrow$ columns are basis:==

​	$2$: LS only trivial solution

​	Coefficients in vector equation form are only zero

​	Vectors are LI, columns form basis for $\R^n$

==To prove invertible $\rightarrow$ rows are basis:==

​	$5$: $\det \neq 0$

​	Transpose $\det \neq 0$

​	Transpose invertible, rows form basis for $\R^n$

#### §3.7 Transition Matrices

Two bases for $V$: $S = \{u_1, u_2, \dots, u_k\}, \space T = \{v_1, v_2, \dots, v_k\}$

**To find transition matrix from ==$S$ to $T$==:**

1. Express each $u_i$ as LC of $T$: $(T|S)$
2. Form column coordinate vectors wrt $T$
3. $P = ([u_1]_T[u_2]_T\cdots[u_k]_T)$
4. $\forall w \in V, \space P[w]_S = [w]_T$

$P$ invertible $\because$ coordinate vectors of LI set are LI

$P^{-1}$ is transition matrix from $T$ to $S$

==To prove $QP = I$:==

 $S = \{u_1, u_2, \dots, u_k\}$

Observations:

$u_i = 0u_1 + \cdots + 1u_i + \cdots + 0u_k$

$(u_i)_S = \begin{pmatrix} 0 & \cdots & 0 & 1 & 0 & \cdots & 0 \end{pmatrix}$

(coordinate vectors of vectors in the set are standard basis)

$i^{th}$ column of $A = A[u_i]_S$

$i^{th}$ column of $QP = QP[u_i]_S = Q[u_i]_T = [u_i]_S$

$\therefore QP = I$

## Chapter 4: Vector Spaces with Matrices

#### §4.1 Row & Column Spaces

$\text{span}$ of rows & columns

$\text{Row}(A) = \text{Col}(A^T)$

RS/CS of $0$: zero space	RS/CS of $I_n$: $\R^n$

$A, \space B$ row equivalent $\leftrightarrow$$\text{Row}(A) = \text{Row}(B)$

**To find basis for RS (or any spanning set):**

May not contain original vectors

1. Reduce to REF
2. Non-zero rows (LI due to staircase)

**To find basis for CS:**

ERO preserve linear dependency

1. Reduce to REF
2. Pivot columns
3. Find corresponding columns

**To find basis for LS:**

Remove redundant

RS method / CS method

**To extend a set to basis for $\R^n$**

Add non-redundant

1. Form matrix using set as rows
2. Reduce to REF
3. Non-pivot columns
4. Form rows with leading entries



$\text{Col}(A) = \{Au | u \in \R^n\}$ (all LC of column vectors)

$Ax = b$ consistent $\leftrightarrow$ $b$ is LC of vectors $\leftrightarrow b \in \text{Col}(A)$ 

#### §4.2 Ranks

$\dim(\text{Row}) = \dim(\text{Col})$

Since no. of non-zero rows = No. of pivot columns

$\text{rank}(A) = $ max no. of LI rows/columns in $A$

$\text{rank}(0) = 0$	$\text{rank}(I_n) = n$

$\text{rank}(A) \leq \text{min}\{m,n\}$

Full rank: $\text{rank}(A) = \text{min}\{m,n\} \leftrightarrow \det(A) \neq 0$

$\text{rank}(A) = \text{rank}(A^T)$

Since $\text{Row}(A) = \text{Col}(A^T)$

$Ax = b$ consistent $\leftrightarrow$ $\text{rank}(A) = \text{rank}(A|b)$

(last column non pivot)

==To prove $\text{rank}(AB) \leq \text{min}\{\text{rank}(A),\text{rank}(B)\}$:==

1. $\text{rank}(AB) \leq \text{rank}(A)$

	$B = (b_1 \space b_2 \space \cdots \space b_p)$

	$AB = (Ab_1 \space Ab_2 \space \cdots \space Ab_p)$

	$Ab_i \in$ $\text{Col}(A) = \{Au | u \in \R^n\}$

	$\text{span}\{Ab_1, \space Ab_2, \space \dots, \space Ab_p\} = \text{Col}(AB) \subseteq \text{Col}(A)$

	$\dim(\text{Col}(AB)) \leq \dim(\text{Col}(A))$

2. $\text{rank}(AB) \leq \text{rank}(B)$

	$\text{rank}(B^TA^T) \leq \text{rank}(B^T)$ (post-multiply)

	$\text{rank}((AB)^T) = \text{rank}(AB) \leq \text{rank}(B)$

#### §4.3 Nullspaces & Nullities

Nullspace: solution space for HLS $Ax = 0$

(all vectors in $\R^n$ killed by $A$)

Nullity: $\dim$ of nullspace

If $A$ has $n$ columns, $\text{rank}(A) + \text{nullity}(A) = n$

(no. of pivot columns + no. of non-pivot columns)

==To prove that solution set of $(Ax = b) = \{u + v | u \in \text{null}(A)\}$ where $v$ is a particular solution:==

$T =$ solution set of $Ax + b$

$S = \{u + v | u \in \text{null}(A)\}$

1. $T \subseteq S$

	Let $w \in T$

	$Aw = b, \space Av = b, \space A(w - v) = 0, \space (w - v) \in \text{null}(A)$

	$w - v = u, \space w = u + v, w \in S$

2. $S \subseteq T$

	$A(u + v) = Au + Av = 0 + Av = b$

$Ax = b$ has one solution $\leftrightarrow$ $\text{null}(A) = \{0\}$

## Chapter 5: Orthogonality

#### §5.1 Inner Product

$u = (u_1, \space u_2, \dots, \space u_n), \space v = (v_1, \space v_2, \dots, \space v_n) \in \R^n$

$u \cdot v = u_1v_1 + u_2v_2 + \cdots + u_nv_n$ (dot/inner/scalar)

$\Vert u\Vert = \sqrt{u \cdot u}$

$\Vert u - v\Vert = \sqrt{(u - v)\cdot(u - v)}$

$\displaystyle \theta = cos^{-1}(\frac{u \cdot v}{\Vert u \Vert \Vert v \Vert})$ (*valid since $|u \cdot v| \leq \Vert u \Vert  \cdot \Vert v \Vert$*)

$u \cdot v = uv^T / u^Tv$ *(regarded as row/column matrix)*

**Properties:**

1. Commutative: $u \cdot v = v \cdot u$
2. Distributive: $(u + v) \cdot w = u \cdot w + v \cdot w$
3. Scalar multi: $(cu)\cdot v = u \cdot (cv) = c(u \cdot v)$
4. $\Vert cu \Vert = |c| \Vert u\Vert \neq c\Vert u \Vert$
5. $u \cdot u \geq 0, \space u \cdot u = 0 \leftrightarrow u = 0$

==To prove $Av = 0 \leftrightarrow A^TAv = 0$:==

$\rightarrow: Av = 0 \rightarrow A^TAv = A^T0 = 0$

$\begin{aligned} \leftarrow: \space & A^TAv = 0 \\ & v^TA^TAv = (Av)^TAv = v^T 0 = 0 \\ & (Av)\cdot(Av) = 0 \rightarrow Av = 0\end{aligned}$

#### §5.2 Orthogonal & Orthonormal Basis

**Meaning of orthogonal:**

1. Vector to vector: $u \cdot v = 0$ ($\perp$ in $\R^2$ and $\R^3$)
2. Set: every pair orthogonal (check $AA^T$ is diagonal matrix)
3. Vector to subspace: $\forall v \in V, \space u \cdot v = 0$
4. Matrix: $A^{-1} = A^T$ *(§5.4)*

**To normalise a vector:**

$\displaystyle u \longrightarrow \frac{1}{\Vert  u \Vert}u$

==To prove **non-zero **orthogonal set $\rightarrow$ LI:==

Let $S = \{u_1, \space u_2, \dots, \space u_n\}$

$c_1u_1 + c_2u_2 + \cdots + c_nu_n = 0$

$(c_1u_1 + c_2u_2 + \cdots + c_nu_n)\cdot u_1 = 0 \cdot u_1$

$c_1(u_1\cdot u_1) + c_2(u_2\cdot u_1) + \cdots + c_n(u_n\cdot u_1) = 0$

$c_1(u_1 \cdot u_1) = 0 \rightarrow c_1 = 0 \because u_1 \neq 0$

Repeat for $c_2, \space c_3, \dots, c_n$

**To check if $S$ is orthogonal/normal basis for $V$:**

1. $S$ is orthogonal/normal
2. $\text{span}(S) = V$ **or** $|S| = \dim(V)$ *(since LI given)*

**To find coordinate vector w.r.t. orthogonal basis:**

$S = \{u_1, \space u_2, \dots, \space u_k\}$ (orthogonal)

$w = c_1u_1 + c_2u_2 + \cdots + c_ku_k$

$\displaystyle (w)_S = (\frac{w\cdot u_1}{{\Vert u_1 \Vert}^2}, \frac{w\cdot u_2}{{\Vert u_2 \Vert}^2}, \dots, \frac{w\cdot u_k}{{\Vert u_k \Vert}^2})$

==To prove:==

Let $w = c_1u_1 + c_2u_2 + \cdots + c_ku_k$

$\begin{aligned} w \cdot u_1 &= (c_1u_1 + c_2u_2 + \cdots + c_ku_k)\cdot u_1 \\ &= c_1(u_1\cdot u_1) + c_2(u_2\cdot u_1) + \cdots + c_k(u_k\cdot u_1) \\ &= c_1(u_1 \cdot u_1) = c_1 {\Vert u_1 \Vert}^2 \end{aligned}$

Repeat for $c_2, \space c_3, \dots, c_n$

**To show a vector is orthogonal to a subspace:**

Let $V = \text{span}\{u_1, \space u_2, \dots, \space u_k\}$

Show $v \cdot u_1 = 0, \space v \cdot u_2 = 0, \dots v \cdot u_k = 0$

**To find all vectors orthogonal to a subspace:**

Let $V = \text{span}\{u_1, \space u_2, \dots, \space u_k\}$, $v$ is orthogonal vector

$v \cdot (c_1u_1 + c_2u_2 + \cdots + c_ku_k) = 0 \space \forall c_1, \space c_2, \dots, c_k$

$v \cdot u_1 = 0, \space v \cdot u_2 = 0, \dots v \cdot u_k = 0$, solve HLS

##### Projection:

$p$ is projection of $u$ onto $V \longleftrightarrow u - p$ is orthogonal to $V$ 

Projection is unique onto a given subspace

**To find projection using orthogonal basis:**

Let $S = \{u_1, \space u_2, \dots, \space u_k\}$ be an orthogonal basis of $V$

$\displaystyle p = \frac{w\cdot u_1}{{\Vert u_1 \Vert}^2}u_1 + \frac{w\cdot u_2}{{\Vert u_2 \Vert}^2}u_2 + \cdots + \frac{w\cdot u_k}{{\Vert u_k \Vert}^2}u_k$

Above expression $\begin{aligned} &= w \text{ if }w \in V \\ &= p \text{ if } w \not \in V \end{aligned}$

Can be used to find coordinate vectors

==To prove:==

Show $w - p$ is orthogonal to $V$

$\displaystyle \begin{aligned}(w - p) \cdot u_i &= w \cdot u_i - p \cdot u_i \\ &= w \cdot u_i - ({\frac{w \cdot u_i}{{\Vert u_i \Vert}^2}})u_i \cdot u_i \\ &= w \cdot u_i - w \cdot u_i = 0\end{aligned}$

**To convert basis to orthogonal basis (Gram-Schmidt):**

Use $u - p$ as basis vectors

Let $\{u_1, \space u_2, \dots, \space u_k\}$ be a basis for $V$

$v_1 = u_1$

$\displaystyle v_2 = u_2 - \frac{u_2 \cdot v_1}{{\Vert v_1 \Vert}^2}v_1$ (*orthogonal to $v_1$*)

$\displaystyle v_3 = u_3 - \frac{u_3 \cdot v_1}{{\Vert v_1 \Vert}^2}v_1 - \frac{u_3 \cdot v_2}{{\Vert v_2 \Vert}^2}v_2$ (*orthogonal to $v_1$ and $v_2$*)

$\cdots$

$\{v_1, \space v_2, \dots, \space v_k\}$ is orthogonal basis

**To convert basis to orthonormal basis:**

Normalise orthogonal basis

$\displaystyle w_i = \frac{1}{\Vert v_1 \Vert}v_1$

#### §5.3 Best Approximations

##### Best approximation

$V$: subspace in $\R^n, \space u \in \R^n$

$p$: projection of $u$ onto $V$

$d(u,p) \leq d(u,v) \space\forall v \in V$

##### Least squares solution

$Ax = b$ inconsistent

$u \in \R^n$ to minimise $\Vert b - Ax \Vert$

$\Vert b - Au \Vert \leq \Vert b - Av \Vert \space \forall v \in \R^n$

**To find LSS:**

1. Solve $Au = p$

	$p$: projection of $b$ onto $\text{Col}(A)$

	$Ax =$ LC of columns $\in \text{Col}(A)$

2. Solve $A^TAx = A^Tb$

	==To prove:==

	$u$ is solution of $A^TAx = A^Tb$

	$\leftrightarrow Au$ is projection of $b$ onto $\text{Col}(A)$

	$\leftrightarrow b - Au$ orthogonal to columns

	$\leftrightarrow A^T(b - Au) = 0$ (dot product)

	$\leftrightarrow A^Tb - A^TAu = 0$

#### §5.4 Orthogonal Matrices

1. $A$ is orthogonal, $A^{-1} = A^T, \space AA^T = I$
2. Rows form orthonormal basis for $\R^n$
3. Columns form orthonormal basis for $\R^n$

**Useful properties:**

1. $(Au) \cdot (Av) = u \cdot v$

	$\begin{aligned} (Au) \cdot (Av) &= (Au)^T(Av) \\ &= u^TA^TAv = u^TA^{-1}Av  \\ &= u^Tv = u \cdot v\end{aligned}$

2. $\Vert u \Vert = \Vert Au \Vert$

	${\Vert Au \Vert}^2 = (Au) \cdot (Au) = u \cdot u = {\Vert u \Vert}^2$

==To prove $1 \leftrightarrow 2$:==

$AA^T = 0, \space A = (a_1, \dots, a_k)$

Diagonal: $a_i \cdot a_i = 1 \space \forall i \leftrightarrow \Vert a_i \Vert = 1$

Other: $a_i \cdot a_j = 0 \space \forall i \neq j \leftrightarrow a_i, a_j$ orthogonal

==To prove $1 \leftrightarrow 3$:==

Use $A^T$

==To prove transition matrix between two orthonormal bases is orthogonal:==

$S = \{u_1, u_2, \dots, u_k\}, \space T = \{v_1, v_2, \dots, v_k\}$

$u_i$ as LC of $\{v_1, v_2, \dots, v_k\}$ (projection onto $T$):

$u_i = (u_i \cdot v_1)v_1 + (u_i \cdot v_2)v_2 + \cdots + (u_i \cdot v_k)v_k$

$\begin{pmatrix} u_i \cdot v_1 & u_i \cdot v_2& \cdots & u_i \cdot v_k\end{pmatrix}$: column of $P: S \rightarrow T$

$v_i$ as LC of $\{u_1, u_2, \dots, u_k\}$ (projection onto $S$):

$v_i = (v_i \cdot u_1)u_1 + (v_i \cdot u_2)u_2 + \cdots + (v_i \cdot u_k)u_k$

$\begin{pmatrix} v_i \cdot u_1 & v_i \cdot u_2& \cdots & v_i \cdot u_k\end{pmatrix}$: column of $Q: T \rightarrow S$

$Q = P^T, Q = P^{-1} \rightarrow P^{-1} = P$, orthogonal

## Chapter 6: Diagonalisation

#### §6.1 Eigenvalues and Eigenvectors

$A_{n \times n}$: square matrix, $x$: nonzero column vector $\in \R^n$

$Ax = \lambda x$ 

$x$ is eigenvector of $A$ associated with eigenvalue $\lambda$

**To find eigenvalues:**

$Ax = \lambda x \leftrightarrow \lambda x - A x = 0 \leftrightarrow (\lambda I - A)x = 0$

HLS has non-trivial solutions, $\det(\lambda I - A) = 0$

$\lambda$ is a root for **characteristic polynomial**

**To find eigenvectors and eigenspace:**

$(\lambda I - A)x = 0$

All $x$ are eigenvectors, solution space is $E_\lambda$

==To prove $\det(A) \neq 0 \leftrightarrow$ eigenvalue of $A \neq 0$:==

$0$ is not root for charpoly, $\det(0I - A) \neq 0$

$ \leftrightarrow \det(-A) \neq 0 \leftrightarrow (-1)^n\det(A)\neq 0 \leftrightarrow \det(A) \neq 0$

==To prove eigenvalues of triangular matrix are diagonal entries:==

$\det(\lambda I - A) = (\lambda - a_{11})(\lambda - a_{22})\cdots(\lambda - a_{nn})$

#### §6.2 Diagonalisation

$A$ is diagonalisable if $\exists P$ s.t. $P^{-1}AP$ is diagonal

**To determine if matrix $A_{n\times n}$ is diagonalisable:**

1. $n$ LI eigenvectors $\leftrightarrow$ diagonisable

2. $n$ distinct eigenvalues $\rightarrow$ diagonalisable

3. already diagonal (converse of $2$ not true)

4. $\forall \lambda_i, \space\dim(E_{\lambda_i}) = r_i \leftrightarrow$ diagonisable

	($\dim$ of eigenspace $\leq$ multiplicity)

==To prove $1$:==

Observations:

$AB = A(b_1 \space b_2 \cdots b_n) = (Ab_1 \space Ab_2 \cdots Ab_n)$

$BD = (b_1 \space b_2 \cdots b_n)D = (d_1b_1 \space d_2b_2 \cdots d_nb_n)$

$D$ is diagonal with entries $d_i$

$\rightarrow$:

$A$ has LI eigenvectors $u_1, \space u_2 ,\dots, \space u_n$ associating eigenvalues $\lambda_1, \space \lambda_2 ,\dots, \space \lambda_n$

Define invertible matrix $P = (u_1 \space u_2 \cdots u_n)$

$\begin{aligned} AP &= (Au_1 \space Au_2 \cdots \space Au_n) \\ &= (\lambda_1 u_1 \space \lambda_2 u_2 \cdots \lambda_n u_n) \\ &= PD  \\ P^{-1}AP &= D\end{aligned}$

$\leftarrow$:

$\begin{aligned} P^{-1}AP &= D \\ AP &= PD \\ A(u_1 \space u_2 \cdots u_n) &= (u_1 \space u_2 \cdots u_n)D \\ (Au_1 \space Au_2 \cdots \space Au_n) &= (\lambda_1 u_1 \space \lambda_2 u_2 \cdots \lambda_n u_n) \\ Au_i &= \lambda_iu_i\end{aligned}$

Since $P$ is invertible, $u_i$ are LI, $A$ has $n$ LI eigenvectors

**To diagonalise a matrix:**

1. Solve charpoly $\det(\lambda I - A) = 0$ to find all $\lambda_i$

2. Find basis $S_{\lambda_i}$ for each eigenspace $E_{\lambda_i}$

3. $S = S_{\lambda_1} \cup S_{\lambda_2} \cup \cdots \cup S_{\lambda_k}$

	$|S| < n: A$ not diagonalisable

	$|S| = n : A$ diagonalisable

$S = \{ u_1, \space u_2, \dots, \space u_n\}, \space P = (u_1 \space u_2 \cdots u_n)$

**To find powers of diagonalisable matrix:**

$(P^{-1} A P)^m = D^m$ ($\lambda_i^m$ in diagonal entries)

$A^m = PD^mP^{-1}$

#### §6.3 Orthogonal Diagonalisation

$P^TAP = D$, $P$ orthogonally diagonalises $A$

Orthogonally diagonalisable $\leftrightarrow$ symmetric

==To prove $\rightarrow$:==

$\begin{aligned} P^TAP &= D \\A &= PDP^T \\ A^T &= (PDP^T)^T\\ A^T &= PDP^T = A\end{aligned}$

**To orthogonally diagonalise a matrix:**

1. ...

2. ...

	Gram-Schmidt process, transform $S_{\lambda_i}$ to orthonormal $T_{\lambda_i}$

3. $T = T_{\lambda_1} \cup T_{\lambda_2} \cup \cdots \cup T_{\lambda_k}$

	$T = \{ v_1, \space v_2, \dots, \space v_n\}, \space P = (v_1 \space v_2 \cdots v_n)$

	(need proof for $T$ being orthonormal

## Chapter 7: Linear Transformations

#### §7.1 Linear Transformation

$T: \R^n \to \R^m, \space T(u) = A_{m \times n}u \space \forall u \in \R^n$

Formula does not contain constant/non-linear terms

Identity trans: $I: \R^n \to \R^n, \space I(u) = I_nu$ (do nothing mapping)

Zero trans: $0 : \R^n \to \R^m, \space 0(u) = 0_{m\times n}u$ (kill everything mapping)

**Properties:**

1. preserves zero vector: $T(0) = 0$
2. preserves LC: $T(au + bv) = aT(u) + bT(v)$

Linear operator: $T: \R^n \to \R^n$, domain = codomain

**To determine LT from basis:**

$v = c_1u_1 + c_2u_2 + \cdots + c_nu_n$

$T(v = c_1T(u_1) + c_2T(u_2) + \cdots + c_nT(u_n)$

**To find standard matrix:**

1. Gaussian elimination

	Express general vector in terms of basis

	Multiply coordinate vector with image of basis

2. Image for standard basis

	$T(e_i) = Ae_i = i^{th}$ column of $A$

	Express $e_1, e_2, \dots, e_n$ in terms of basis

	$A = (T(e_1) \space T(e_2) \space \cdots T(e_n))$

3. Stacking matrices

	$Au_i = T(u_i)$

	$A(u_1 \space u_2 \cdots u_n) = (T(u_1) \space T(u_2) \cdots T(u_n))$

	$A = (T(u_1) \space T(u_2) \cdots T(u_n))(u_1 \space u_2 \cdots u_n)^{-1}$

$S: \R^n \to \R^m, \space S(u) = Au$

$T = \R^m \to \R^k, \space T(u) = Bu$

$T \circ S : \R^n \to \R^k, \space T \circ S(u) = BAu$

#### §7.2 Ranges and Kernel

$T: \R^n \to \R^m$

Range: $R(T) = \{T(u):u\in \R^n\} = \text{Col}(A) \sube \R^m$

**To find $R(T)$:**

1. Formula given, find basis

2. $A$ given, find $\text{Col}(A)$

3. Image of a basis given

	$R(T) = \text{span}\{T(u_1), \space T(u_2), \cdots, \space  T(u_n)\}$

Rank: $\text{rank}(T) = \dim(R(T)) = \dim(\text{Col(A)}) = \text{rank}(A)$

Kernel: $\ker(T) = \{u \in \R^n: T(u) = 0\} = \text{null}(A) \sube \R^n$

**To find $\ker(T)$:**

- $\ker(T) = \{u \in \R^n: Au = 0\} = \text{null}(A)$

Nullity: $\text{nullity}(T) = \text{nullity}(A)$

