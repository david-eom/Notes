## Lecture 01: Intelligent Agents & Problem Environments

##### Types of AI

Strong: dynamic, solves many problems

Weak/Narrow: less dynamic, solves fewer problems

### § Intelligent Agents

##### Agent Components

Sensors $\to$ Function $\to$ Actuators

- Sensors: what can/should be captured

	Percept data at time $t$: $p_t$

	Percept history $P = \{p_1, p_2, \dots, p_t\}$

- Function:

	Maps any given percept sequence to an action

	$f: P \to a$

- Actuators: how will agent affect change

	Set of actions $A$

Goal in AI: determine $f:P \to a, a\in A$

Given available data, rational agent optimises performance measure

##### Types of Agents

- Reflex

	Use if-statements to make decisions, direct mapping of percepts to actions

- Model-based reflex

	Makes decision based on maintained internal state

- Goal-based and utility-based

	Achieves goals/Maximises "happiness"

- Learning

	Agents that learn how to optimise performance

### § Problem Environments

##### Environment Properties

| Property                   | Meaning                                                      |
| -------------------------- | ------------------------------------------------------------ |
| Fully/Partially observable | Agent cannot access all information as some cannot be sensed |
| Deterministic/Stochastic   | Intermediate state is not completely determined by action taken at a given state |
| Episodic/Sequential        | Action might impact all future decisions                     |
| Discrete/Continuous        | State information, time, percepts, actions                   |
| Single/Multi agent         | Other entities exist within the environment that are themselves agents |
| Known/Unknown              | Knowledge of the agent/designer about "laws of physics"      |
| Static/Dynamic             | Environment changes while agent is deciding an action        |

## Lecture 02: Uninformed Search

Path planning problem properties:

Fully observable, deterministic, discrete, episodic

### § Search Spaces

| Term             | Symbol                               | Meaning                                                      |
| ---------------- | ------------------------------------ | ------------------------------------------------------------ |
| State            | $s_i$                                | ADT describing an instance of the environment                |
| Initial state    | $s_0$                                | State that agent starts in                                   |
| Goal test        | $\text{isGoal}:s_i \to \{0, 1\}$     | Checks if $s_i$ is a goal state                              |
| Action           | $\text{actions}:s_i \to A$           | Returns possible actions at given state                      |
| Action cost      | $\text{cost}:(s_i, a_j, s_i') \to v$ | Returns cost of taking $a_j$ at $s_i$ to reach $s_i'$, generally assume $v \geq 0$ |
| Transition model | $\text{T}:(s_i, a_j) \to s_i'$       | Returns state transitioned to when $a_j$ applied to $s_i$    |

##### Performance Criteria

- Time complexity: time needed
- Space complexity: memory needed
- Completeness: correctly report failure when no solution exists
- Optimality: find solution with lowest path cost

##### Redundant Path

More expensive path from $s_0$ to $s1$, cycle is a special case of redundant path

Affects optimality of the algorithm

To rectify, use graph search instead of tree search:

- Maintain reached hash table
- Add nodes to state reached (state, parent node, action, path cost, depth)
- Add new node to frontier if state not previously reached or path to state cheaper than stored

### § Uninformed Search Algorithms

No domain knowledge beyond search space formulation, no clue on how close a state is to goal state(s)

##### General Algorithm

```python
frontier = {initial state} # data structure
while frontier:
  current = frontier.pop()
  if isGoal(current):
    return True
  for a in actions(current):
    frontier.push(T(current, a))
return False
```

##### Algorithm Implementations

$b$: branching factor

$d$: depth for optimal solution

$m$: maximum depth

$e$: $1 + \lfloor C^* / \epsilon \rfloor$, where $C^*$ is the optimal path, $\epsilon $ is a small positive constant

$\ell$: depth limit

| Algorithm                                  | DS             | Time                            | Space                         | Notes                                                    |
| ------------------------------------------ | -------------- | ------------------------------- | ----------------------------- | -------------------------------------------------------- |
| Breadth-first search<br />(BFS)            | Queue          | $O(b^d)$                        | $O(b^d)$                      |                                                          |
| Uniform-cost search<br />(UCS, Dijkstra's) | Priority queue | $O(b^e)$                        | $O(b^e)$                      | Increments of $\epsilon$                                 |
| Depth-first search<br />(DFS)              | Stack          | $O(b^m)$                        | $O(bm)$                       | Memory efficient                                         |
| Depth-limited search<br />(DLS)            | Stack          | $O(b^\ell)$                     | $O(b\ell)$                    | Search up to $\ell$                                      |
| Iterative deepening search<br />(IDS)      | Stack          | $O(b^d)$ / $O(b^m)$ no solution | $O(bd)$ / $O(bm)$ no solution | Increment $\ell$, BFS completeness, DFS space complexity |

| Algo |                           Complete                           |           Optimal           | Improvement                                           |
| :--: | :----------------------------------------------------------: | :-------------------------: | ----------------------------------------------------- |
| BFS  | ✅<br />if $b$ finite AND<br />($d$ finite OR contain solution) | ❎<br />unless costs uniform | Performing early goal test before pushing to frontier |
| UCS  |                              ✅                               |              ✅              | Early goal test would not return optimal solution     |
| DFS  |                ✅<br />if finite search space                 |              ❎              | Backtracking, DLS                                     |
| DLS  |                              ❎                               |              ❎              | IDS                                                   |
| IDS  |                         ✅<br />(BFS)                         |        ❎<br />(BFS)         | Overhead might seem wasteful, but not by a lot        |

## Lecture 03: Informed Search

### § Informed Search Algorithms

Use domain knowledge to estimate cost to nearest goal

##### Heuristic Function

$h(n)$ efficiently uses domain knowledge to approximate path cost to nearest goal

- Admissibility

	$h(n)$ never overestimates the cost

	$\forall n, h(n) \leq h^*(n), \, h(G) = 0$

- Consistency

	$f(n)$ to be monotonically increasing along path

	$\forall n, h(n) \leq \text{cost}(n, a, n') + h(n')$

	Consistency $\to$ admissibiilty

- Dominance

	$h_1(n)$ dominates $h_2$ if $\forall n, h_1(n) \geq h_2(n)$

	$h_1$ is closer to $h^*$

##### Greedy Best-First Search

$f(n) = h(n)$

Algorithm tries to get as close to a goal as it can, never exploits information on path already travelled

Worst-case time and space complexity $O(b^m)$

| Tree search                              | Graph search                        |
| ---------------------------------------- | ----------------------------------- |
| NOT complete, can get stuck in the group | Complete, if search space is finite |
| NOT optimal                              | NOT optimal                         |

##### A* Search

$f(n) = g(n) + h(n)$

$g(n)$: actual path cost from $\text{Start}$ to $n$

$h(n)$: estimated cheapest path cost from $n$ to $\text{Goal}$

Efficiency depends on accuracy of heuristics

| Tree search                   | Graph search                  |
| ----------------------------- | ----------------------------- |
| Optimal, if $h(n)$ admissible | Optimal, if $h(n)$ consistent |

To prove optimality of admissible A* tree search:

$s \to t$, $s \to n \to t^*$, $t^*$ is optimal path

1. Assume $t$ expanded before $n$, $f(t) < f(n)$
2. For a non-goal node $m$, $f(m) \leq g(m) + h^*(m)$
3. For a goal node $m^*$ on path from $m$, $f(m) \leq f(m^*)$
4. $f(n) \leq f(t^*)$ from $3.$, $f(t^*) < f(t)$ since $t^*$ optimal, hence $f(n) < f(t)$
5. Contradicts $1.$

Updating prevents skipping of non-redundant paths for graph search, but updates are expensive, must link to ancestors and descendants to update path costs

Remedy: add state to reached on pop

## Lecture 04: Heuristics

Heuristics domination translates directly to efficiency

Identify $h$ that approximates $h^*(n)$, ideally $O(1)$ time complexity

Generating heuristics from relaxed problems for 8-puzzle

1. Number of misplaced tiles
2. Sum of Manhattan distances of tiles from goal positions

## Lecture 05: Local Search

Only interested about goal state, not the path and cost to get there

**Pros**:

1. $O(b)$ space complexity, only store current and immediate successor states
2. Applicable to large or infinite state spaces

**Cons**:

1. Incomplete

### § Hill-Climbing Search

##### Algorithm (Greedy Local Search)

```python
current = initial state
while True:
	neighbour = highest-valued successor of current
	if value(neighbour) <= value(current):
	  return current
	current = neighbour
```

`value`: a way to evaluate each state e.g. $f(n) = -h(n)$

Can use $f(n) = h(n)$ by replacing `<=` with `>=`

##### Example: n-queens problem

Complete-state formulation: every state has all components of a solution, but they might not be in the right places

$h(n)$: number of pairs of queens threatening each other

##### Issues

Hill-climbing may not return a solution and can get stuck at:

- Local maxima
- Ridges
- Plateaus / Shoulders

##### Variants

1. Sideways move

	`value(neighbour) < value(current)` in hope that plateau is a shoulder

2. Stochastic hill climbing

	Choose among uphill moves at random

3. First-choice hill climbing

	Generate successors randomly until one has better value than current

4. Random restart hill climbing

	Keep attempting from randomly generated initial states until goal is found

##### Expected Computation

$\displaystyle \text{steps for success} + \frac{1-p}{p} \times \text{steps for failure}$

### § Local Beam Search

Keep track of $k$ states rather than just one

- Begin with $k$ randomly generated states
- Generate all successors for all $k$ states
- Selects $k$ best successors and repeat

Performs better than $k$ random restarts in parallel

##### Variants

1. Stochastic beam search

	Choose successors with probability proportional to successor's value

## Lecture 06: Constraint Satisfaction Problems

Use systematic search to reduce search space, prune invalid subtrees as early as possible

### § Formulation

1. $\mathcal{X}$: set of variables $\{X_1, \dots, X_n\}$

2. $\mathcal{D}$: set of domains $\{D_1, \dots, D_n\}$

	$D_i = \{v_1, \dots, v_k\}$

3. $\mathcal{C}$: set of constraints $\{C_i, \dots, C_m\}$

	$C_j = \langle \text{scope}, \text{rel} \rangle$

	- $\text{scope}$: tuple of participating variables
	- $\text{rel}$: relation that defines values that variables can take using set / algebra / logic / function

Initial state: all variables unassigned

Intermediate state: partial assignment

Goal state: complete and consistent assignment

- Complete: all variables assigned value
- consistent: no constraint violated

##### Domain Types

Continuous / discrete	Finite / infinite

##### Constraint Types

Linear / non-linear	Unary / binary / global

##### Constraint Graph

Simple vertex: variable

Linking vertex: global constraint

Edge: relation

##### Search Tree Size

Example: $D$ same for all domains, no constraints

Order of assignment is not important

At depth $\ell$ : $(|\mathcal{X}| - \ell) \cdot |D|$ states

Total # leaf states: $nm \times (n-1)m \times \dots \times m = n! m^n$

Where $n = |\mathcal{X}|$ and $m = |D|$

### § Local Consistency

Constraint propagation may result in chain-reaction of domain reductions

1. Node Consistency

	All values satisfy unary constraints, easy to eliminate

2. Arc Consistency

	All values satisfy binary constraints

	$X_i$ is arc-consistent wrt $X_j$ iff $\forall x \in D_i \, \exists\,  y \in D_j$ such that $(X_i, X_j)$ satisfied

	Arc consistency propagation detects failure earlier than forward checking

	Can be pre-processed with AC-3

##### AC-3 Algorithm

Maintains a queue of arcs, pops arbitrary arc $(X_i, X_j)$ to check arc consistency

If $D_i$ unchanged, move onto the next arc

If $D_i$ revised, add all $(X_k, X_i)$ to queue where $X_k$ is a neighbour of $X_i$

Time Complexity

$|\mathcal{X}| = n \to O(n^2)$ arcs, 

$|\mathcal{D}| = d \to (X_i, X_j)$ inserted in the queue at most $d$ times since $|D_j| = d$

Checking consistency: $O(d^2)$

Total: $O(n^2d^3)$

### § Backtracking Search

1. Choose an unassigned variable: $\textsf{select-unassigned-variable}$
2. Determine value to assign: $\textsf{order-domain-values}$
3. Determine if chosen assignment will lead to terminal state: $\textsf{inference}$
4. Recursive call or restore to previous state: $\textsf{backtrack}$

#### 1. $\textsf{select-unassigned-variable}$ (Variable-Order Heuristics)

##### Minimum-Remaining-Values Heuristic (MRV)

"Most-constrained-variable", "fail-first"

Pick variable with smallest domain size, prune larger subtrees earlier

##### Degree Heuristic

Tie-breaker for MRV heuristics

Pick variable that restricts most number of other unassigned variables

#### 2. $\textsf{order-domain-values}$ (Value-Order Heuristics)

##### Least-Constraining-Value Heuristic (LCV)

Pick value that rules out fewest choices for neighbouring variables, avoids failure

#### 3. $\textsf{inference}$

##### Forward Checking

Tracking remaining legal values of unassigned variables, terminate search when any variable has no legal values

Does not provide early detection for failures

##### Maintaining Arc Consistency (MAC)

After each assignment, start with queue with arcs of neighbouring unassigned variables

## Lecture 07: Adversarial Search

### § Two-Player Zero-Sum Game

Two-player: $\text{MAX}$ (maximise minimum payoff) & $\text{MIN}$ (minimise maximum payoff)

Zero sum: $\text{utility}(\max, s) + \text{utility}(\min, s) = 0$

Winning strategy: for any strategy by opponent, game ends with player as winner

Non-losing strategy: for any strategy by opponent, game ends in either tie or player as winner

| Symbol                              | Meaning                                                      |
| ----------------------------------- | ------------------------------------------------------------ |
| $s_i$ / $s_0$                       | As per general formulation                                   |
| $\text{toMove}: s \to p$            | Returns next player to move in state                         |
| $\text{actions}:s_i \to A$          | Returns legal moves in state                                 |
| $\text{result}:(s, a) \to s'$       | As per transition model                                      |
| $\text{isTerminal}:s  \to \{0, 1\}$ | Terminal test                                                |
| $\text{utility}:(s, p) \to v$       | Final numeric value for player $p$ when game ends in terminal state $s$ |

### § MiniMax Search Algorithm

$\begin{aligned} &\text{MiniMax}(s) = \\ & \begin{aligned}\begin{cases} \text{utility}(s, \text{toMove}(s)) &\text{if isTerminal}(s) \\ \max_{a \in \text{actions}(s)}\text{MiniMax}(\text{result}(s, a)) &\text{if toMove}(s)= \text{MAX} \\ \min_ {a \in \text{actions}(s)}\text{MiniMax}(\text{result}(s, a)) &\text{if toMove}(s)= \text{MIN} \end{cases}\end{aligned} \end{aligned}$

Optimal play for $MAX$ assumes min also plays optimally

DFS: complete if game tree finite, optimal, $O(b^m)$ time, $O(bm)$ space

### § 𝜶-𝜷 Pruning

Motivation: impossible to expand entire game tree, need to prune subtrees that will never affect $\text{MiniMax}$ decision

Serves as an additional layer of MiniMax

Maintain "at least" $\alpha$ of $\text{MAX}$, initially $- \infty$

Maintain "at most" $\beta$ of $\text{MIN}$, initially $+ \infty$

Given $\text{MIN/MAX}$ node, prune if some $\text{MAX/MIN}$ with $\alpha \geq \beta$

Effectiveness depends on move ordering

- Perfect ordering: $O(b^{\frac{m}{2}})$
- Random ordering: $O(b^{\frac{3}{4}m})$

### § Heuristic MiniMax

Motivation: still have to traverse to terminal states

Cut off search early and apply heuristic evaluation function to states

$\begin{aligned} &\text{H-MiniMax}(s, d) = \\ & \begin{aligned}\begin{cases} \text{eval}(s, \text{toMove}(s)) &\text{if isCutoff}(s, d) \\ \max_{a \in \text{actions}(s)}\text{H-MiniMax}(\text{result}(s, a), d+1) &\text{if toMove}(s)= \text{MAX} \\ \min_ {a \in \text{actions}(s)}\text{H-MiniMax}(\text{result}(s, a), d+1) &\text{if toMove}(s)= \text{MIN} \end{cases}\end{aligned} \end{aligned}$

| Symbol                            | Meaning                                                      |
| --------------------------------- | ------------------------------------------------------------ |
| $\text{isCutoff}:s  \to \{0, 1\}$ | Returns true for terminal states, free to decide when to cut off |
| $\text{eval}:(s, p) \to v$        | Returns an estimate of the expected utility, must not take too long, must correlate with chances of winning |

Calculating feature with weighted function: $\displaystyle \text{eval}(s, p) = \sum_{i = 1} ^n w_if_i(s, p)$

Can replace DLS with IDS

## Lecture 08: Logical Agents I

Problem-solving agents' knowledge is limited, no general facts

Knowledge-based agents represent domain knowledge with sentences

Each time agent program:

1. Tell $\text{KB}$ what was perceived
2. Ask $\text{KB}$ what action to perform
3. Tells $\text{KB}$ which action was chosen

Inference engine: domain-independent algorithms

$\text{KB}$: domain-specific content

##### Wumpus World

Deterministic, discrete, static, single-agent, sequential, partially observable

##### Entailment

If a sentence $\alpha$ is true in model $m$, $m$ is a model of $\alpha$

Let $M(\alpha)$ be the set of all models for $\alpha$

$\alpha \models \beta \Leftrightarrow M(\alpha) \subseteq M(\beta)$

##### Inference Algorithm

$\text{KB} \vdash_\mathcal{A} \alpha$

Sentence $\alpha$ is derived from $\text{KB}$ by inference algorithm $\mathcal{A}$

1. Soundness

	$\text{KB} \vdash_\mathcal{A} \alpha \implies \text{KB} \models \alpha$

	$\mathcal{A}$ will not infer nonsense

2. Completeness

	$\text{KB} \models \alpha \implies \text{KB} \vdash_\mathcal{A} \alpha$

	$\mathcal{A}$ can infer any sentence that the $\text{KB}$ entails

$X$: all sentences drive from KB using $\mathcal{A}$

$Y$: all sentences entailed by KB

|                | Sound         | Unsound                                        |
| -------------- | ------------- | ---------------------------------------------- |
| **Complete**   | $X = Y$       | $X \supset Y$                                  |
| **Incomplete** | $X \subset Y$ | $X \not \subset Y, X \not \supset Y, X \neq Y$ |

##### Truth Table Enumeration

$n$ variables, $2^n$ truth assignments

Enumerate all models, and check if $M(\text{KB}) \subseteq M(\alpha)$

Depth-first enumeration, $O(2^n)$ time, $O(n)$ space

Sound as it implements definition of entailment directly

Complete as there are finite models to examine

## Lecture 09: Logical Agents II

##### Validity and Satisfiability

* Validity: true for all possible truth value assignments (tautology)

	Deduction theorem: $\text{KB} \models \alpha \Leftrightarrow \text{KB} \Rightarrow \alpha$ is valid

* Satisfiability: true for some possible assignments

	$\text{KB} \Rightarrow \alpha \Leftrightarrow \text{KB} \land \neg \alpha$ is unsatisfiable

### § Inference

Applying inference rules, generate and search new sentences to grow $\text{KB}$

* States: versions of the $\text{KB}$
* Actions: application of inference rules
* Transition: update $\text{KB}$ with inferred sentences
* Goal: $\text{KB}$ contains sentence to prove

##### Inference Rules

1. Modus Ponens: $\alpha \to \beta \land \alpha \implies \beta$

2. And-elimination: $\alpha \land \beta \implies \alpha$

3. Commutativity, associativity, double negation, contraposition, implication, De Morgan's, distributivity

4. Resolution: $(\ell_1 \lor \dots \lor \ell_i \lor \dots \lor \ell_k) \land m \implies \ell_1 \lor \dots \lor \ell_{i-1} \lor \ell_{i+1} \lor \dots \lor \ell_k$

	$\ell_i \equiv \neg m$

	Yields a complete inference algorithm when coupled with any complete search algorithm

##### Conjunctive Normal Form (CNF)

Conjunctions of disjunction of literals

Rules for conversion to CNF:

1. Eliminate $\Leftrightarrow$: $\alpha \Leftrightarrow \beta \longrightarrow (\alpha \Rightarrow \beta) \land (\beta \Rightarrow \alpha)$ 
2. Eliminate $\Rightarrow$: $\alpha \Rightarrow \beta \longrightarrow \neg \alpha \lor \beta$
3. $\neg$ appear only in literals: move $\neg$ inwards by De Morgan's and double negation
4. Apply distributivity: $\alpha \lor (\beta \land \gamma) \longrightarrow (\alpha \lor \beta) \land  (\alpha \lor \gamma)$

### § Resolution Algorithm

Proof by contradiction, show $\text{KB} \land \neg \alpha$ is unsatisfiable to prove $\text{KB} \models \alpha$

1. Make a clause list, convert $\text{KB} \land \neg \alpha$ into CNF
2. Repeatedly resolve two clauses from clause list until:
	1. Empty clause yielded: $\text{KB} \models \alpha$
	2. No more resolution and non-empty: $\text{KB} \not \models \alpha$

Soundness: each step uses sound inference rules

Completeness: resolution closure

## Lecture 10: Uncertainty

Chain rule: $\displaystyle P(R_1 \land R_2 \land \dots \land R_k) = \prod_{j= 1, \dots, k}P(R_j | R_1 \land \dots \land R_{j-1})$

Conditional independence: $P(T_1 \land \dots \land T_n \land S) = P(T_1 | S) \cdot P(T_2|S) \cdots P(T_n|S) \cdot P(S)$

Without conditional independence: $2^n - 1$ entries

With conditional independence: $n + 1$ entries

##### Bayes' Rule

$P(\text{Cause}|E_1, \dots, E_k) = \alpha \cdot \prod_iP(E_i|\text{Cause}) \cdot P(\text{Cause})$

##### Bayesian Network Specification

1. Nodes represent random variables
2. Directed acyclic graph, $X \to Y \implies X$ directly influences $Y$
3. Probability information for each node given its parents
	$P(X | \text{Parents}(X))$ in conditional probability table

##### Relationships

1. Independent events

	```mermaid
	flowchart TD
	  A((A))
	  B((B))
	  C((C))
	```

	$P(A \land B \land C) = P(A) \cdot P(B) \cdot P(C)$

2. Independent causes

	```mermaid
	flowchart LR
	  A((A))
	  B((B))
	  C((C))
	  A --> C
	  B --> C
	```

	$P(A \land B \land C) = P(C|A, B) \cdot P(A) \cdot P(B)$

3. Conditional independent effects

	```mermaid
	flowchart LR
		A((A))
	  B((B))
	  C((C))
	  A --> B
	  A --> C
	```

	$P(A \land B \land C) = P(C|A) \cdot P(B|A) \cdot P(A)$

4. Casual chain

	```mermaid
	flowchart LR
	  A((A))
	  B((B))
	  C((C))
	  A --> B
	  B --> C
	```

	$P(A \land B \land C) = P(C|B) \cdot P(B|A) \cdot P(A)$

##### Bayesian Network Construction Algorithm

1. Choose an ordering of variables $\{X_1, \dots, X_n\}$
2. For $i = 1, \dots, n$:
	1. Select minimal set of parents such that $P(X_i|\text{Parents}(X_i)) = P(X_i|X_1, \dots, X_i-1)$
	2. Link every parent to $X_i$
	3. Write down CPT for $P(X_i|\text{Parents}(X_i))$

Network constructed is acyclic, contains no redundant probability values


