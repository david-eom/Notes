## Chapter 1: Probability and Counting

Naive definition of probability: $\displaystyle P(\text{Event}) = \frac{|\text{Event}|}{|\text{Sample space}|}$

Binomial coefficients: $\displaystyle \pmatrix{n \\ k} = \frac{n!}{(n-k)! \cdot k!}$

Sampling $n$ items $k$ draws:

* w/o replacement, ordered: $n(n-1) \cdots (n-k+1)$
* w/o replacement, unordered: $\displaystyle \pmatrix{n\\k}$
* with replacement, ordered: $n^k$
* with replacement, unordered: $\displaystyle \pmatrix{n + k - 1 \\ k}$ (Bose-Einstein)

Combinatorial argument:

* Team captain: $\displaystyle n \pmatrix{n - 1 \\ k - 1} = k \pmatrix{n \\ k}$

- Parrots and eagles: $\displaystyle \pmatrix{m + n \\ k} = \sum _{j = 0} ^k \pmatrix{m \\ j} \pmatrix{n \\ k - j}$

Axioms of probability:

1. $P(\varnothing) = 0, P(S) = 1$
2. $\displaystyle P\left( \bigcup _{n = 1} ^\infty A_n\right) = \sum _{n = 1} ^\infty P(A_n)$ if $A_n$ are disjoint (mutex) events

==To prove probability of a complement:==

$P(S) = 1 = P(A \cup A^c) = P(A) + P(A^c)$

$\therefore P(A^c) = 1 - P(A)$

==To prove smaller sets have smaller probability:==

$A \subseteq B, B = A \cup (B \cap A^c)$

$P(B) = P(A) + P(B \cap A^c) \geq P(A)$

$\therefore A \subseteq B \to P(A) \leq P(B)$

==To prove probability of a union:==

$\begin{aligned} P(A\cup B) &= P(A \cup (B \cap A^c)) \\ &= P(A) + P(B \cap A^c) \end{aligned}$

$P(B) = P(B \cap A) + P(B \cap A^c)$

$\therefore P(A \cup B) = P(A) + P(B) - P(A \cap B)$

Inclusion-exclusion:

 $\displaystyle \begin{aligned} P\left( \bigcup _{i=1} ^n A_i\right) =& \sum_i P(A_i) - \sum_{i<j}(A_i \cap A_j) + \sum_{i<j<k} P(A_i \cap A_j \cap A_k) \\ &- \cdots + (-1)^{n+1}P(A_1 \cap \cdots \cap A_n) \end{aligned}$

## Chapter 2: Conditional Probability

Conditional probability: $\displaystyle P(A|B) = \frac{P(A,B)}{P(B)}$

Probability of an intersection:

$P(A,B) = P(A) P(B|A) = P(B) P(A|B)$

Probability of an intersection of $n$ events:

$P(A_1 \cap \cdots \cap A_n) = P(A_1) P(A_2|A_1) P(A_3|A_1, A_2) \cdots P(A_n|A_1, \dots, A_{n-1})$

Bayes' rule: $\displaystyle P(A|B) = \frac{P(B|A)P(A)}{P(B)}$

Law of total probability:

$\displaystyle P(B) = \sum_{j=1} ^n P(B|A_j)P(A_j)$ if $A_1, \dots, A_n$ is a partition of $S$

Independence: if $P(A\cap B) = P(A) P(B)$

Conditional independence: if $P(A\cap B | C) = P(A|C) P(B|C)$

Independence $\not \to$ conditional independence

Conditional independence $\not \to$ independence

Extra conditioning Bayes: $\displaystyle P(A|B,E) = \frac{P(B|A,E)P(A|E)}{P(B|E)}$ 

Extra conditioning LOTP: $P(B|E) = P(B|A,E)P(A|E) + P(B|A^c,E)P(A^c|E)$

Simpson's paradox: it is possible to have $P(A|B, C) < P(A|B^c, C)$ and $P(A|B, C^c) < P(A|B^c, C^c)$ but $P(A|B) > P(A|B^c)$

Gambler's ruin:

$\displaystyle p_i = \frac{1 - \left(\frac{q}{p}\right)^i}{1 - \left(\frac{q}{p}\right)^N}$ if $\displaystyle p \neq \frac{1}{2}$

$\displaystyle p_i = \frac{i}{N}$ if $\displaystyle p = \frac{1}{2}$

## Chapter 3: Discrete Random Variables

Random variable: $f: S \to \R$

Probability mass function

1. Nonnegative: $P(X=x) \geq 0$
2. Sums to 1: $\sum P(X=x)=1$

Cumulative distribution function

1. Increasing: $x_1 \leq x_2 \to F(x_1) \leq F(x_2)$
2. Right continuous: $\displaystyle F(a) = \lim_{x \to a^+}F(x)$
3. Convergence: $\displaystyle \lim_{x \to -\infty} F(x)= 0 \land \lim_{x \to \infty} F(x) = 1$

Indicator random variable: $I(A) = 1$ if $A$ occurs, $0$ otherwise, $E(I(A)) = P(A)$

$\text{Bern}(p) \equiv \text{Bin}(1, p)$

$X \sim \text{Bin}(n, p),\ Y \sim \text{Bin}(m, p) \to X+Y \sim \text{Bin}(n+m, p)$

$\text{NBin(r, p)} \equiv r \, \text{ i.i.d } \text{Geom}(p)$

## Chapter 4: Expectation

Expectation of discrete random variable: $\displaystyle E(X) = \sum_xxP(X=x)$

Linearity:

1. $E(cX) = cE(X)$
2. $E(X+Y) = E(X) + E(Y)$ even if dependent

Proof by $\displaystyle \sum_x x P(X=x) = \sum_{s \in S}X(s)P(\{s\})$

| Distribution                                                 | PMF                                                          | Expectation                                                  | Variance                                                   |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ---------------------------------------------------------- |
| Bernoulli<br />$X \sim \text{Bern}(p)$<br />One time         | $\begin{aligned}&P(X=1)=p \\ &P(X=0)=1-p \end{aligned}$      | $E(X) = p$                                                   | $\text{Var}(X) = p(1-p)$                                   |
| Binomial<br />$X \sim \text{Bin}(n, p)$<br />#S in $n$ trials | $\begin{aligned}& P(X=k) = 0 \text{ if } k \not \in \{0, 1, \dots, n\} \\& P(X = k) = \pmatrix{n \\ k}p^k(1- p)^{n-k} \end{aligned}$ | $E(X) = np$                                                  | $\text{Var}(X) = np(1-p)$                                  |
| Discrete Uniform<br />$X \sim \text{DUnif}(C)$<br />Equally likely | $\displaystyle P(X = x) = \frac{1}{\mid C \mid}$             | $\displaystyle E(X) = \frac{\sum C}{\mid C \mid}$            |                                                            |
| Geometric<br />$X \sim \text{Geom}(p)$<br />#F before 1st S  | $P(X = k) = p(1 - p)^k$                                      | $\displaystyle \begin{aligned} E(X) &= \sum_{k=1}^{\infty} k p(1-p)^k \\ &= \frac{1-p}{p} \end{aligned}$ | $\displaystyle \text{Var}(X) = \frac{1-p}{p^2}$            |
| Negative Binomial<br />$X \sim \text{NBin}(r, p)$<br />#trials for $k$ S | $P(X = k) = \pmatrix{k + r - 1 \\ r - 1}p^r(1-p)^k$          | $\begin{aligned} E(X) &= E(X_1 + \dots + X_r)(p) \\ &= r\frac{1-p}{p} \\ & X_j \sim \text{Geom}(p) \end{aligned}$ | $\displaystyle \text{Var}(X) = r\frac{1-p}{p^2}$           |
| Hypergeometric<br />$X \sim \text{HGeom}(w, b, n)$<br />#$w$ in chosen $n$ | $P(X=k) = \frac{\pmatrix{w\\k} \pmatrix{b\\n-k}}{\pmatrix{w+b\\n}}$ | $\displaystyle E(X) = n \frac{w}{w + b}$                     | $\displaystyle \text{Var}(X) = \frac{w+b-n}{w+b-1}np(1-p)$ |
| Poisson<br />$X \sim \text{Pois}(\lambda)$                   | $\displaystyle P(X = k) = \frac{e^{-\lambda}\lambda^k}{k!}$  | $\displaystyle E(X) = \lambda$                               | $\text{Var}(X) = \lambda$                                  |

|                      | W/ replacement    | W/o replacement         |
| -------------------- | ----------------- | ----------------------- |
| **Fixed #trials**    | Binomial          | Hypergeometric          |
| **Fixed #successes** | Negative binomial | Negative hypergeometric |

Law of the Unconscious Statistician (LOTUS): $\displaystyle E(g(x)) = \sum_x g(x)P(X=x)$

Variance:

1. $\text{Var}(X) = E(X-\mu)^2 = E(X^2) - (E(X))^2$

	$\begin{aligned} \text{Var}(X) &= E(X-\mu)^2 = E(X^2 - 2\mu X + \mu^2) \\ &= E(X^2) - 2\mu^2 + \mu^2 = EX^2 - \mu^2 \\ &= E(X^2) - (E(X))^2\end{aligned}$

2. $\text{Var}(X) \geq 0, \, E(X^2) \geq (E(X))^2$

	$\text{Var}(X) = 0 \to \exists c \text{ s.t. } P(X=c) = 1$

3. $\text{Var}(cX) = c^2 \text{Var}(X)$

	$\text{Var}(X + c) = \text{Var}(X)$
	
	$\text{Var}(X+Y) \neq \text{Var}(X) + \text{Var(Y)}$ unless $X$ and $Y$ independent

$X \sim \text{Pois}(\lambda) \to \text{Var}(X) = E(X) = \lambda$

$X \sim \text{Pois}(\lambda_1), \, Y \sim \text{Pois}(\lambda_2)$, independent

* $X + Y \sim \text{Pois}(\lambda_1 + \lambda_2)$
* $X \sim \text{Bin}(n, \lambda_1/(\lambda_1 + \lambda_2))$ given $X + Y = n$

## Chapter 5: Continuous Random Variables

Continuous random variable: 

1. Support is $\R$ or sub-interval of $\R$
2. Cumulative distributive function is differentiable

Valid PDF for CRV

1. Nonnegative: $f(x) \geq 0$
2. Integrates to 1: $\displaystyle \int^\infty_{-\infty}f(x)dx = 1$

CRV CDF:

$\displaystyle F(x) = \int^x_{-\infty}f(t)dt$

CRV Expectation:

$\displaystyle E(X) = \int^\infty_{-\infty}xf(x)dx$

CRV LOTUS:

$\displaystyle E(g(X)) = \int^{\infty}_{-\infty}g(x)f(x)dx$

| Distribution                                                 | PDF                                                          | CDF                                                          | Expectation                              | Variance                                            |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ---------------------------------------- | --------------------------------------------------- |
| Uniform<br />$U \sim \text{Unif}(a, b)$                      | $\displaystyle f(x) = \frac{1}{b - a}, \, a < x < b$         | $\displaystyle F(x) = \frac{x-a}{b-a}, \, a < x < b$         | $\displaystyle E(U) = \frac{a+b}{2}$     | $\displaystyle \text{Var}(U) = \frac{(b-a)^2}{12}$  |
| Standard normal<br />$Z \sim \mathcal{N}(0, 1)$              | $\displaystyle \varphi(z) = \frac{1}{\sqrt{2 \pi}}e^{-z^2/2}, \, -\infty < z < \infty$ | $\displaystyle \Phi(z) = \int^z_{-\infty}\varphi(t)dt$       | $E(Z) = 0$                               | $\text{Var}(Z) = 1$                                 |
| General normal<br />$X \sim \mathcal{N}(\mu, \sigma^2)$<br />$X = \mu + \sigma Z$ | $\displaystyle f(x) = \varphi\left(\frac{x - \mu}{\sigma}\right) \frac{1}{\sigma}$ | $\displaystyle F(x) = \Phi\left(\frac{x - \mu}{\sigma}\right)$ | $E(X) = \mu$                             | $\text{Var}(X) = \sigma^2$                          |
| Simple exponential<br />$X \sim \text{Expo}(1)$              | $f(x) = e^{-x}, \, x > 0$                                    | $F(x) = 1 - e^{-x}$                                          | $E(X) = 1$                               | $\text{Var}(X) = 2 - 1^2 = 1$                       |
| Exponential<br />$X \sim \text{Expo}(\lambda)$<br />$\lambda X \sim \text{Expo}(1)$ | $f(x) = \lambda e^{-\lambda x}, \, x > 0$                    | $F(x) = 1 - e^{-\lambda x}, \, x > 0$                        | $\displaystyle E(X) = \frac{1}{\lambda}$ | $\displaystyle \text{Var}(X) = \frac{1}{\lambda^2}$ |

$U \sim \text{Unif}(a, b), \, (c, d) \in (a, b) \Rightarrow U | U\in (c, d) \sim \text{Unif}(c, d)$

Universality of the uniform:

Let $F$ be a continuous, strictly increasing CDF

1. Let $U \sim \text{Unif}(0, 1)$, $X = F^{-1}(U)$, then $X$ is an r.v. with CDF $F$
2. Let $X$ be an r.v. with CDF $F$, then $F(X) \sim \text{Unif}(0,1)$

==To prove $1$:==

$P(X \leq x) = P(F^{-1}(U) \leq x) = P(U \leq F(x)) = F(x)$

==To prove $2$:==

$P(Y \leq y) = P(F(X) \leq y) = P(X \leq F^{-1}(y)) = F(F^{-1}(y)) = y$

Symmetry of standard normal PDF & CDF:

1. Symmetry of PDF: $\varphi(z) = \varphi(-z)$
2. Symmetry of tail areas: $\Phi(z) = 1 - \Phi(-z)$
3. Symmetry of $Z$ and $-Z$: $P(-Z \leq z) = P(Z \geq -z) = 1 - \Phi(-z)$

If $X \sim \mathcal{N}(\mu, \sigma^2)$:

* $P(|X - \mu| < \sigma) \approx 0.68$
* $P(|X - \mu| < 2 \sigma) \approx 0.95$
* $P(|X - \mu| < 3 \sigma) \approx 0.997$

Memoryless property:

$P(X \geq s + t | X \geq s) = P(X \geq t)$

Exponential distribution is the only continuous memoryless distribution

A process of arrivals in continuous time is a Poisson process if:

1. Number of arrivals in interval $t$ is $\text{Pois}(\lambda t)$ random variable
2. Number of arrivals in disjoint intervals are independent

$P(T_n > t) = P(N_t < n)$

$\displaystyle X_1 = t_1, X_2 = t_2 - t_1, \dots \Rightarrow X_1, X_2 ,\dots \stackrel{\text{i.i.d}}{\sim} \text{Expo}(\lambda)$

## Chapter 6: Moments

Let $X$ be a r.v. with mean $\mu$, median $m$

* To minimise $E((X-c)^2)$, $c = \mu$
* To minimise $E(|X-c|)$, $c = m$

Nth moment of a random variable $X$: $E(X^n)$

Moment generating function: $M(t) = E(e^{tX})$

Discrete: $\displaystyle \sum_xe^{tx}P(X=x)$

Continuous: $\displaystyle \int_{-\infty}^\infty e^{tx}f_X(x)dx$

| Distribution      | MGF                                                        |
| ----------------- | ---------------------------------------------------------- |
| Bernoulli         | $M(t) = pe^t + (1-p)$                                      |
| Binary            | $M(t) = (pe^t + (1-p))^n$                                  |
| Geometric         | $\displaystyle M(t) = \frac{p}{1-(1-p)e^t}$                |
| Negative binomial | $\displaystyle M(t) = \left(\frac{p}{1-(1-p)e^t}\right)^r$ |
| Uniform           | $\displaystyle M(t) = \frac{e^{tb} -e^{ta}}{t(b-a)}$       |
| Standard normal   | $M(t) = e^{t^2/2}$                                         |
| Normal            | $\displaystyle M(t) = e^{\mu t + \frac 1 2 \sigma^2t^2}$   |
| Exponential       | $\displaystyle M(t) = \frac{\lambda}{\lambda - t}$         |

==To prove $E(X^n) = M^{(n)}(0)$:==

By Taylor expansion: $\displaystyle M(t) = \sum_{n=0}^{\infty}M^{(n)}(0)\frac{t^n}{n!}$ 

By Taylor series of $e^x$: $\displaystyle M(t) = E\left(e^{tX}\right) = E\left(\sum_{n=0} ^{\infty} X^n \frac{t^n}{n!}\right) = \sum_{n=0} ^{\infty} E(X^n) \frac{t^n}{n!}$

MGF is useful because

1. Can be used to calculate moments
2. Determines a distribution uniquely
3. Works well with sum of independent r.v.s $M_{X+Y}(t)=M_X(t)M_Y(t)$

**Log-normal**:

Log of log-normal is normal

$Y \sim \mathcal{LN}(\mu, \sigma^2)$

$Y = e^X, \, X \sim \mathcal{N}(\mu, \sigma^2)$

MGF does not exist, since $E(e^{tY})$ is infinite

But its nth moment can be generated with normal MGF at $t = n$

**Weibull Distribution**:

Generalisation of exponential

$T \sim \text{Wei}(\lambda, \gamma)$

$T = X^{1/\gamma}, \, X \sim \text{Expo}(\lambda), \, \lambda, \gamma > 0$

$f(t) = \gamma \lambda e^{-\lambda t^\gamma}t^{{\gamma - 1}}$

MGF does not exist, but all moments exist

## Chapter 7: Joint Distributions

|                         | Discrete                                                     | Continuous                                                   |
| :---------------------: | ------------------------------------------------------------ | ------------------------------------------------------------ |
|    **Joint PMF/PDF**    | $p_{X,Y}(x, y) = P(X = x, Y = y)$<br />Nonnegative, sums to 1<br />$\displaystyle P((X,Y)\in A) = \mathop{\sum\sum}_{(x, y) \in A}P(X = x, Y = y)$ | $\displaystyle f_{X, Y}(x, y) = \frac{\partial^2}{\partial x \partial y}F_{X, Y}(x, y)$<br />Nonnegative, integrates to 1<br />$\displaystyle P((X, Y)\in A) = \mathop{\iint}_{A}f_{X, Y}(x, y)dxdy$ |
|  **Marginal PMF/PDF**   | $\displaystyle \begin{aligned} P(X=x) &= \sum_yP(X=x, Y=y) \\ &= \sum_y P(X=x \mid Y = y)P(Y = y) \end{aligned}$ | $\displaystyle \begin{aligned} f_X(x) &= \int^{\infty}_{-\infty}f_{X ,Y}(x, y)dy \\ &= \int^{\infty}_{-\infty}f_{X \mid Y}(x \mid y)f_Y(y)dy \end{aligned}$ |
| **Conditional PMF/PDF** | $\displaystyle \begin{aligned} P(Y = y \mid X = x) &= \frac{P(X = x, Y = y)}{P(X = x)} \\ &= \frac{P(X = x \mid Y = y)P(Y = y)}{P(X = x)} \end{aligned}$ | $\displaystyle \begin{aligned} f_{Y \mid X}(y \mid x) &= \frac{f_{X, Y}(x, y)}{f_X(x)} \\ &= \frac{f_{{X \mid Y}}(x \mid y)f_Y(y)}{f_X(x)} \end{aligned}$ |
|    **Independence**     | $P(X = x, Y = y) = P(X = x)P(Y = y)$                         | $f_{X,Y}(x, y) = f_X(x)f_Y((y))$                             |

If $N \sim \text{Pois}(\lambda), \, X \mid N = n \sim \text{Bin}(n, p)$, then $X \sim \text{Pois}(\lambda p), \, Y \sim \text{Pois}(\lambda(1-p))$, and $X$ and $Y$ are independent

Comparing exponentials of different rates:

$T_1 \sim \text{Expo}(\lambda_1), \, T_2 \sim \text{Expo}(\lambda_2)$

$\displaystyle P(T_1 < T_2) = \frac{\lambda_1}{\lambda_1 + \lambda_2}$

Cauchy PDF:

$X, Y \stackrel{\text{i.i.d}}{\sim} \mathcal{N}(0, 1), \, T = X/Y$

$\displaystyle f_T(t) = \frac{1}{\pi (1 + t^2)}$

$\displaystyle F_T(t) = \frac{1}{\pi} \arctan(t) + \frac{1}{2}$

Although PDF symmetric about 0, its expected value does not exist

2D LOTUS:

$\displaystyle E(g(X, Y)) \sum_x \sum_yg(x, y) P(X = x, Y = y)$

$\displaystyle E(g(X, Y)) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x, y)f_{X, Y}(x,y)dxdy$

Covariance:

$\text{Cov}(X, Y) = E((X - E(X))(Y-E(Y))) = E(XY) - E(X)E(Y)$

If $X$ and $Y$ are independent, $\text{Cov}(X ,Y) = 0$

Uncorrelated does not imply independent

Properties of covariance:

1. $\text{Cov}(X, X) = \text{Var}(X)$
2. $\text{Cov}(X, Y) = \text{Cov}(Y, X)$
3. $\text{Cov}(X, Y+c) = \text{Cov}(X, Y)$
4. $\text{Cov}(X, c) = 0$
5. $\text{Cov}(aX, Y) = a \text{Cov}(X, Y)$
6. $\text{Cov}(X + Y, Z) = \text{Cov}(X, Z) + \text{Cov}(Y, Z)$ (bilinearity)
7. $\displaystyle \text{Var}(X_1 + \dots + X_n) = \sum_{i = 1}^n \text{Var}(X_i) + 2 \sum_{i < j} \text{Cov}(X_i, X_j)$

Correlation:

$\displaystyle \text{Corr(X, Y)} = \frac{\text{Cov(X, Y)}}{\sqrt{\text{Var}(X)\text{Var}(Y)}}$

==To prove $-1 \leq \text{Corr}(X,Y) \leq 1$:==

WLOG, $\text{Var}(X) = 1$, $\text{Var}(Y) = 1$

$\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X, Y) = 2 + 2\rho \geq 0$

$\text{Var}(X-Y) = \text{Var}(X) + \text{Var}(Y) - 2\text{Cov}(X, Y) = 2 - 2\rho \geq 0$

$-1 \leq \rho \leq 1$

##### Multinomial

$\sum_{j=1}^kp_j=1, \, X_1 + \dots + X_k = n, \, \vec{X} = (X_1, \dots, X_k), \, \vec{p} = (p_1, \dots, p_k)$

$\sim \text{Mult}_k(n, \vec{p})$

* Joint PMF

$\displaystyle \frac{n!}{n_1!n_2!\cdots n_k!}\cdot p_1^{n_1}p_2^{n_2}\cdots p_k^{n_k}$

* Marginal distribution:

$X_j \sim \text{Bin}(n, p_j)$

* Lumping:

$(X_1+X_2, X_3, \dots, X_k) \sim \text{Mult}_{k-1}(n, (p_1+p_2, p_3, \dots, p_k))$

* Conditioning:

$(X_2, \dots, X_k)|(X_1 = n_1)\sim \text{Mult}_{k-1}(n - n_1, (p'_2, \dots, p'_k))$

$P'_j = p_j/((p_2 + \dots + p_k))$

* Covariance

For $i \neq j$, $\text{Cov}(X_i, X_j) = -np_ip_j$

==Proof:==

$X_i + X_j \sim \text{Bin}(n, p_1 + p_2)$

$\text{Var}(X_1 + X_2) = \text{Var}(X_1) + \text{Var}(X_2) + 2 \text{Cov}(X_1, X_2)$

* Correlation

$\displaystyle \text{Corr}(X_i)(X_j) = \frac{\text{Cov}(X_i, X_j)}{\text{SD}(X_1)\text{SD}(X_2)} = - \sqrt{\frac{p_ip_j}{(1-p_i)(1-p_j)}}$

## Chapter 8: Transformations

$\displaystyle Y = g(X) \Rightarrow f_Y(y) = f_X(x) \left| \frac{dx}{dy} \right|, x = g^{-1}(y)$ 

$g$ is differentiable and strictly increasing/decreasing

$f_Y(y)dy = f_X(x)dx$

$\vec{Y} = g \left(\vec{X} \right), \vec{X} = (X_1, \dots, X_n)$

$\displaystyle f_{\vec{Y}}(\vec{y}) = f_{\vec{X}}(\vec{x}) \cdot |\left
| \frac{\partial x}{\partial y} \right| |, x = g^{-1}(y)$ (determinant of Jacobian matrix)

$\displaystyle \frac{\partial x}{\partial y} = \pmatrix{\frac{\partial x_1}{\partial y_1} & \frac{\partial x_1}{\partial y_2} & \cdots & \frac{\partial x_1}{\partial y_n} \\ \vdots & & & \vdots \\ \frac{\partial x_n}{\partial y_1} & \frac{\partial x_n}{\partial y_2} & \cdots & \frac{\partial x_n}{\partial y_n}}$

To determine the distribution of $T = X + Y$, $X,Y$ independent:

1. Story (binomial, geometric & negative binomial)
2. MGF (normal)
3. Convolution sum/integral

Discrete convolution:

$\displaystyle \begin{aligned} P(T=t) & = \sum_x P(Y = t - x)P(X = x) \\ & = \sum_y P(X = t - y)P(Y = y) \end{aligned}$

Continuous convolution: 

$\displaystyle \begin{aligned} f_T(t) &= \int_{-\infty}^{\infty} f_Y(t-x)f_X(x)dx \\ &= \int_{-\infty}^{\infty} f_X(t-y)f_Y(y)dy \end{aligned}$

**Beta Distribution**:

$X \sim \text{Beta}(a, b), a > 0, b> 0$

$\displaystyle f(x) = \frac{1}{\beta(a, b)}x^{a-1}(1-x)^{b-1}$

$\displaystyle \beta(a,b) = \int^1_0 x^{a-1}(1-x)^{b-1}dx$ (normalising constant)

$\displaystyle E(X) = \frac{a}{a + b}$

$\text{Beta}(1, 1) \equiv \text{Unif}(0, 1)$

$a < 1, b < 1 \Rightarrow$ PDF U-shaped opens upward

$a > 1, b > 1 \Rightarrow$ PDF U-shaped opens downward

$a = b \Rightarrow$ PDF symmetric about $1/2$

$a </> b \Rightarrow$ PDF favours smaller/larger than $1/2$

**Beta-Binomial Conjugacy**:

Prior distribution: $p \sim \text{Beta}(a, b), X|p \sim \text{Bin}(n, p)$

Posterior distribution: $p|(X = k) \sim \text{Beta}(a + k, b + n - k)$

Beta is the conjugate prior of the binomial

$\displaystyle P(X = k) = \pmatrix{n\\k} \frac{\beta(a + k, b + n - k)}{\beta(a, b)}$

**Gamma Function**:

$\displaystyle \Gamma(a) = \int_0^\infty x^a e^{-x}\frac{dx}{x}$ for $a > 0$

Generalisation of exponential

* $\Gamma(a+1) = a \Gamma(a)$
* $\Gamma(n) = (n - 1)!$
* $\Gamma(\frac{1}{2}) = \sqrt{\pi}$

**Gamma Distribution**:

Divide both sides by $\Gamma(a)$

$\displaystyle X \sim \text{Gamma}(a, 1) \Rightarrow f_X(x) = \frac{1}{\Gamma(a)}x^ae^{-x}\frac{1}{x}, x > 0$

$E(X) = a = \text{Var}(X) = a$

$\displaystyle E(X^c) = \frac{\Gamma(a+c)}{\Gamma(a)}$

$\displaystyle Y = \frac{1}{\lambda}X \Rightarrow Y \sim \text{Gamma}(a, \lambda)$

$\displaystyle f_Y(y) = \frac{1}{\Gamma(a)}(\lambda y)^ae^{-\lambda y}\frac{1}{y}, y > 0$

$\displaystyle E(Y^c) = \frac{1}{\lambda^c}\frac{\Gamma(a + c)}{\Gamma(a)}, c > -a$

**Gamma-Exponential Connection**:

$\text{Gamma}(1, \lambda) \equiv \text{Expo}(\lambda)$

$X_1,\dots ,X_n \stackrel{\text{i.i.d}}{\sim} \text{Expo}(\lambda) \Rightarrow X_1 + \cdots + X_n \sim \text{Gamma}(n, \lambda)$

**Gamma-Poisson Conjugacy**:

Prior distribution: $\lambda \sim \text{Gamma}(n_0, \lambda_0), X|\lambda \sim \text{Pois}(\lambda t)$

Posterior distribution: $\lambda|(X = x) \sim \text{Gamma}(n_0 + x, \lambda_0 + t)$

Gamma is the conjugate prior of the Poisson

**Beta-Gamma Connection**:

$X \sim \text{Gamma}(a, \lambda), Y \sim \text{Gamma}(b, \lambda)$

$T = X + Y  \sim \text{Gamma}(a + b, \lambda)$

$\displaystyle W = \frac{X}{X+Y} \sim \text{Beta}(a, b), \frac{1}{\beta(a, b)} = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}$

$T$ and $W$ are independent

**Order statistics**:

$\begin{aligned} P(X_{(j)} \leq x) &= P(\text{at least } j \text{ to the left of }x) \\ &= \sum_{k = j}^n \pmatrix{n \\ k} F(x)^k(1 - F(x))^{n - k} \end{aligned}$

$\begin{aligned} f_{X_{(j)}}(x)dx &= P(j - 1 \text{ to the left of and } n - j \text{ to the right}) \\ &= n \cdot \pmatrix{n - 1 \\ j - 1} f(x)F(x)^{j-1} (1 - F(x))^{n - j} \end{aligned}$

$X_1,\dots ,X_n \stackrel{\text{i.i.d}}{\sim} \text{Unif}(0, 1) \Rightarrow X_{(j)} \sim \text{Beta}(j ,n - j + 1)$

$\displaystyle E(X_{(j)}) = \frac{j}{n + 1}$

## Chapter 9: Conditional Expectation

**Conditional expectation given an event**:

Discrete: $\displaystyle E(Y|A) = \sum_y y P(Y=y|A)$

Continuous: $\displaystyle E(Y|A) = \int_{-\infty}^{\infty}y f(y |A) dy$

Law of Total Expectation:

$\displaystyle E(Y) = \sum_{j=1}^n E(Y|B_j)P(B_j)$

If $Y = I(B) \Rightarrow$ LOTE reduces to LOTP, $E(Y) = P(A)$

**Conditional expectation given a random variable**:

Let $g(x) = E(Y|X=x)$, $E(Y|X) = g(X)$ (random variable)

**Properties of Conditional Expectation**:

1. Dropping what's independent: $X, Y$ indep $\Rightarrow E(Y|X) = E(Y)$
2. Taking out what's known: $E(h(X)Y|X) = h(X)E(Y|X)$
3. Linearity: $E(Y_1 + Y_2) = E(Y_1 | X) + E(Y_2 | X), E(cY|X) = cE(Y|X)$
4. Adam's law: $E(E(Y|X)) = E(Y), E(E(Y|X, Z)|Z) = E(Y|Z)$

**Conditional Variance**:

$\text{Var}(Y|X) = E((Y - E(Y|X))^2|X) = E(Y^2|X) - (E(Y|X))^2$

Eve's law: $\text{Var}(Y) = E(\text{Var}(Y|X)) + \text{Var}(E(Y|X))$ (EVVE)

Variability = within-group variance + between-group variance

## Chapter 10: Inequalities and Limit Theorems

**Markov**:

$\displaystyle P(|X| \geq a) \leq \frac{E(|X|)}{a}, a > 0$

**Chebyshev**:

$E(X) = \mu, \text{Var}(X) = \sigma^2$

$\displaystyle P(|X - \mu| \geq a) \leq \frac{\sigma^2}{a^2}, a > 0$

$\displaystyle P(|X - \mu|  \geq c \sigma) = \frac{1}{c^2}$

**Cauchy-Schawarz**:

$|E(XY)| \leq \sqrt{E(X^2)E(Y^2)}$

Can prove $-1 \leq \text{Cov}(X,Y) \leq 1$ when $E(X) = E(Y) = 0$

Can prove $\text{Var}$ non-negative when $Y = 1$

**Jensen**:

$g$ convex $\Rightarrow E(g(X)) \geq g(E(X))$

$g$ concave $\Rightarrow E(g(X)) \leq g(E(X))$

Applies to $x^2, |x|, e^x, \log x, 1/x$

**Sample Mean & Variance**:

$X_1, X_2, \dots \stackrel{\text{i.i.d}}{\sim} \text{Dist}, E(X_i) = \mu, \text{Var}(X_i) = \sigma^2$

$\displaystyle \bar{X}_n =\frac{X_1+ \dots + X_n}{n}, S_n^2 = \frac{1}{n-1} \sum_{j=1}^n(X_j - \bar{X}_n)^2$

$\displaystyle E(\bar{X}_n) = \mu, \text{Var}(\bar{X}_n) = \frac{\sigma^2}{n}$

**Strong Law of Large Numbers**:

$P(\bar{X}_n \rightarrow \mu) = 1 \text{ as } n \rightarrow \infty$

**Weak Law of Large Numbers**:

$\forall \epsilon > 0, P(|\bar{X}_n - \mu| \geq \epsilon) \to 0 \text{ as } n \to \infty$

**Central Limit Theorem**:

$\displaystyle \sqrt{n}\left(\frac{\bar{X}_n - \mu}{\sigma} \right) \to \mathcal{N}(0, 1) \text{ as } n \to \infty$

$\bar{X}_n \sim \mathcal{N}(\mu, \sigma^2/n) \text{ as } n \to \infty$

$\displaystyle \frac{S_n- n \mu}{\sigma \sqrt{n}} \to \mathcal{N}(0, 1), S_n = X_1 + \dots + X_n = n \bar{X}_n$

