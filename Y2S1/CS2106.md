## Lecture 01: Introduction

#### 1. Motivations of OS

* Abstraction
* Resource allocator
* Control program

#### 2. OS Structures

Flexibility, robustness, maintainability, performance

|              | Monolithic                                                | Microkernel                                         |
| :----------: | :-------------------------------------------------------- | --------------------------------------------------- |
| **Features** | One big special program                                   | Provides basic and essential facilities             |
|    **+**     | Well understood, good performance                         | Robust, extensible, better isolation and protection |
|    **-**     | Highly coupled components, complicated internal structure | lower performance                                   |

#### 3. Virtual Machine

A software emulation of hardware created and managed by hypervisor

* Type 1: provides individual virtual machine to guest OS
* Type 2: runs in host OS

## Lecture 02: Process Abstraction

### Program Execution

Fetched, dispatched to functional unit, completed, executable, under execution

#### 1. Hardware Context

Registers, program counters, stack pointer, frame pointer, etc.

#### 2. Memory Context

##### a) Text

##### b) Data

##### c) Stack (Function Call)

Setup parameters, transfer control to callee, setup local variables, store result if applicable, return to caller

<u>Control flow issues:</u>

* Need to jump to the function body
* Multiple calls where to return

<u>Data storage issues:</u>

* Need to pass parameters and capture return result
* Recursive calls

**Solution:** stack memory

Stack frame stores information of function invocation:

1. Return address of caller

2. Arguments for the function

3. Local variables

4. Stack pointer 

	Top of stack region

5. Frame pointer

	Fixed location in stack frame, platform dependent

6. Saved registers

	Function spill the registers before function starts, restore the registers at the end of function

	Callee-saved

**Stack Frame Setup**

* Caller: pass arguments with registers or stack

* Caller: save return PC on stack

	TRANSFER

* Callee: save registers, old FP, old SP

* Callee: allocate space for local variables on stack

* Callee: adjust SP to new stack top

**Stack Frame Teardown**

* Callee: restore saved registers, FP, SP

	TRANSFER

* Caller: continues execution

##### d) Heap (Dynamically Allocated Memory)

Allocated at runtime, cannot place in data

No definite deallocation timing, cannot place in stack

**Solution:** heap memory

Can easily create holes in memory

#### 3. OS Context

Process ID, process state, etc.

### Process Management

To be able to switch from program A to program B:

* Store information regarding program A
* Replace with information required to run program B

##### Process ID & Process State

PID, unique among processes

Process states:

* New
* Ready
* Running
* Blocking
* Terminated

Process transitions:

* Create: nil $\to$ new
* Admit: new $\to$ ready
* Switch: ready $\leftrightarrow$ running
* Event wait: running $\to$ blocked
* Event occurs: blocked $\to$ running

##### Process Table & Process Control Block 

Kernel maintains PCB for all processes

PCB: hardware, memory, OS contexts

### OS Interaction with Processes

##### Syscall

1. Program invokes libcall
2. Libcall places syscall number in designated location
3. Libcall switch from user mode to kernel mode (TRAP)
4. Appropriate syscall handler determined by dispacther
5. Syscall handler executed
6. Syscall handler ended
7. Libcall return to program 

##### Exception & Interrupt

| Exception                                          | Interrupt                              |
| -------------------------------------------------- | -------------------------------------- |
| Machine level instruction                          | Hardware related                       |
| Synchronous                                        | Asynchronous                           |
| Exception handler, similar to forced function call | Execution suspended, Interrupt handler |

### Process in Unix

##### Identification

PID

##### Information `ps`

Process state, parent PID, cumulative CPU time

##### Creation `fork()`

```c
#include <unistd.h>
int fork();
// returns PID of the created process (for parent)
// returns 0 (for child)
```

Child process is a duplicate, memory is a copy of parent

Child process differs in PID, PPID and `fork()` return value

Make use of `exec()` family to execute another program

`init` process with `PID = 1` is the common ancestor

##### Termination `exit()`

```c
#include <stdlib.h>
void exit(int status);
// 0 for normal termination
// !0 for problematic execution
```

Most resources released on `exit()`, except basic process

Return from `main()` implicitly calls `exit()`

##### Parent/Child Synchronisation

```c
#include <sys/types.h>
#include <sys/wait.h>
int wait(int *status);
// stores exit status for terminated child
```

Blocking until at least one child terminates

Kill zombie process

`waitpid()`: wait for specific child

`waitid()`: wait for child to change status

Orphan: parent terminates before child, `init` becomes pseudo parent

Zombie: child terminates before parent but parent did not call `wait()`

##### Implementation of `fork()`

1. Create address space for child
2. Allocate new PID
3. Create kernel process data structures
4. Copy kernel environment
5. Initialise child OS context
6. Copy memory regions
7. Acquire shared resources
8. Initialise child hardware context
9. Ready to run

Optimisation: copy on write

## Lecture 03: Process Scheduling

#### Concurrent Execution

- Interleave instructions (time-slicing)
- Context switch (incurs overhead)

#### Process scheduling

##### Process Behaviour

- Compute-bound
- IO-bound

##### Processing Environment

- Batch
- Interactive
- Real time

##### Criteria for Good Scheduling

- Fairness (no starvation)
- Utilisation

##### Procedure for Process Scheduling

Non-preemptive: process stay schedules until it blocks or gives up CPU voluntarily

Preemptive: Process given a fixed time quota, suspended at the end of quota

1. Scheduler is triggered
2. Context of current process saved and placed on queue
3. Pick process P based on algorithm
4. Setup context for P
5. Let P run

### Scheduling Algorithm

#### Batch Processing

##### Criteria

1. Turnaround time

	Total time taken, related to waiting time

2. Throughput

	Number of tasks per unit time

3. CPU utilisation

	Percentage of time when CPU is working

##### 1. First Come First Served (FCFS)

NP, First-in-first-out based on arrival time

**Pros:** no starvation

**Cons:** simple reordering reduce average waiting time, convoy effect

##### 2. Shortest Job First (SJF)

NP, Select task with smallest total CPU time, need to know/guess total CPU time in advance

**Pros:** minimises average waiting time

**Cons:** starvation is possible

Predicting CPU time

$\text{Predicted}_{n+1} = \alpha \text{Actual}_n + (1 - \alpha)\text{Predicted}_n$

##### 3. Shortest Remaining Time (SRT)

P, Select job with shortest remaining time

**Pros:** good service for short job even when it arrives late

**Cons:** starvation is possible

#### Interactive

##### Criteria

1. Response time

	Time between request and response

2. Predictability

	Variation in response time

Timer interrupt: hardware clock, invokes scheduler

Interval of timer interrupt (ITI): scheduler invoked every ITI

Time quantum: execution duration given to a process

##### 1. Round Robin (RR)

Tasks stored in FIFO queue, after quantum elapsed / task gives up CPU / task blocks, it is placed at the end of queue

Preemptive version of FCFS

**Pros:** response time guarantee $(n - 1)q$

**Cons:** require timer interrupt, choice of quantum duration (big: longer waiting time, small: bigger overhead)

##### 2. Priority Based

Assign a priority value to all tasks

P: higher priority process preempts running process

NP: late coming higher priority process has to wait

**Pros:** some processes more important

**Cons:** low priority process can starve, locked resource, priority inversion

Remedy:

- Decrease priority after time quantum
- Give running process time quantum

##### 3. Multi-Level Feedback Queue (MLFQ)

If Priority(A) > Priority(B), A runs

If Priority(A) == Priority(B), A and B runs in RR

1. Highest priority for new job
2. Priority reduced after time quantum fully utilised
3. Priority retained if job gives up CPU / blocks

##### 4. Lottery Scheduling

Give out lottery tickets to processes, chosen randomly, winner granted the resource

In the long run process with X% tickets use resource X% of the time

**Pros:**

1. Responsive, newly created process can participate
2. Good level of control, can distribute to child, give more to important process
3. Simple implementation

## Lecture 04: Inter-Process Communication

Memory space is independent, hard for cooperating processes to share information

#### Common IPC Mechanisms

##### • Shared Memory

1. P1 creates shared memory region M
2. P1 and P2 attach M to its own memory space
3. P1 and P2 can communicate using M

```c
int shmid;
int* shm;
// create
shmid = shmget(IPC_PRIVATE, 40, IPC_CREAT | 0600);
// attach
shm = (int*) shmat(shmid, NULL, 0);
// read from/write to shm
...
// detach
shdt((char*)shm);
// destroy
shmctl(shmid, IPC_RMID, 0);
```

**Pros:**

1. Efficient, only initial steps involve OS
2. Ease of use, memory behaves the same as normal

**Cons:**

1. Need to synchronise
2. Harder to implement

##### • Message Passing

1. P1 prepares message M and sends to P2
2. P2 receives M

(sending and receiving of M through syscalls)

##### Naming Scheme

1. Direct

	Explicitly name the other party, one link per pair of communicating processes

	`Send(P2, Msg)`/`Receive(P1, Msg)`

2. Indirect

	Messages in mailbox, shared among a number of processes

	`Send(MB, Msg)`/`Receive(MB, Msg)`

##### Synchronisation

1. Blocking

	Receiver blocked until message arrived

2. Non-blocking: 

	Receiver receive message if available, some other indication if message not ready yet

**Pros:**

1. Portable
2. Easier synchonisation

**Cons:**

1. Inefficient, requires OS intervention and extra copying

#### Unix IPC Mechanism

##### • Unix Pipe

Communication channel with one end for reading, other for writing

Producer-consumer relationship

FIFO

```c
#include <unistd.h>
int pipe(int fd[])
// returns 0 if success
// returns !0 for errors
// fd[0] == reading, fd[1] == writing
```

##### • Unix Signal

Asynchoronous notification

Recipient handle signal by default or supplied handler

```c
void myHandler(int signo)
  if (signo == SIGSEGV)
    printf("memory access blows up\n");
    exit(1);

int main()
  if (signal(SIGSEGV, myHandler) == SIG_ERR)
    printf("failed to register handler\n");
```

## Lecture 05: Threads

Process creation and context switch are expensive, hard for independent processes to communicate with each other

Threads: multiple parts of the programs executing at the same time conceptually

Thread switch is lighter than process context switch

| Threads share                                                | Threads differ in     |
| ------------------------------------------------------------ | --------------------- |
| Memory context: text, data, heap<br />OS context: PID, files | TID, registers, stack |

**Pros:**

1. Economy, less resources as compared to multiple processes
2. Resource sharing
3. Responsiveness
4. Scalability for multiple CPUs

**Cons:**

1. System call concurrency, have to guarantee correctness
2. Impact on process operations (e.g. `fork()`, `exit()`, `exec()`)

#### User Thread

Implemented as a user library

**Pros:**

1. Can have multithreaded program on any OS
2. Thread operations are just libcalls
3. More configurable and flexible

**Cons:**

1. OS not aware of threads, process-level scheduling, one thread blocked all threads blocked

#### Kernel Thread

Implemented in the OS, handled as syscalls

**Pros:**

1. Kernel can schedule on thread levels, use multiple CPUs

**Cons:**

1. Thread operations are syscalls, slower and more resource intensive
2. Less flexible

#### Hybrid Thread

User thread bind to kernel thread, offer great flexibility

#### Simultaneous Multi-Threading

Hardware support for threading

#### POSIX Thread

##### Creation

```c
#include <pthread.h>
int pthread_create(
	pthread_t* thread,
  const pthread_attr_t* attr,
  void* (*startRoutine) (void*),
  void* arg
);
// thread: thread ID
// attr: control the behaviour of new thread
// startRoutine: function pointer to the function to be executed
// arg: arguments for startRoutine
```

##### Termination

```c
int pthread_exit(void* exitValue);
```

If not used, pthread will terminate automatically at the end of `startRoutine`

##### Synchornisation

```c
int pthread_join(pthread_t thread, void** status);
// returns 0 for success
// returns !0 for errors
```

## Lecture 06: Synchronisation

##### Race Condition

Execution of concurrent processes may be non-deterministic due to unsynchronised access to shared resources

Solution: designate critical section, only one process can execute at any point of time

##### Critical Section (CS)

Properties:

1. Mutual exclusion (mutex)

	Other processes are prevented from entering the CS

2. Progress (no deadlock)

	If no process in CS, waiting process should have access

3. Bounded wait (no starvation)

	If process request to enter CS, exists upper bound on the number of times other processes can enter CS

4. Independence

	Process not executing in CS should never block

##### Incorrect Synchronisation

* Deadlock
* Livelock
* Starvation

#### Implementations of CS

##### • Assembly Level

`TestAndSet Register, MemoryLocation`, atomic operation

1. Load content at `MemoryLocation` to `Register`
2. Stores 1 into `MemoryLocation`

```c
void EnterCS(int* Lock)
  while (TestAndSet(Lock) == 1);

void ExitCS(int* Lock)
  *Lock = 0;
```

Busy waiting, wasteful use of processing power

##### • High Level Language

Peterson's Algorithm

```c
Turn = 0;
Want[0] = 0;
Want[1] = 0;
```

```c
// P0
Want[0] = 1; Turn = 1;	// writing to Turn is atomic
while (Want[1] && Turn == 1);
// CS
Want[0] = 0;
```

```c
// P1
Want[1] = 1; Turn = 0;	// writing to Turn is atomic
while (Want[0] && Turn == 0);
// CS
Want[1] = 0;
```

**Disadvantages:**

1. Busy waiting
2. Low level, error prone
3. Not general, only for 2 processes

##### • High Level Abstraction

Semaphore `S` contains a protected integer and a list to keep track of waiting processes

2 atomic semaphone operations:

* `Wait(S)`: if `S <= 0` blocks, `S--`
* `Signal(S)`: `S++`, wake up process if any

$\text{S}_{\text{current}} = \text{S}_{\text{initial}} - \#\text{wait(S)} + \#\text{signal(S)}$

**To prove mutex:**

$\text{N}_{\text{CS}} = \#\text{wait(S)} - \#\text{signal(S)}$

$\text{S}_{\text{current}} + \text{N}_{\text{CS}} = 1$

$\text{S}_{\text{current}} \geq 0 \to \text{N}_{\text{CS}} \leq 1$

**To prove no deadlock:**

Deadlock $\to \text{S}_{\text{current}} = 0 \land \text{N}_{\text{CS}} = 0$

$\text{S}_{\text{current}} + \text{N}_{\text{CS}} = 1$, contradiction

**To prove no starvation:**

P1 blocked at `wait(S)` due to P2 in CS, P2 exits with `signal(S)`

P1 eventually wakes up assuming fair scheduling 

#### Producer Consumer

##### Busy Waiting

`count = in = out = 0`, `mutex = S(1)`

`canProduce = true`, `canConsume = false`

```c
// producer
while (true)
  // produce item
  while (!canProduce);
	wait(mutex);
	if (count < K)
    buffer[in] = item; in = (in + 1) % K; count++;
		canConsume = true;
	else
    canProduce = false;
	signal(mutex);
```

```c
// consumer
while (true)
  while (!canConsume);
	wait(mutex);
	if (count > 0)
    item = buffer[out]; out = (out + 1) % K; count--;
		canProduce = true;
  else
    canConsume = false;
	signal(mutex);
  // consume item
```

##### Blocking

`count = in = out = 0`

`mutex = S(1)`, `notFull = S(K)`, `notEmpty = S(0)`

```c
// producer
while (true)
  // produce item
  wait(notFull);
	wait(mutex);
	buffer[in] = item; in = (in + 1) % K; count++;
	signal(mutex);
	signal(notEmpty);
```

```c
// consumer
while (true)
  wait(notEmpty);
	wait(mutex);
	item = buffer[out]; out = (out + 1) % K; count--;
	signal(mutex);
	signal(notFull);
  // consume item
```

#### Reader Writer

Writer must have exclusive access, reader can have access with other readers

`roomEmtpy = S(1)`, `mutex = S(1)`, `nReader = 0`

```c
// writer
while (true)
  wait(roomEmpty);
	// modify data
	signal(roomeEmpty);
```

```c
// reader
while (true)
  wait(mutex);
  nReader++;
	if (nReader == 1) wait(roomEmpty);
	signal(mutex);
	// read data
	wait(mutex);
	nReader--;
	if (nReader == 0) signal(roomEmpty);
	signal(mutex);
```

#### Dining Philosophers

##### Tanenbaum's Solution

```c
int state[N];
Semaphore mutex = 1; Semaphore s[N];

void philisopher(int i)
  while (true)
    Think();
    takeChpstks(i);
    Eat();
    putChpstks(i);

void takeChpstks(int i)
  wait(mutex);
  state[i] = HUNGRY;
  safeToEat(i);
  signal(mutex);
  wait(s[i]);

void putChpstks(int i)
  wait(mutex);
  state[i] = THINKING;
  safeToEat(LEFT);
  safeToEat(RIGHT);
  signal(mutex);

void safeToEat(int i)
  if (state[i] == HUNGRY &&
      state[LEFT] != EATING &&
      state[RIGHT] != EATING)
    state[i] = EATING;
	  signal(s[i])
```

##### Limited Eater

`seats = S(K - 1)`, `chpstk = S(1)[K]`

```c
void philosohper(int i)
	while (true) {
    Think();
    wait(seats);
    wait(chpstk[LEFT]);
    wait(chpstk[RIGHT]);
    Eat();
    signal(chpstk[LEFT]);
    signal(chpstk[RIGHT]);
    signal(seats)
```

## Lecture 07: Memory Abstraction

### § Memory Abstraction

**Pros**: fixed address, no mapping required

**Cons**: conflicts, hard to protect memory space

#### Fix #1: Address Relocation

Add offset to memory references when process is loaded

**Cons**: slow loading time, hard to distinguish memory reference from integer constant

#### Fix #2: `Base` + `Limit` Registers

`Base` as starting address during loading, memory access checked against `Limit` for protection

**Cons**: access incus more time (addition and comparison)

#### Fix #3: Logical Address

Each process has self-contained, independent logical memory space

### § Contiguous Memory Allocation

##### Assumptions:

1. Each process occupies contiguous memory region
2. Physical memory is large enough to contain processes

#### Memory Partitioning

##### • Fixed Partitioning

**Pros**: easy to manage, fast to allocate

**Cons**: internal fragmentation, partition size must be larger than largest process

##### • Dynamic Partitioning

Merge holes

**Pros**: flexible, remove internal fragmentation

**Cons**: external fragmentation, OS need to maintain more information, takes more time to locate region

#### Partition Allocation Algorithms

##### • Merging and Compaction

Use first/best/worst-fit to search for hole

Split and create new hole with left over space

Merge adjacent hole if possible

E.g. linked list of $\texttt{[status, start address, legnth]}$

##### • Buddy System

Keep an array $\texttt{A[0..K]}$, largest block size $\texttt{2}^\texttt{K}$

Each element $\texttt{A[J]}$ is a linked list of blocks of size $\texttt{2}^\texttt{J}$

Each free block indicated by starting address

$\texttt{B}$ and $\texttt{C}$ are buddy of $\texttt{2}^\texttt{S}$ if

* $\texttt{S}^\texttt{th}$ bit of $\texttt{B}$ and $\texttt{C}$ is complement
* Leading bits up to $\texttt{S}^\texttt{th}$ bit of $\texttt{B}$ and $\texttt{C}$ are the same

##### Allocation of $\texttt{N}$

1. Find smallest $\texttt{S}$ such that $\texttt{2}^\texttt{S}\texttt{ >= N}$
2. If $\texttt{A[S]}$ exists
	* Remove from list and allocate

3. Else

	* Find smallest $\texttt{R}$ such that $\texttt{A[R]}$ has free block $\texttt{B}$

	* Repeatedly split $\texttt{B}$

##### Deallocation of $\texttt{B}$

1. Check $\texttt{A[S]}$ where $\texttt{2}^\texttt{S}\texttt{ == B}$
2. If buddy $\texttt{C}$ of $\texttt{B}$ exists
	* Remove $\texttt{B}$ and $\texttt{C}$ from list
	* Merge $\texttt{B}$ and $\texttt{C}$ to $\texttt{B'}$
	* Repeat for $\texttt{B'}$
3. Else
	* Insert $\texttt{B}$

## Lecture 08: Disjoint Memory Schemes

### § Disjoint Memory Allocation

Remove assumption #1

#### • Paging

##### Implementation

Physical memory split into fixed size: **frames**

Logical memory split into same size: **pages**

Pages loaded into available frame at execution

Use page table to look up

##### Translation

1. Keep frame & page size power of 2
2. Frame size == page size

Logical address: $\texttt{[m-n|n]}$

Physical address: $\texttt{[}\text{translated }\texttt{F|n] = F * 2}^\texttt{n} \texttt{ + offset}$

##### Pros and Cons

* Removes external fragmentation
* Still has internal fragmentation
* Clear separation of logical and physical address, flexible and simple translation

##### Hardware Support

Two memory accesses for every memory reference

1. Read the indexed page table entry
2. Read the actual memory item from frame

Translation Look-aside Buffer (TLB) acts as cache

Hit: retrive frame number

Miss: access full page table, retrieve frame number and update TLB

When context switch occurs, **TLB entries are flushed**

##### Protection

Access right bits: whether page is writable, readable, executable

Valid bit: whether page is valid for access

##### Page Sharing

Page table allows several processes to share the same frame

1. Shared code
2. Copy-on-write

#### • Segmentation

##### Motivation

Memory regions may grow/shrink

Hard to place them contiguously and allow them to grow/shrink

##### Implementations

Separate regions into segments, each with a name and limit

All memory references are $\texttt{segment name + offset}$

Each segment mapped to contiguous memory with $\texttt{Base}$ and $\texttt{Limit}$

##### Translation

Logical address: $\texttt{<SegID, Offset>}$, find $\texttt{<Base, Limit>}$ with $\texttt{SegID}$ 

Physical address: $\texttt{Base + Offset}$, $\texttt{Offset < Limit}$

##### Pros and Cons

**Pros**: each segment can grow/shrink/protect/share independently

**Cons**: external fragmentation

#### • Segmentation with Paging

Each segment has a page table

## Lecture 09: Virtual Memory Management

### § Virtual Memory Management

Remove assumption #2

#### Virtual Memory

##### Implementation

Some pages reside in physical memory, others stored on secondary storage

Page table keep track of residency, CPU can only access resident pages

##### Steps

1. Check page table for residency

	Yes: access memory, done

	No: continue

2. Page fault, trap to OS

3. Locate page in secondary storage

4. Load page into physical frame

5. Update page table

6. Go to step 1

##### Justification

Secondary storage access time >> physical memory access time

Thrashing unlikely to happen due to temporal locality and spatial locality

##### Benefits

* Completely separate logical memory from physical memory
* More efficient use of physical memory
* Allow more processes to reside in memory

#### Page Table Structures

##### • Direct Paging

Keep all entries in single table

$\texttt{2}^\texttt{P}$ logical memory page, $\texttt{P}$ bits to identify page, $\texttt{2}^\texttt{P}$ PTEs

##### • 2-Level Paging

Split page table into smaller page tables, size usually equal to page size

Page directory

$\texttt{2}^\texttt{p}$ PTEs, $\texttt{2}^\texttt{M}$ smaller page tables, $\texttt{M}$ bits to identify page table, $\texttt{2}^\texttt{P-M}$ page directory entries

**Pros**: can have empty entries in page directory, corresponding page table need not be allocated

##### • Inverted Table

Keep single mapping of physical from to $\texttt{<pid, page\#>}$

**Pros**: one table for all processes, space saving, easy and fast frame management

**Cons**: slow translation

#### Page Replacement Algorithms

$T_{\text{access}} = (1-p) * T_{\text{mem}} + p * T_{\text{page fault}}$, $T_{\text{page fault}} >> T_{\text{mem}}$

Good algorithm reduce number of page faults

##### • Optimal Page Replacement (OPT)

Replace page that will not be used again from the longest period

**Pros**: guarantees minimum #page faults, base of comparison for other algorithms

**Cons**: not feasible, need future knowledge

##### • First In First Out (FIFO)

OS maintains a queue, evict the oldest/earliest loaded page

**Pros**: simple to implement

**Cons**: does not exploit temporal locality, $\uparrow$ frames $\to$ $\uparrow$ #page faults (Belady's Anomaly)

##### • Least Recently Used (LRU)

Replace page that has not been accessed for the longest time

**Pros**: approximates OPT, does not suffer from Belady's Anomaly

**Cons**: not easy to implement

###### Approach A: counter

PTE has time-of-use field, replace page with smallest time-of-use

**Problem**: need to search through all pages, time-of-use might overflow

###### Approach B: stack

Page removed and pushed on top of stack if referenced

**Problem**: replacing bottom page requires searching through all pages, not a pure stack, hard to implement

##### • Second-Chance (CLOCK)

Each PTE maintains reference bit, 1 = accessed, 0 = not accessed

Use circular queue and victim page

1. Oldest FIFO page selected
2. If reference bit == 0, page is replaced
3. If reference bit == 1, page is given second chance
	* Reference bit cleared to 0
	* Load time reset, page taken as newly loaded
	* Next FIFO page selected, go to step 2
4. Degenerate to FIFO if all pages have same reference bit

#### Frame Allocation Policies

##### Equal/Proportional Allocation

$\texttt{N}$ frames, $\texttt{M}$ processes

Equal: each process gets $\texttt{N/M}$ frames

Proportional: process $\texttt{p}$ gets $\texttt{size}_\texttt{p}\texttt{/size}_\texttt{total}\texttt{*N}$ frames

##### Local/Global Replacement

Local: victim selected among pages of the process

**Pros**: performance stable between runs

**Cons**: frames might not be enough, performance hindered, single process can hog I/O

Global: victim chosen among all frames

**Pros**: allow dynamic self-adjustment

**Cons**: badly behaved process cause other processes to thrash, frames allocated are different between runs

##### Working Set Model

Working set window $\Delta$

$\texttt{W(t,}\Delta\texttt{)}$: active pages in the interval at $\texttt{t}$

Transient region & stable region

## Lecture 10: File System Abstraction

Abstraction, resource management, protection, sharing

Criteria: self-contained, persistent, efficient

### File

Abstract data type with common operations

#### • Metadata

| Field      | Notes                                                        |
| :--------- | ------------------------------------------------------------ |
| Name       | Length, case sensitivity, special symbols, extension         |
| Type       | Regular (ASCII / Binary) / Directories / Special<br />To distinguish: use file extension / use embedded information |
| Protection | Access control list<br />Permission bits (`rwx`)             |

#### • File Data

##### Structure

* Array of bytes ($\texttt{offset(distance)}$)
* Fixed length records ($\texttt{record size*(N-1)}$)
* Variable length records (harder to locate)

##### Access methods

* Sequential
* Random ($\texttt{read(offset)}$/$\texttt{seek(offset)}$)
* Direct (random to fixed length records)

##### Syscalls

Information kept in an open file:

* File pointer
* Disk location
* Open count

3 tables:

* Per-process open-file

	Each entry points to system-wide open-file entry

* System-wide open-file

	Each entry points to V-node entry

* System-wide V-node

	Link to physical drive

###  Directory

Provide logical grouping, keep track

| Structure     | Notes                                                        |
| ------------- | ------------------------------------------------------------ |
| Single-level  |                                                              |
| Tree          | Absolute path<br />Relative path                             |
| DAG           | Hard link:<br />• Create separate pointers<br />• +overhead, -deletion<br />Soft link:<br />• Create special link file<br />• +deletion, -overhead |
| General graph | Not desirable:<br />• Hard to traverse, need to prevent infinite loop<br />• Hard to remove file/directory |

### I/O Scheduling Algorithms

* First-come-first-served (FCFS)

* Shortest seek first (SSF)

	Akin to SJF

* SCAN

	Bi-direction: SCAN

	1-direction: C-SCAN

* Deadline

	3 queues: sorted, read FIFO, write FIFO

* No-operation (noop)

* Completely fair queuing (CFQ)

* Budget fair queuing (BFQ)

## Lecture 11: File System Implementation

$\texttt{[MBR Partition ... Partition]}$

### Partition Organisation

#### 1. OS Boot Block

#### 2. Partition Details (Free Space Management)

Maintain a free space list

##### • Bitmap

1 == free, 0 == occupied

**Pros**: good set of manipulations

**Cons**: need to keep bitmap in memory

##### • Linked List

Each disk block contains:

* A number of free disk block numbers, **OR**
* A pointer to the next free disk block

**Pros**: easy to locate free block, only first pointer needed in memory

**Cons**: high overhead (mitigated by storing free block list in free blocks)

#### 3. Directory Structure

Main tasks: keep track of files, map file name to file info

Two approaches:

1. Store everything in directory entry
2. Store only file name, points to other data structure for file info

##### • Linked List

Each entry store file name & file info

**Pros**: can cache the latest few searches

**Cons**: inefficient due to linear search

##### • Hash Table

File name hashed, use chained collision resolution

**Pros**: fast lookup

**Cons**: table has limited size, depend on good hash function

#### 4 & 5. File Information & Data (File Block Allocation)

* Keep track of logical blocks
* Efficient access
* Utilise disk space effectively

##### • Contiguous

Allocate consecutive blocks to a file

**Pros**: simple to keep track, fast access

**Cons**: external fragmentation, file size needs to be specified in advance

##### • Linked List

Each disk block stores pointer to next disk block and actual file data

**Pros**: solve fragmentation

**Cons**: slow random access, overhead for pointers, less reliable

##### • FAT (Linked List)

Move all pointers to a single table, keep in memory

**Pros**: fast random access

**Cons**: overhead by keeping tracks of all disk blocks

##### • Indexed

Each file has index block (an array of disk block addresses)

**Pros**: lesser memory overhead, fast direct access

**Cons**: maximum file size limited by number of block entries, index block overhead

### Case Study: FAT

Partition: $\texttt{[boot|FAT\_1|FAT\_2|root directory|data blocks]}$

OS cache FAT in RAM

Directory is represented as a special type of file

##### FAT entry:

* `FREE`: unused
* `<block number>`: next block
* `EOF`: `NULL` pointer
* `BAD`: unusable

##### Directory Entry:

* File name + extension: 8B + 3B
* File creation datetime: 2B + 2B
* First disk block index: 2B for FAT16 (16 bits)

### Case Study: ext2

Partition: $\texttt{[boot|block group 0|block group 1|...]}$

##### Block group:

1. Superblock

	#I-nodes, I-nodes/group, #disk blocks, disk blocks/group, etc.

	Duplicated

2. Group descriptor

	#free I-nodes, #free disk blocks, location of bitmaps

	Duplicated

3. Block bitmap

	Usage status of blocks of this block group

4. I-node bitmap

	Usage status of I-nodes of this block group

5. I-node table

	Array of I-nodes of this block group

6. Data blocks

##### I-Node

128B, data block pointers 15B * 4, multilevel index

12 \* direct, 1 \* indirect, 1 \* double indirect, 1 \* triple indirect

Flexible in handling huge files

##### Directory entries:

Stored as linked list in disk blocks

* I-node number for file/subdirectory
* Size of directory entry (for locating next directory entry)
* Type (file/subdirectory)
* Length of file/subdirectory name
* File/subdirectory name

#### Common Tasks

##### File Opening

1. Locate file, if not found terminate with error
2. Load file information into a new entry in system-wide file open table
3. Create entry in process' table to point to file descriptor
4. Return file descriptor

##### File Deletion

1. Remove its directory entry from parent directory

	Point previous entry to next entry / end

2. Update I-node bitmap, mark I-node as free

3. Update block bitmap, mark block(s) as free

##### Hard Link

Create directory entry with same I-node number

Problem: multiple references of I-node, need to maintain a I-node refcount and decrement for every deletion

##### Symbolic Link

Create new file that contains pathname of shared file

Problem: only pathname stored, link can be easily invalidated
