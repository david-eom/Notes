## Lecture 01: Introduction

##### To Improve Performance

* Work hard: higher clock frequency
* Work smart: pipelining
* Get help: replication

##### Why Parallel Computing

* Primary reasons
	* Overcome limits of serial computing
	* Save time
	* Solve larger problems
* Other reasons
	* Take advantage of non-local resources
	* Cost saving
	* Overcome memory constraint

##### Parallel Model Attributes

1. Operation mechanism (primitive units)
2. Data mechanism (address spaces)
3. Control mechanism (schedulable units)
4. Communication mechanism
5. Synchronisation mechanism (info arrives at the right time)

## Lecture 02: Processes, Threads, Synchronisation

##### Steps for Prallelisation:

1. Decomposition of computations
2. Scheduling of tasks
3. Mapping of processes

#### • Processes

Identified by PID, own address space, comprises of:

1. Executable program
2. Global data
3. Stack / heap
4. Current values of registers

##### Inter-Process Communication

1. Shared memory
2. Message passing
3. Pipes & signal (UNIX specific)

##### Disadvantages

* Creation of process is costly
* Communicating between processes is costly

#### • Threads

Share address space of the process, faster than process

Can be user-level / kernel

##### Disadvantages

* OS cannot map threads to different resources, no parallelism
* OS cannot switch to other thread in the event of blocking IO operation

#### • Synchronisation

##### Race Condition

1. Two concurrent threads/processes access shared resouce without synchronisation
2. At least one modifies the shared resource

##### Critical Section Requirements

1. Safety: mutex
2. Liveness: progress & no startvation (will eventually leave/enter critical section)
3. Performace: small overhead

##### Conditions for Deadlock

1. Mutex
2. Hold and wait: must have one holding and one waiting
3. No pre-emption: critical sections cannot be aborted externally
4. Circular wait

## Lecture 03: Parallel Computing Architectures

Concurrency: tasks in overlapping time periods

Parallelism: tasks execute simultaneously

#### • Sources of Performance Gain

##### 1. Bit Level

Larger word size, more computation

##### 2. Instruction Level

1. Pipelining (parallelism across time)

   Disadvantage: independence, bubbles, data and control hazards

2. Superscalar (parallelism across space)

   Duplicate pipelines, allow multiple instruction pass through the same stage

   Disadvantage: challenging scheduling, structural hazard

##### 3. Thread

Processor execute threads in parallel

* Fine-grained multithreading (switch after instruction)
* Coarse-grained multithreading (switch on stalls)
  * Time-slice
  * Switch-on-event
  * Simultaneous multithreading

##### 4. Processor

Add more cores to processor

#### • Flynn's Parallel Architecture Taxonomy

Instruction stream: single execution flow

Data stream: data being manipulated

1. SISD
2. SIMD: each instruction work on multiple data
3. MISD (no actual implementation)
4. MIMD: Each PU fetch own instruction, each PU operates own data

Variant: SIMD + MIMD

* One set of threads executing same code (SIMD)
* Multiple sets of threads executing in parallel (MIMD)

#### • Multicore Architecture

##### Hierarchical Design

Cache size increase from leaves to root, all cores share common external memory

##### Pipelined Design

Data processed by multiple execution cores in sequence

Useful if same computation needs to be applied to long sequence

##### Network-based Design

Cores, local caches & memories connected via network

#### • Memory Organisation

Memory bandiwidth is the critical resource (as compared to memory latency)

Parallel progarms should fetch data from memory less often & favour performing additional arithmetic

##### 1. Distributed Memory

Memory in a node is private, data exchanges via message passing

##### 2. Shared Memory

Programs/threads access through shared memory provider

Require cache coherence and memory consistency

Factors that differentiate shared memory systems:

1. Processor to memory delay (whether uniform)
2. Presence of local cache with cache coherence protocol

Types of shared memory:

1. Uniform Memory Access (UMA)

   Suitable for small number of processors

2. Non-Uniform Memory Access (NUMA)

   AKA distrubuted shared memory

3. Cache-Only Memory Access (COMA)

   Each memory block works as cache

Advantages: no need partition code / data, no need move data among processors

Disavantages: synchronisation required, lack of scalability due to contention

##### 3. Hybrid (Distributed-Shared)

## Lecture 04: Parallel Programming Models (I)

#### • Types of Parallelism

##### Data Parallelism

Same operation applied to different elements of a data set (SIMD)

Loop parallelism, iterations are independent

Common model: single program multiple data (SPMD)

##### Task Parallelism

Independent program parts executed in parallel

Task dependence graph

* Directed acyclic graph
* Critical path length: slowest completion time
* Degree of concurrency: total work / critical path length

#### • Representation of Parallelism

* Implicit parallelism
  * Automatic
  * Functional 
* Explicit parallelism
  * Implicit scheduling
  * Explicit scheduling
    * Implicit mapping
    * Explicit mapping

#### • Models of Coordination

##### Shared Address Space

Tasks communicate by reading/writing from/to shared variables

Ensure mutex via locks

Disadvantages:

1. Requires hardware support
2. Memory contention
3. Costly to scale

##### Data Parallel

Map function onto a large collection of data

Side-effect free execution, no communication needed

##### Message Passing

Tasks communicate by explicitly sending/receiving messages

##### Correspondence with Hardware

No direct link, possible to implement abstractions on any machines

#### • Foster's Design Methodology

##### 1. Partitioning

Divide computation and data to discover maximum parallelism

* Data centric: domain decomposition
* Computation centric: functional decomposition

Rules of thumb:

1. 10x more primitive tasks than cores
2. Minimise redundant computations & data storage
3. Primitive tasks roughly same size
4. Number of tasks increase with problem size

##### 2. Communication

* Local communication: create channels for neighbours
* Global communication: don't do it early in design

Rules of thumb:

1. Communication balanced among tasks
2. Each task communicates with small group of neighbours
3. Tasks can communicate in parallel
4. Overlap computation & communication

##### 3. Agglomeration

Combine tasks into larger tasks to improve performance, maintain scalability and simplify programming

Eliminates communication between primitive tasks

Rules of thumb:

1. Locality of parallel also has increased
2. Number of task increase with problem size
3. Number of tasks suitable for target systems
4. Tradeoff between agglomeration & code modification cost

##### 4. Mapping

Assign tasks to execution units to maximise processor utilisation and minimise communication

May be performed by OS or user

Rules of thumb:

1. Rely on heuristic to find optimal mapping
2. Consider designs for 1 task/core vs multiple tasks/core
3. Evaluate static & dynamic task allocation, task allocator should not be a bottleneck

#### • Parallel Programming Patterns

1. Fork-Join

2. Parbegin-Parend

3. SIMD

4. SPMD

   Same program executed on different cores

5. Master-Worker

   Master coordinate and initialise, worker wait for instruction

6. Client-Server

   Server computes request from client tasks

7. Task Pool

   Fixed number of threads, threads retrieve tasks

8. Producer-Consumer

9. Pipelining

## Lecture 05: Performance of Parallel Systems

#### • Performance Goals

Users: reduced response time

Computer managers: high throughput

##### Million-Instruction-Per-Second 

$\displaystyle MIPS(A) = \frac{N_{\text{instr}(A)}}{T_{\text{user}(A)} \times 10^6} = \frac{\text{clock freq}}{CPI(A) \times 10^6}$

Drawback: only consider number of instructions, easily manipulated

##### Million-Floating point-Operation-Per-Second

$\displaystyle MFLOPS(A) = \frac{N_{\text{fl ops}(A)}}{T_{\text{user}(A)} \times 10^6}$

Drawback: no differentiation between different types of FL ops

#### • Execution Time

##### Sequential

1. User CPU time

   Time CPU spends for executing program

   $\displaystyle \text{T}_{\text{user}}(A) = N_{\text{cycle}}(A) \times \text{T}_{\text{cycle}}, \, N_{\text{cycle}}(A) = \sum^{n}_{i = 1}n_i(A) \times \text{CPI}_i$

   Refinement with memory access:

   $\text{T}_{\text{user}}(A) = (N_{\text{instr}}(A) \times CPI(A) + N_{\text{mm\_cycle}}(A)) \times \text{T}_{\text{cycle}}$

   $ N_{\text{mm\_cycle}}(A) = N_{\text{rw\_op}}(A) \times R_{\text{miss}}(A) \times N_{\text{miss\_cycles}}$

   Average memory access time:

   $T_{\text{read\_access}}^{L_n}(A) = T_{\text{read\_hit}}^{L_{n + 1}} + R_{\text{read\_miss}}^{L_{n + 1}}(A) \times T_{\text{read\_miss}}^{L_{n + 1}}$

2. System CPU time

   Time CPU spends for executing OS routines

3. Waiting time

   I/O waiting time and execution of other programs because of sharing

##### Parallel

$T_p(n)$, $p$ processors, problem of size $n$

1. Time for executing local computation
2. Time for exchange of data between processors
3. Time for synchronisation between processors
4. Waiting time

#### • Speedup & Efficiency

##### Cost

$C_p(n) = p \times T_p(n)$, processor-runtime product

Parallel program is cost-optimal if it executes same total number of operations as the fastest sequential program

##### Speedup

$\displaystyle S_p(n) = \frac{T_\text{best\_seq}(n)}{T_p(n)}$

Theoretically, $S_p(n) \leq p$

In practice, $S_p(n) > p$ superlinear speedup can occur

##### Efficiency

$\displaystyle E_p(n) = \frac{S_p(n)}{p}$

If ideal speedup, $S_p(n) = p, E_p(n) = 1$

#### • Scalability

Interaction between size of problem and size 

Constraints: application-oriented, resource-oriented

##### Amdahl's Law

Speedup limited by the fraction of the algorithm that cannot be parallelised ($f$)

$0 \leq f \leq 1$, sequential fraction / fixed-workload performance

$\displaystyle s_p(n) \leq \frac{1}{f}$

Manufacturers are discouraged from making large parallel computers, should reduce sequential fraction instead

##### Gustafson's Law

Rebuttal to Amdahl's Law

$f$ is not a constant but decreases when problem size increases

$\displaystyle \lim_{n \to \infty}f(n) = 0, \lim_{n \to \infty}S_p(n) = p$

$S_p(n) \leq p$

#### • Performance Analysis

Try the simplest parallel solution first and measure performance

Determine the bottleneck

1. Instruction-rate limited: add non-memory instructions
2. Memory: remove almost all maths, but load same data
3. Locality of data access: change all array access to `A[0]`
4. Sync overhead: remove all atomic operations / locks

## Lecture 06: GPGPU

#### • Drawbacks of GPU

1. Hard to transfer data between CPU and GPU
2. No scatter
3. No communication between fragments
4. Coarse thread synchronisation

#### • Nvidia GPU Architecture

Multiple streaming multiprocessors (SMs)

Each SM consists of multiple compute cores

#### • CUDA Programming Model

* Device: GPU
* Host: CPU
* Kernel: function that runs on the device

CUDA kernel executed by an array of threads that are extremely lightweight, all threads run the same code

Thread array divided into multiple blocks, each block has shared memory, atomic operation and barrier synchronisation

Threads in different blocks cannot cooperate

Threads in a warp start together at the same program address

#### • CUDA Memory Model

Global memory is cached, shared memory is not cached

Simultaneous accesses to global memory by threads in a warp are coalesced

Shared memory has higher bandwidth and lower latency than local & global memory

Shared memory divided into banks, addresses from different banks can be accessed simultaneously, memory access has to be serialised to avoid bank conflict

#### • Optimising CUDA Programs

##### a. Optimise Memory for Maximum Bandwidth

1. Minimise data transfer between host and device

   Bandwidth between device & GPU is much higher than bandwidth between host & device

   - Batch small transfers

   - Use page-locked transfer

2. Coalesce global memory accesses

3. Use shared memory to minimise global memory accesses

4. Minimise bank conflicts in shared memory

##### b. Maximise Parallel Execution

1. Concurrent data transfer

   `cudaMemcpyAsync()` instead of `cudaMemcpy()`

   Overlap asynchronous transfers with computation

2. Strike balance between occupancy and utilisation

   #warps should be larger than #multiprocessor

   #threads per block should be multiple of warp size

   Avoid multiple contexts per GPU

##### c.  Optimise Instruction for Maximum Throughput

1. Minimise arithmetic instructions with low throughput

   Trade precision for speed

2. Minimise divergent warps caused by control flow

3. Reduce number of instructions

## Lecture 07: Memory Coherence & Consistency

#### • Coherence

##### Cache Properties

* Cache size: larger cache increases access time but reduces cache misses
* Block size: larger block increases chance of spatial locality but replacement lasts longer

##### Write Policy

* Write-through: write access immediately transferred to main memory

  Pro: memory block always contain newest value

  Con: slow speed

* Write-back: write performed only in cache, use dirty bit to write to main memory when cache block is replaced

  Pro: less write operations

  Con: memory may contain invalid entries

##### Definition

Each processor has consistent view of memory through its local cache, all processors must agree on the read/write order to the same memory location

Properties of coherent memory system:

1. Program order

   $\texttt{P}$ write to $\texttt{X}$ $\to$ no write to $\texttt{X}$ $\to$ $\texttt{P}$ read from $\texttt{X}$

   $\texttt{P}$ should get the same value

2. Write propagation

   $\texttt{P}_{\texttt{1}}$ write to $\texttt{X}$ $\to$ no write to $\texttt{X}$ $\to$ $\texttt{P}_{\texttt{2}}$ read from $\texttt{X}$

   $\texttt{P}_{\texttt{2}}$ should read value written by $\texttt{P}_{\texttt{1}}$

3. Write serialisation

   $\texttt{V}_{\texttt{1}}$ written to $\texttt{X}$ $\to$ $\texttt{V}_{\texttt{2}}$ written to $\texttt{X}$

   Processors will never read $\texttt{X}$ as $\texttt{V}_{\texttt{2}}$ before $\texttt{V}_{\texttt{1}}$

Coherence can be maintained by:

1. Software-based solution

   OS / compiler / hardware

2. Hardware-based solution

   Cache coherence protocols

   * Snooping based
   * Directory based

##### Implications

* Overhead in shared address space

  Increased memory latency, lowered cache hit rate

* Cache ping-pong

  Multiple processors read & modify same global variable

* False sharing

  2 processors write to different addresses, but are mapped to the same cache line

#### • Consistency

##### Definition

Memory operation performed by one thread become visible to another thread for different memory locations

Coherence only guarantees that writes will eventually propagate

Consistency deals with when the writes propagate, relative to reads and writes to other addresses

##### Memory operations

Four types of orderings:

* W $\to$ R
* R $\to$ R
* R $\to$ W
* W $\to$ W

##### Sequential Consistency Model

Maintains all four memory operations, but results in loss of performance

One memory operation at any point in time

##### Relaxed Consistency

Relax ordering of memory operations if data dependencies allow

Motivation: hide memory latencies to gain performance

Cannot relax: R $\to$ W, W $\to$ W & W $\to$ R to the same memory location

1. Relax W $\to$ R

   Allow read on processor to be reordered wrt previous write

   **Total Store Ordering (TSO)**: all processors see store operations in the same order (if one sees change, all see change)

   **Processor Consistency (PC)**: write is not propagated or serialised to all processors

2. Relax W $\to$ W and W $\to$ R

   Writes can bypass earlier writes to different locations

   **Partial Store Ordering (PSO)**

## Lecture 08: Performance Instrumentation

#### • Aspects

Quality attributes: scalability, reliability, resource usage

Workload: normal, overloaded

Timeline: testing before lease, incident performance response

#### • Terminology

Latency: amount of time spend waiting

Response time: time for operation to complete

Throughput: rate of work performed

IOPS: input/output operations

Utilisation: how busy a resource is

Saturation: degree to which queued work cannot service

Bottleneck: resource that limits performance

Workload: input to the system

#### • Perspectives

##### Resource Analysis

Focus on utilisation

1. Time-based: average amount of time for resource to be busy
2. Capacity-based: amount of throughput

##### Workload Analysis

Targets: requests, latency, completion

Metrics: throughput, latency

#### • Methodologies

1. Problem statement

2. USE

   Utilisation, Saturation, Errors

3. Monitoring

4. Perf analysis in 60 minutes (Linux)

5. Tools

   List available perf tools

   For each tool, list useful metrics

   For each metric, list possible ways to interpret

   Categorisation:

   * Fixed counters
   * Event-based counters (enabled )

6. CPU profile

##### (Anti-Methods)

Random change / Streetlight / Drunk man

## Lecture 09: Parallel Programming Models (II)

#### • Data Distribution

##### 1D Arrays

Assume $p$ identical processors, $n$ elements

1. Blockwise

   Block size $\displaystyle b = \left\lceil \frac{n}{p} \right\rceil$

   $p_j$ takes elements $[(j - 1) \times B + 1 \cdots j \times B ]$

2. Cyclic

   $p_j$ takes elements 

   $\displaystyle [j, j + p, \cdots, j + \left(\left\lceil \frac{n}{p} \right\rceil - 1\right) \times p]$ if $j \leq n \mod p$

   $\displaystyle [j, j + p, \cdots, j + \left(\left\lceil \frac{n}{p} \right\rceil - 2\right) \times p]$ otherwise

##### 2D Arrays

Combination of clockwise / cyclic in one or both dimensions

* One dimensional distributions

  1. Block-cyclic

     Form blocks of size $b$, perform cyclic allocation along one dimension

* Two dimensional distributions

  Checkerboard distribution, processors virtually organised into 2D mesh of $R \times C$

  1. Blockwise

     Split elements into blocks along both dimensions depending on $R$ and $C$

  2. Cyclic

     Cyclic assignment of elements according to processor mesh

  3. Block-cyclic

     Elements split into $b_1 \times b_2$ size blocks, then cyclical assignment

#### • Information Exchange

##### Shared Address Space (Shared Variables)

Assume global memory accessible by all processors

Need synchronisation operations for safe concurrent access

##### Distributed Address Space (Communications Operations)

Assume disjoint memory space

Use message-passing programming model (send/receive)

Types:

1. Point-to-point
2. Global

Principles:

* Data explicitly partitions for each process
* All interaction requires both parties to participate
* Programmer has to explicitly express paraleelism
* Loosely synchronous (between interactions, tasks execute asynchronously)

#### • Communication Protocols

|                  | Blocking                                                     | Non-blocking                                                 |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Buffered**     | Sending process returns after data has been copied into buffer<br />**Issues**: idling & buffer copying overhead | Sending process returns after initiating the transfer to buffer |
| **Non-buffered** | Sending process blocks until matching receive operation encountered<br />**Issues**: idling & deadlocks | Note: programmer must explicitly ensure completion by polling |

##### Semantics of `send/receive` Operations

| Local View                                                   | Global View                                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| Blocking:<br />Return indicates user is allowed to reuses resources | Synchronous:<br />Communication does not complete before both send & receive started operation |
| Non-blocking:<br />Procedure may return before operation completes | Asynchronous:<br />Sender can execute operation without any coordination with receiver |

## Lecture 10: Message Passing Interface

#### • Overview

1. Initialise communications
2. Do communications
3. Exit from message-passing system

#### • Initialisation, Finalisation & Abort

`MPI_Init`: called only once before any other MPI routines

`MPI_Finalise`: must be the last MPI call

`MPI_Abort`: return error code to `mpirun`

#### • Point-To-Point Communication

Blocking asynchronous: `MPI_Send` / `MPI_Recv`

Non-blocking asynchronous: `MPI_Isend` / `MPI_Irecv`

Blocking and non-blocking can be mixed

Order of receive is guaranteed with two processes, but not with >= 2 processes

Message = data + envelope

* Data: start-buffer + count + datatype
* Envelope: destination/source + tag + communicator

Reasons for deadlock:

1. Both process receive then send

   Always deadlock

2. Both process send then receive

   Deadlock if no buffer / buffer too small

Deadlock-free logical ring:

* Even rank: send then receive
* Odd rank: receive then send

#### • Groups & Communicators

Provides basis for user-defined topologies & safe communication

* Group:

  Each process has a unique rank, can be a member of multiple groups

* Communicator:

  Domain for a group of processes

  * Intra-communicators: collective communication on a single group
  * Inter-communicators: point-to-point communication between two groups

#### • Collective Communication

Operations that involve all processes in a communicator

Blocking operations by default

1. Single transfer: Point to point communication
2. Gather & scatter
3. Single broadcast: Root send same data to all other processors
4. Multi broadcast: Each processor send same data to every other processor, data blocks collected in rank order
5. Single accumulation: Each processor provide data with same type and size, result reduced in root processor
6. Multi accumulation: Each processor provide data for every other processor, accumulate in each processor
7. Total exchange

##### Duality

Communication operation can be represented by a spanning tree graph

Two operations are duality if same spanning tree can be used for 

## Lecture 11: Interconnection Networks

#### • Shear Sort Algorithm

1. Row sorting

   Odd rows in ascending order

   Even rows in descending order

2. Column sorting

   Top to bottom in ascending order

3. Repeat

For $N$ numbers, $\log_2 N + 1$ phases

Each row use odd-even transposition sort $O(\sqrt{n})$

Time complexity: $O((\log_2N + 1)\sqrt{n})$

#### • Topology

##### § Direct Interconnection

Static/point-to-point, endpoints are of the same type

##### Types

1. All in the chart

2. Elaboration on CCC

   From a $k$-dimension hypercube, substitute each node with a cycle of $k$ nodes

   Each node in CCC is labeled as $(X, Y)$, $X$ being node index in hypercube, $Y$ being position in the cycle

   $(X, Y)$ is connected to:

   - $(X, (Y+1) \text{ mod } K), \, (X, (Y-1) \text{ mod } K)$ (cycle buddies)

   - $(X \oplus 2^Y, Y)$ (link from corresponding dimension in hypercube)

##### Metrics

1. Diameter

   Maximum distance between any pair of nodes

   Small diameter ensures small distances for message transmission

2. Degree

   Number of direct neighbour nodes

   Small node degrees reduces hardware overhead

3. Bisection width

   Minimum number of edges that must be removed to divide network into two halves

   Measure for capacity of network when transmitting simultaneously

4. Connectivity

   Minimum number of nodes/edges that must fail to disconnect the network

   Determine number of independent paths between any pair of nodes

| Network with $n$ Nodes                 |  Degree  |                           Diameter                           |              Bisection Width               | Edge Connectivity |
| -------------------------------------- | :------: | :----------------------------------------------------------: | :----------------------------------------: | :---------------: |
| Complete graph                         | $n - 1$  |                             $1$                              | $\displaystyle \left(\frac{n}{2}\right)^2$ |      $n - 1$      |
| Linear array                           |   $2$    |                           $n - 1$                            |                    $1$                     |        $1$        |
| Ring                                   |   $2$    |    $\displaystyle \left\lfloor \frac{n}{2}\right\rfloor $    |                    $2$                     |        $2$        |
| $d$-dimension mesh<br />$n = r^d$      |   $2d$   |                     $d(\sqrt[d]{n} - 1)$                     |     $\displaystyle n^{\frac{d-1}{d}}$      |        $d$        |
| $d$-dimension torus<br />$n = r^d$     |   $2d$   | $\displaystyle d \left\lfloor \frac{\sqrt[d]{n}}{2}\right\rfloor$ |     $\displaystyle 2n^{\frac{d-1}{d}}$     |       $2d$        |
| $k$-dimension hypercube<br />$n = 2^k$ | $\log n$ |                           $\log n$                           |        $\displaystyle \frac{n}{2}$         |     $\log n$      |
| $k$-dimension CCC<br />$n = k2^k$      |   $3$    | $\displaystyle 2k - 1 + \left\lfloor \frac{k}{2}\right\rfloor$ |        $\displaystyle \frac{n}{2k}$        |        $3$        |
| Complete binary<br />$n = 2^k - 1$     |   $3$    |             $\displaystyle 2 \log \frac{n+1}{2}$             |                    $1$                     |        $1$        |

##### § Indirect Interconnection

Dynamic, formed by switches

Reduce hardware costs by sharing switches and links

##### Types

1. Bus

   Only one pair of devices can communicate at a time

2. Crossbar 

   Switch has two states: straight or direction change

   Hardware is costly ($n \times m$ switches)

3. Multistage switching

   Several intermediate switches, obtain small distance for arbitrary input & output

4. Omega

   One unique path for every input & output

   $n \times n$ Omega network has $\log n$ stages, $n/2$ switches per stage

   Node $(\alpha, i)$ connects to two $(\beta, i+1)$ where (e.g. `010, 0`)

   * $\beta$ is cyclic left shift (e.g. `100, 1`)
   * $\beta$ is cyclic left shift + inversion of LSB (e.g. `101, 1`)

5. Butterfly

   Node $(\alpha, i)$ connects to (e.g. `010, 0`)

   * $(\alpha, i + 1)$, straight edge (e.g. `010, 1`)
   * $(\alpha', i+1)$, $\alpha$ and $\alpha'$ differ in the $(i+1)^{\text{th}}$ bit from the left (e.g. `110, 1`)

6. Baseline

   Node $(\alpha, i)$ connects to two $(\beta, i+1)$ where (e.g. `010, 1`)

   * $\beta$ is cyclic right shift of last $(k - i)$ bits (e.g. `001, 2`)
   * $\beta$ is inversion of LSB + cyclic right shift of last $(k - i)$ bits (e.g. `011, 2`)

##### Metrics

1. Cost
2. Concurrent connections

#### • Routing

##### Classfication

* Based on path length

  Minimal vs non-minimal (whether the shortest path is chosen)

* Based on adaptivity

  Deterministic vs adaptive

##### Types

1. XY Routing

   Move in $X$ direction until $X_\text{src} = X_\text{dst}$

   Move in $Y$ direction until $Y_\text{src} = Y_\text{dst}$

2. E-Cube Routing for Hypercube

   Hamming distance

   Start from MSB to LSB, revert different bits, at most $n$ hops

3. XOR-Tag Routing

   Let $T = \text{src } \oplus \text{ dst}$, at stage $k$:

   * Go straight if $k^{\text{th}}$ bit of $T$ is 0
   * Crossover if $k^{\text{th}}$ bit of $T$ is 1
