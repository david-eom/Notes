## Probability

##### • Expectation

$\displaystyle E(X) = \sum^{n}_{i = 1}x_ip_i$

$\displaystyle E(X) = \int^{\infty}_{-\infty}xf(x)dx$

##### • Sample Mean

$\displaystyle \bar{x} = \frac{x_1 + x_2 + \dots + x_n}{n} \approx E(X)$

More generally: $\displaystyle \frac{h(x_1) + h(x_2) + \dots + h(x_n)}{n} \approx E(h(X))$

##### • IID Random Variables

$X_1, X_2, \dots, X_n$

$P(X_1 = a, X_2 = b) = P(X_1 = a) P(X_2 = b)$

##### • Algebra of Random Variables

If $X, Y$ are RVs, $a, b, c$ are constants, then $Z = aX + bY + c$ is also an RV

##### • Joint Distribution

$f(x, y) = f_X(x) f_Y(y | x)$

$\text{cov}(X, Y) = E((X - \mu_X)(Y - \mu_Y))$

$\text{var}(aX + bY + c) = a^2\text{var}(X) + b^2\text{var}(Y) + 2ab\text{cov}(X, Y) $

$\displaystyle \text{var}(X_1 + \dots + X_n) = \sum_{i = 1}^{n}\text{var}(X_i) + 2\sum_{i < j}\text{cov}(X_i, X_j)$

##### • Mean Square Error (MSE)

If constant $c$ is used to predict RV $Y$, MSE is $E((Y - c)^2)$

Given $x$ is observed, MSE is $E((Y - c)^2 | x) = \text{var}(Y | x) + (E(Y | x) - c)^2$

To minimise MSE, $c = E(Y)$, MSE is $\text{var}(Y)$

Without observing $X$, best predictor of $Y$ is $E(Y)$ with MSE $\text{var}(Y)$

Given realisation $x$, best predictor of $Y$ is $E(Y|x)$ with MSE $\text{var}(Y|x)$

If we still use $E(Y)$ to predict, the MSE will be $(E(Y|x) - E(Y))^2$ larger

##### • Random Conditional Expectations

$E(E(X_2 | X_1)) = E(X_2)$

$\text{var}(E(X_2 | X_1)) + E(\text{var}(X_2|X_1)) = \text{var}(X_2)$

##### • Mean MSE

$\displaystyle \frac{1}{n} \sum^n_{i = 1}\text{var}(Y|x_i) \approx E(\text{var}(Y|X))$

##### • Normal Distribution

Standard normal: $Z \sim N(0, 1)$, $E(Z) = 0$, $\text{var}(Z) = 1$

Normal: $Y = \mu + \sigma Z$, $E(Y) = \mu$, $\text{var}(Y) = \sigma^2$

##### • Gamma Distribution

$\displaystyle \Gamma(\alpha, \lambda) = \frac{\lambda^\alpha}{\Gamma(\alpha)}t^{\alpha - 1}e^{-\lambda t}$

##### • $\chi^2_1$ Distribution

$V = Z^2$

$E(V) = 1, \, \text{var}(V) = 2$

$P(V \leq v) = P(Z^2 \leq v) = P(Z \leq \sqrt{v}) - P(Z \leq -\sqrt{v}) $

$\displaystyle \chi_1^2 \sim \Gamma \left(\frac{1}{2}, \frac{1}{2}\right)$

##### • $\chi^2_n$ Distribution

$\displaystyle V = \sum^{n}_{i = 1}V_i, \, V_i \sim\chi^2_1$

$E(V) = n, \, \text{var}(V) = 2n$

$\displaystyle \chi_n^2 \sim \Gamma \left(\frac{n}{2}, \frac{1}{2}\right)$

##### • $t$ Distribution

$Z \sim N(0, 1), \, V \sim \chi_n^2$ independent

$\displaystyle t_n = \frac{Z}{\sqrt{V / n}}$

$n \to \infty, \, t_n \to Z$

##### • $F$ Distribution

$ V \sim \chi_m^2, \, W \sim \chi_n^2$ independent

$\displaystyle F_{m, n} = \frac{V / m}{W / n}$

$\displaystyle n \to \infty, \, F_{m, n} \to \frac{V}{m}$ (scaled by $m$)

##### • Sample Variance

$\displaystyle S^2 = \frac{1}{n - 1}\sum^n_{i = 1}\left(X_i - \bar{X}\right)^2$

Let $X_1, \dots, X_n$ be IID $N(\mu, \sigma^2)$

1. $\bar{X}$ and $S^2$ are independent
2. $\bar{X} \sim N(\mu, \sigma^2 / n)$
3. $\displaystyle \frac{(n - 1)S^2}{\sigma^2} \sim \chi^2_{n - 1}$
4. $\displaystyle \frac{\bar{X} - \mu}{S / \sqrt{n}} \sim t_{n - 1}$

## Survey Sampling

##### • Simple Random Sampling (SRS)

Make $n$ random draws without replacement

$E(X_i) = \mu, \, \text{var}(X_i) = \sigma^2$

$\displaystyle \text{cov}(X_i, X_j) = -\frac{\sigma^2}{N - 1}$

##### • SRS Mean

$\displaystyle \bar{X} = \frac{1}{n}\sum^n_{i = 1}X_i$

$\displaystyle E(\bar{X}) = \mu, \, \text{var}(\bar{X}) = \frac{N - n}{N - 1} \frac{\sigma^2}{n}$

##### • Exchangeable RV

$\forall \pi \in \{1, \dots, K\}, \, \left(Y_{\pi(1)}, \dots,  Y_{\pi(k)} \right) = (Y_1, \dots, Y_k)$

IID RVs are exchangeable, but exchangeable RVs might not be IID

##### • Estimate and Estimator of $\mu$ and $\sigma$, $n \ll N$

$\displaystyle \bar{x} = \frac{1}{n}\sum^n_{i = 1}x_i$

$\bar{x}$ is an estimate of $\mu$ and realisation of estimator $\bar{X}$

$\bar{x}$ has an error of $\mu - \bar{x}$, but error can neither be calculated nor estimated

Bootstrap for $\sigma$

$\displaystyle \hat{\sigma}^2 = \frac{1}{n}\sum^n_{i = 1}(x_i - \bar{x})^2, \, E(\hat{\sigma}^2) = \frac{n - 1}{n}\sigma^2$

$\displaystyle s^2 = \frac{1}{n - 1}\sum^n_{i = 1}(x_i - \bar{x})^2, \, E(S^2) = \sigma^2$

$s^2$ estimates $\sigma^2$

##### • Standard Error (SE)

$\sigma / \sqrt{n}$, which indicates how much $\bar{X}$ fluctuates around $\mu$

SE can be estimated with $s / \sqrt{n}$

SE cannot be calculated (we do not know $\sigma$)

SE is an unknown fixed constant, not a random variable

##### • Estimating proportion, $n \ll N$

Let $p$ be the proportion of 1

$\mu = p, \, \sigma^2 = p(1 - p)$

SE $\displaystyle = \frac{\sqrt{p(1 - p)}}{\sqrt{n}}$

##### • Interval Estimation

Confidence interval can be constructed using formula if the population has normal distribution, or if $n$ is large due to Central Limit Theorem

For sufficiently large $n$, and $0 < \alpha < 1$:

$\displaystyle \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \sim N(0, 1), \, P\left(-z_{\frac{\alpha}{2}} \leq \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \leq z_{\frac{\alpha}{2}} \right) \approx 1 - \alpha$

$\displaystyle P\left(\bar{X} - z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X} + z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} \right) \approx 1 - \alpha$

Random interval: $\displaystyle \left(\bar{X} - z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} ,\,  \bar{X} + z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} \right)$

$(1 - \alpha)$ confidence interval: $\displaystyle \left(\bar{x} - z_{\frac{\alpha}{2}}\frac{s}{\sqrt{n}} , \, \bar{x} + z_{\frac{\alpha}{2}}\frac{s}{\sqrt{n}} \right)$

$\displaystyle \left(\text{estimate} - z_{\frac{\alpha}{2}}\text{SE} , \, \text{estimate} + z_{\frac{\alpha}{2}}\text{SE} \right)$

Exact confidence interval: $\displaystyle \left(\bar{x} - t_{\frac{\alpha}{2}, n-1}\frac{s}{\sqrt{n}} , \, \bar{x} + t_{\frac{\alpha}{2}, n-1}\frac{s}{\sqrt{n}} \right)$

$\displaystyle t_{\frac{\alpha}{2}, n-1}$: number such that $P(t_{n - 1} > t_{\frac{\alpha}{2}, n-1}) > \alpha / 2$

If $\displaystyle \frac{n}{N}$ is not small, moderate SE by multiplying with $\displaystyle \sqrt{\frac{N - n}{N - 1}}$

##### • Bias

$E(\bar{X}) = w + b$

$\displaystyle E((\bar{X} - w)^2) = \text{var}(\bar{X}) + (E(\bar{X}) - w)^2 = \frac{\sigma^2}{n} + b^2$

$\text{MSE} = \text{SE}^2 + \text{bias}^2$

## Parameter Estimation

##### • $\text{Pois}$ Distribution

$\displaystyle P(X = k) =\frac{\lambda^k e^{-\lambda}}{k!}$

$E(X) = \lambda, \, \text{var} = \lambda$

##### • Parameter Space

Parameter $\theta$ lies in parameter space $\Theta \subset \R$

Poisson: $\displaystyle f(x|\lambda) = \frac{\lambda^xe^{-\lambda}}{x!}, \theta = \lambda, \Theta = \R^+$

Bernoulli: $f(x|p) = p^x(1-p)^{1-x}, \theta = p, \Theta = \{x \in \R | 0 \leq x \leq 1\}$

Two ways to estimate $\theta$ from realisations $x_1, \dots, x_n$:

1. Method of moments (MOM)
2. Method of maximum likelihood (ML)

##### • Moments

$k$-th moment of an RV $X$: $\mu_k = E(X^k), k \in \R^+$

Unbiased estimator of moments: $\displaystyle \hat{\mu}_k = \frac{1}{n} \sum_{i = 1}^n{X_i}^k$

##### • MOM Estimator

Let $X_1, \dots, X_n$ be IID with $f(x|\theta)$, $\theta$ lies in $\Theta \subset \R^p$ where $p \geq 1$

Suppose $\theta = g(\mu_1, \dots, \mu_q)$ for some function $g$ of the first $q$ moments

Estimator $\hat{\theta} = g(\hat{\mu}_1, \dots, \hat{\mu}_q)$

1. Poisson: $\theta = \begin{bmatrix}\lambda\end{bmatrix} = \begin{bmatrix}\mu_1\end{bmatrix}, \, g(x) = x$ 
1. Bernoulli: $\theta = \begin{bmatrix}p\end{bmatrix} = \begin{bmatrix}\mu_1\end{bmatrix}, \, g(x) = x$ 
1. Normal: $\theta = \begin{bmatrix}\mu \\ \sigma^2\end{bmatrix} = \begin{bmatrix}\mu_1 \\ \mu_2 - {\mu_1}^2\end{bmatrix}, \, g\left(\begin{bmatrix}x \\ y\end{bmatrix}\right) = \begin{bmatrix}x \\ y - x^2\end{bmatrix}$ 

Every MOM estimator is consistent, it goes to parameter as $n \to \infty$

MOM estimators might be biased (for normal, $\hat{\mu}$ unbiased, $\hat{\sigma}$ biased)

##### • Likelihood / Log Likelihood Function

Let $x_1, \dots, x_n$ be realisations of IID $X_1, \dots, X_n$ with PDF/PMF $f(x|\theta)$

$\displaystyle L(\theta) = \prod_{i = 1}^{n}f(x_i|\theta), \, \ell(\theta) = \sum_{i = 1}^n\log f(x_1|\theta)$

##### • Maximum Likelihood Estimation

Maximiser of $L/\ell$ is the maximum likelihood estimator $\hat{\theta}$

1. Poisson: $\hat{\theta} = \begin{bmatrix}\hat{\lambda}\end{bmatrix} = \begin{bmatrix}\bar{X}\end{bmatrix}$ with realisation $\bar{x}$ 
1. Normal: $\hat{\theta} = \begin{bmatrix}\hat{\mu} \\ \hat{\sigma}^2\end{bmatrix} = \begin{bmatrix}\bar{X} \\ \hat{\sigma}^2\end{bmatrix}$ with realisation $\begin{bmatrix}\bar{x} \\ \hat{\sigma}^2\end{bmatrix}$ 
1. Gamma: satisfies:
   1. $\displaystyle \log\left(\frac{\hat{\alpha}}{\bar{X}}\right) - \frac{\Gamma'(\hat{\alpha})}{\Gamma(\hat{\alpha})} + \frac{1}{n}\sum_{i = 1}^n\log(X_i) = 0$
   1. $\displaystyle \hat{\lambda} = \frac{\hat{\alpha}}{\bar{X}}$
1. Binomial: use Lagrangian function, $\displaystyle \hat{p_i} = \frac{X_i}{n}$

Refinement of $L$ and $\ell$: we can take out constant factors in $L(\theta)$ and additive constants in $\ell(\theta)$

##### • Comparison Between MOM and ML

MOM: can be written down as formulae

ML: asymptotically unbiased, also beats MOM on SE

## Distribution of ML Estimators

##### • Fisher Information

Let $X$ have density $f(x|\theta), \, \theta \subset \Theta \subset \R^p$

Fisher Information of one sample is a symmetric $p \times p$ matrix

 $\displaystyle \mathcal{I}(\theta) = -E\left[\frac{d^2\log f(X)}{d\theta^2}\right]$

$\displaystyle (i, j)\text{-entry} = -E\left[\frac{\partial ^2 \log f(X)}{\partial \theta_i \partial \theta_j}\right]$

##### • Fisher Information for Multinomial

$X \sim \text{Mult}(n, (p_1, \dots, p_r))$

$\theta = (p_1, \dots, p_{{r - 1}}), \, p_r = 1 - p_1 - \dots - p_{r - 1}$

$\displaystyle \log f(X) = \sum_{i = 1}^r X_i \log p_i + c$

$\displaystyle \frac{\partial \log f(X)}{\partial p_i} = \frac{X_i}{p_i} - \frac{X_r}{p_r}, \, 1 \leq i \leq r - 1$

$\displaystyle \frac{\partial^2 \log f(X)}{\partial {p_i}^2} = -\frac{X_i}{{p_i}^2} - \frac{X_r}{{p_r}^2}, \, 1 \leq i \leq r - 1$

$\displaystyle \frac{\partial^2 \log f(X)}{\partial p_i \partial p_j} = -\frac{X_r}{{p_r}^2}, \, 1 \leq i \neq j \leq r - 1$

$(i, j)\text{-entry}$ of $\mathcal{I}(\theta)$:

* $\displaystyle \frac{n}{p_i} + \frac{n}{p_r}, \, i = j$
* $\displaystyle \frac{n}{p_r}, \, i \neq j$

##### • Fisher Information for $n$ Joint Samples

Let IID $X_1, X_2, \dots, X_n$ have density $f(x|\theta), \, \theta \subset \Theta \subset \R^p$

Joint density: $g(\bold{X}|\theta) = f(X_1 | \theta) \cdots f(X_n | \theta)$

Information in $\bold{X} = n\mathcal{I}(\theta)$

##### • Distribution of ML Estimators

As $n \to \infty$, approximately:

$\displaystyle \hat{\theta}_n \sim N \left(\theta,  \frac{\mathcal{I}(\theta)^{-1}}{n} \right)$

ML estimators are asymptotically unbiased, $\hat{\theta}_n \to \theta$

$1- \alpha$ confidence interval of ML estimators:

$\displaystyle \left( \hat{\theta}_n - z_{\frac{\alpha}{2}}\sqrt{\frac{\mathcal{I}(\theta)^{-1}}{n}}, \, \hat{\theta}_n + z_{\frac{\alpha}{2}}\sqrt{\frac{\mathcal{I}(\theta)^{-1}}{n}} \right)$

## Goodness-Of-Fit

##### • Pearson's $X^2$

$(X_1, \dots, X_r) \sim \text{Mult}(n, \bold{p}(\theta)), \, \theta \in \Theta \subset\R^k, \, k < r - 1$

$\displaystyle X^2 = \sum_{i = 1}^r \frac{(X_i - np_i(\hat{\theta}))^2}{np_i(\hat{\theta})}$ 

$\displaystyle X^2 = \sum\frac{(\text{observed} - \text{expected})^2}{\text{expected}}$

As $n \to \infty$, distribution of $X^2$ converges to $\chi^2_{r - 1 - k}$

##### • Likelihood Ratio

$\displaystyle G = 2 \log \left(\frac{L_1}{L_0}\right) = 2 \sum_{i = 1}^r X_i \log \left(\frac{X_i / n}{p_i(\hat{\theta})} \right) = 2(\ell_1 - \ell_0)$

The larger the ratio, the we doubt null hypothesis

As $n \to \infty$, distribution of $G$ converges to $\chi^2_{k_1 - k_9}$

